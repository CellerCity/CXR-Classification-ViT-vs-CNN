{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184a7510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:39:17.375959Z",
     "iopub.status.busy": "2024-04-20T16:39:17.375573Z",
     "iopub.status.idle": "2024-04-20T16:39:32.942399Z",
     "shell.execute_reply": "2024-04-20T16:39:32.941247Z"
    },
    "papermill": {
     "duration": 15.600527,
     "end_time": "2024-04-20T16:39:32.944802",
     "exception": false,
     "start_time": "2024-04-20T16:39:17.344275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting thop\r\n",
      "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\r\n",
      "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: thop\r\n",
      "Successfully installed thop-0.1.1.post2209072238\r\n"
     ]
    }
   ],
   "source": [
    "!pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c6f517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:39:33.009781Z",
     "iopub.status.busy": "2024-04-20T16:39:33.009381Z",
     "iopub.status.idle": "2024-04-20T16:39:36.697761Z",
     "shell.execute_reply": "2024-04-20T16:39:36.696677Z"
    },
    "papermill": {
     "duration": 3.724778,
     "end_time": "2024-04-20T16:39:36.700589",
     "exception": false,
     "start_time": "2024-04-20T16:39:32.975811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "num_classes = 15\n",
    "IMAGE_SIZE = (224, 224)\n",
    "# IMAGE_SIZE[0], IMAGE_SIZE[1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17ce135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:39:36.764200Z",
     "iopub.status.busy": "2024-04-20T16:39:36.763704Z",
     "iopub.status.idle": "2024-04-20T16:39:36.897804Z",
     "shell.execute_reply": "2024-04-20T16:39:36.896805Z"
    },
    "papermill": {
     "duration": 0.168943,
     "end_time": "2024-04-20T16:39:36.900371",
     "exception": false,
     "start_time": "2024-04-20T16:39:36.731428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "from thop import clever_format\n",
    "import torch\n",
    "\n",
    "def display_params_flops(model):\n",
    "    #params\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    num_params_millions = num_params / 1e6\n",
    "    print(f\"Number of parameters in millions: {num_params_millions:.2f} M\")\n",
    "\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    num_params_millions = num_params / 1e6\n",
    "    print(f\"Number of trainable parameters in millions: {num_params_millions:.2f} M\")\n",
    "\n",
    "\n",
    "    #FLOPS\n",
    "    input_size = (1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1])  \n",
    "\n",
    "\n",
    "    # Move the model to GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Use thop.profile to count FLOPs\n",
    "    input_tensor = torch.randn(*input_size)\n",
    "    if torch.cuda.is_available():\n",
    "        input_tensor = input_tensor.cuda()\n",
    "    flops, params = profile(model, inputs=(input_tensor,))\n",
    "\n",
    "    # Convert FLOPs to gigaFLOPs and format the results\n",
    "    flops, params = clever_format([flops, params], \"%.2f\")\n",
    "    print(f\"FLOPs: {flops}, Params: {params}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e6912",
   "metadata": {
    "papermill": {
     "duration": 0.030429,
     "end_time": "2024-04-20T16:39:36.961323",
     "exception": false,
     "start_time": "2024-04-20T16:39:36.930894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb09acf",
   "metadata": {
    "papermill": {
     "duration": 0.030466,
     "end_time": "2024-04-20T16:39:37.022937",
     "exception": false,
     "start_time": "2024-04-20T16:39:36.992471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b784254f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:39:37.085542Z",
     "iopub.status.busy": "2024-04-20T16:39:37.085145Z",
     "iopub.status.idle": "2024-04-20T16:39:47.446740Z",
     "shell.execute_reply": "2024-04-20T16:39:47.445289Z"
    },
    "papermill": {
     "duration": 10.395974,
     "end_time": "2024-04-20T16:39:47.449346",
     "exception": false,
     "start_time": "2024-04-20T16:39:37.053372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:03<00:00, 148MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.1141, -0.1989,  0.0149,  0.3392,  0.0469, -0.3226,  0.3937,  0.6723,\n",
      "         -0.3124, -0.3450,  0.0357, -0.4151, -0.0462,  0.2496,  0.0574]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 139.63 M\n",
      "Number of trainable parameters in millions: 119.61 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "FLOPs: 19.63G, Params: 139.63M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class VGG19Model(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(VGG19Model, self).__init__()\n",
    "\n",
    "        # Add a convolutional layer at the top\n",
    "        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Assuming input is grayscale (1 channel)\n",
    "\n",
    "        self.vgg19 = models.vgg19(pretrained=True)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            # Freeze all layers except classifier layers\n",
    "            for param in self.vgg19.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze the classifier layers\n",
    "            for param in self.vgg19.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "        # Get the number of input features for the final fully connected layer\n",
    "        in_features = self.vgg19.classifier[6].in_features\n",
    "\n",
    "        # Replace the final fully connected layer with a new one for the specified number of classes\n",
    "        self.vgg19.classifier[6] = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.vgg19(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGG19Model(num_classes, fine_tune=False) # change fine_tune as required\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924d167",
   "metadata": {
    "papermill": {
     "duration": 0.034511,
     "end_time": "2024-04-20T16:39:47.518552",
     "exception": false,
     "start_time": "2024-04-20T16:39:47.484041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe97a87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:39:47.590003Z",
     "iopub.status.busy": "2024-04-20T16:39:47.589490Z",
     "iopub.status.idle": "2024-04-20T16:40:05.520398Z",
     "shell.execute_reply": "2024-04-20T16:40:05.519181Z"
    },
    "papermill": {
     "duration": 17.969647,
     "end_time": "2024-04-20T16:40:05.523267",
     "exception": false,
     "start_time": "2024-04-20T16:39:47.553620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\r\n",
      "Building wheels for collected packages: efficientnet_pytorch\r\n",
      "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=5aa126a5656fd926a8c8974a568d3b2086ddf3843a5f110abab1d0e0006f16c4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "Successfully built efficientnet_pytorch\r\n",
      "Installing collected packages: efficientnet_pytorch\r\n",
      "Successfully installed efficientnet_pytorch-0.7.1\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
      "100%|██████████| 20.4M/20.4M [00:00<00:00, 155MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.0184, -0.1523,  0.0733,  0.0891, -0.0507, -0.0303, -0.0032, -0.0084,\n",
      "          0.0310, -0.0577, -0.0176,  0.0120, -0.0025, -0.0212, -0.0970]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 4.03 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.padding.ZeroPad2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 27.03M, Params: 61.23K\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch # for installing efficientnet model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class EfficientNetModel(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(EfficientNetModel, self).__init__()\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0', num_classes=num_classes, in_channels=1)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.efficientnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            for param in self.efficientnet._fc.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.efficientnet(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a2c2b",
   "metadata": {
    "papermill": {
     "duration": 0.037421,
     "end_time": "2024-04-20T16:40:05.598625",
     "exception": false,
     "start_time": "2024-04-20T16:40:05.561204",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049f7d45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:05.674712Z",
     "iopub.status.busy": "2024-04-20T16:40:05.674272Z",
     "iopub.status.idle": "2024-04-20T16:40:09.492051Z",
     "shell.execute_reply": "2024-04-20T16:40:09.490852Z"
    },
    "papermill": {
     "duration": 3.859101,
     "end_time": "2024-04-20T16:40:09.494398",
     "exception": false,
     "start_time": "2024-04-20T16:40:05.635297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
      "100%|██████████| 230M/230M [00:01<00:00, 136MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.3527,  0.2662, -0.2867,  0.5065,  0.1777, -0.1360,  0.1384,  0.3014,\n",
      "          0.1933,  0.0919, -0.0363,  0.1002,  0.3396,  0.4503, -0.2354]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 58.17 M\n",
      "Number of trainable parameters in millions: 0.03 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 11.60G, Params: 58.17M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(ResNetModel, self).__init__()        \n",
    "         # Add a convolutional layer at the top\n",
    "        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Assuming input is grayscale (1 channel)\n",
    "\n",
    "        self.resnet = models.resnet152(pretrained=True)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.resnet.fc.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "        in_features = self.resnet.fc.in_features\n",
    "\n",
    "        # Replace the final fully connected layer with a new one for the specified number of classes\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNetModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0d808",
   "metadata": {
    "papermill": {
     "duration": 0.038449,
     "end_time": "2024-04-20T16:40:09.572392",
     "exception": false,
     "start_time": "2024-04-20T16:40:09.533943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. CoatNet ( 512 x 512 image size not compatible )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21e0e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:09.651767Z",
     "iopub.status.busy": "2024-04-20T16:40:09.650809Z",
     "iopub.status.idle": "2024-04-20T16:40:19.110209Z",
     "shell.execute_reply": "2024-04-20T16:40:19.108857Z"
    },
    "papermill": {
     "duration": 9.50082,
     "end_time": "2024-04-20T16:40:19.112642",
     "exception": false,
     "start_time": "2024-04-20T16:40:09.611822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9ffca07d324dfba9f584f6d7e2f90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/727M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 1.0493,  0.4044,  0.6934, -2.3854,  2.4097,  0.5271, -2.4023, -2.9863,\n",
      "         -0.0977,  1.1776, -2.9211, -1.2702, -1.6801,  2.0718, -0.2862]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 163.66 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 32.49G, Params: 163.28M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "\n",
    "class CoatNetModel(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(CoatNetModel, self).__init__()\n",
    "        self.coatnet = create_model(\n",
    "            'timm/coatnet_3_rw_224.sw_in12k', # accepts only 224x224 images\n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1\n",
    "        )\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.coatnet.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.coatnet.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.coatnet(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoatNetModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits\n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b358b2",
   "metadata": {
    "papermill": {
     "duration": 0.039887,
     "end_time": "2024-04-20T16:40:19.193151",
     "exception": false,
     "start_time": "2024-04-20T16:40:19.153264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. ConvNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0cb5c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:19.275005Z",
     "iopub.status.busy": "2024-04-20T16:40:19.274254Z",
     "iopub.status.idle": "2024-04-20T16:40:22.967274Z",
     "shell.execute_reply": "2024-04-20T16:40:22.966158Z"
    },
    "papermill": {
     "duration": 3.736414,
     "end_time": "2024-04-20T16:40:22.969809",
     "exception": false,
     "start_time": "2024-04-20T16:40:19.233395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c323f230024eec96fd553896666dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.9079,  0.9408, -0.0791,  0.0431, -0.3826, -0.4962,  0.5288,  0.7123,\n",
      "         -1.1503,  0.4666, -0.7218,  0.7928,  0.0454,  0.4059,  1.0176]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 27.83 M\n",
      "Number of trainable parameters in millions: 0.01 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "FLOPs: 4.45G, Params: 27.81M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "class ConvNeXtModel(nn.Module):\n",
    "    def __init__(self, num_classes, model_name=\"convnext_tiny\", pretrained=True, fine_tune=False):\n",
    "        super(ConvNeXtModel, self).__init__()\n",
    "        self.convnext_model = create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1\n",
    "        )\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.convnext_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.convnext_model.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convnext_model(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ConvNeXtModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f892e",
   "metadata": {
    "papermill": {
     "duration": 0.038749,
     "end_time": "2024-04-20T16:40:23.048341",
     "exception": false,
     "start_time": "2024-04-20T16:40:23.009592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9342749f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:23.128814Z",
     "iopub.status.busy": "2024-04-20T16:40:23.128097Z",
     "iopub.status.idle": "2024-04-20T16:40:25.257415Z",
     "shell.execute_reply": "2024-04-20T16:40:25.256426Z"
    },
    "papermill": {
     "duration": 2.172153,
     "end_time": "2024-04-20T16:40:25.259932",
     "exception": false,
     "start_time": "2024-04-20T16:40:23.087779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7f163353364617a4e6f5c6c7e2426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/32.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.2303,  0.3872, -0.1187, -0.0159,  0.2146,  0.4215, -0.4641, -0.2141,\n",
      "          0.3579, -0.0363, -0.2009,  0.7574, -0.1098, -0.0292, -0.1578]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 6.96 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 2.75G, Params: 6.88M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "class DenseNetModel(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(DenseNetModel, self).__init__()\n",
    "        self.densenet = create_model(\n",
    "            'densenet121.tv_in1k', \n",
    "            pretrained=pretrained,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1\n",
    "        )\n",
    "\n",
    "        if not fine_tune:\n",
    "            for param in self.densenet.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.densenet.global_pool.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "            for param in self.densenet.head_drop.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "            for param in self.densenet.classifier.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.densenet(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DenseNetModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521697bd",
   "metadata": {
    "papermill": {
     "duration": 0.048809,
     "end_time": "2024-04-20T16:40:25.357118",
     "exception": false,
     "start_time": "2024-04-20T16:40:25.308309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b05b1",
   "metadata": {
    "papermill": {
     "duration": 0.050658,
     "end_time": "2024-04-20T16:40:25.458268",
     "exception": false,
     "start_time": "2024-04-20T16:40:25.407610",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Swin Transformer (512 x 512 not compatible) only 224 or 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac638e11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:25.622831Z",
     "iopub.status.busy": "2024-04-20T16:40:25.621586Z",
     "iopub.status.idle": "2024-04-20T16:40:35.159047Z",
     "shell.execute_reply": "2024-04-20T16:40:35.157818Z"
    },
    "papermill": {
     "duration": 9.590063,
     "end_time": "2024-04-20T16:40:35.161356",
     "exception": false,
     "start_time": "2024-04-20T16:40:25.571293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddf72f20f2f47a9ac4c3e9e1d15af98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/916M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.2104,  0.0284, -0.3546, -0.2533,  0.2120,  0.7453, -0.1186, -0.7598,\n",
      "          0.3066,  0.2589, -0.0577,  0.7392, -0.3076, -0.0241, -0.2682]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 195.01 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 34.06G, Params: 194.92M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "\n",
    "class SwinTransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(SwinTransformerModel, self).__init__()\n",
    "        self.swin = create_model(\n",
    "            'swin_large_patch4_window7_224.ms_in22k', \n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1\n",
    "        )\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.swin.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.swin.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swin(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SwinTransformerModel(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a4359",
   "metadata": {
    "papermill": {
     "duration": 0.040215,
     "end_time": "2024-04-20T16:40:35.242659",
     "exception": false,
     "start_time": "2024-04-20T16:40:35.202444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. MViT (512 x 512 image not supported) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b74ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:35.323173Z",
     "iopub.status.busy": "2024-04-20T16:40:35.322772Z",
     "iopub.status.idle": "2024-04-20T16:40:41.274414Z",
     "shell.execute_reply": "2024-04-20T16:40:41.272872Z"
    },
    "papermill": {
     "duration": 5.995456,
     "end_time": "2024-04-20T16:40:41.277591",
     "exception": false,
     "start_time": "2024-04-20T16:40:35.282135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84964edcfd92439787a353bcfc748ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/206M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.0954,  0.0023, -0.1967, -0.0303, -0.0759, -0.0975,  0.2859, -0.1323,\n",
      "          0.3687,  0.3437, -0.0465,  0.2573, -0.2109, -0.0827,  0.0485]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 50.71 M\n",
      "Number of trainable parameters in millions: 0.01 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 8.83G, Params: 50.48M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "class MViT(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(MViT, self).__init__()\n",
    "        self.mvit = timm.create_model('mvitv2_base.fb_in1k', \n",
    "           pretrained=pretrained, \n",
    "           num_classes=num_classes,\n",
    "            in_chans=1)\n",
    "        \n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.mvit.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.mvit.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mvit(x)\n",
    "    \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MViT(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits    \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96aff0f",
   "metadata": {
    "papermill": {
     "duration": 0.039755,
     "end_time": "2024-04-20T16:40:41.358952",
     "exception": false,
     "start_time": "2024-04-20T16:40:41.319197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. DaViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bcc253f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:41.442945Z",
     "iopub.status.busy": "2024-04-20T16:40:41.442504Z",
     "iopub.status.idle": "2024-04-20T16:40:46.653466Z",
     "shell.execute_reply": "2024-04-20T16:40:46.651819Z"
    },
    "papermill": {
     "duration": 5.255078,
     "end_time": "2024-04-20T16:40:46.656174",
     "exception": false,
     "start_time": "2024-04-20T16:40:41.401096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b705801d7254feb86e26e8dea6ab9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.2269,  0.1547, -0.0736,  0.1810,  0.1348, -0.2158, -0.2421,  0.1566,\n",
      "          0.0277,  0.1135,  0.0463, -0.1975, -0.2434,  0.2884,  0.2682]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 86.93 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "FLOPs: 15.18G, Params: 86.88M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch import nn\n",
    "from timm import create_model\n",
    "\n",
    "\n",
    "class DaViT(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(DaViT, self).__init__()\n",
    "        self.davit = create_model(\n",
    "            'davit_base.msft_in1k', \n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1\n",
    "        )\n",
    "        \n",
    "\n",
    "        if not fine_tune:\n",
    "            for param in self.davit.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.davit.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.davit(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DaViT(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fad5c",
   "metadata": {
    "papermill": {
     "duration": 0.042327,
     "end_time": "2024-04-20T16:40:46.741782",
     "exception": false,
     "start_time": "2024-04-20T16:40:46.699455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. PVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "859a4884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:46.990613Z",
     "iopub.status.busy": "2024-04-20T16:40:46.989498Z",
     "iopub.status.idle": "2024-04-20T16:40:56.355862Z",
     "shell.execute_reply": "2024-04-20T16:40:56.354583Z"
    },
    "papermill": {
     "duration": 9.571555,
     "end_time": "2024-04-20T16:40:56.358157",
     "exception": false,
     "start_time": "2024-04-20T16:40:46.786602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bad69ebac44d43a24c03f4fd668658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.8003,  0.2739, -0.2441,  0.9140,  0.2193,  0.1956, -0.4293, -0.3385,\n",
      "         -0.1421, -0.2328, -0.8976,  0.5361, -0.1937,  0.5554, -0.2802]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 81.44 M\n",
      "Number of trainable parameters in millions: 0.01 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "FLOPs: 11.33G, Params: 81.38M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PVT(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(PVT, self).__init__()\n",
    "        self.pvt = timm.create_model('pvt_v2_b5', \n",
    "           pretrained=pretrained, \n",
    "           num_classes=num_classes,\n",
    "           in_chans=1)\n",
    "        \n",
    "#         model.pvt.head_drop\n",
    "#         model.pvt.head\n",
    "\n",
    "        if not fine_tune:\n",
    "            for param in self.pvt.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.pvt.head_drop.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            for param in self.pvt.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pvt(x)\n",
    "\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PVT(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2b1c8",
   "metadata": {
    "papermill": {
     "duration": 0.043009,
     "end_time": "2024-04-20T16:40:56.443371",
     "exception": false,
     "start_time": "2024-04-20T16:40:56.400362",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. GC ViT (Not pretrained) 512 compatible version is very time consuming [1 hidden cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9507f275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:56.528980Z",
     "iopub.status.busy": "2024-04-20T16:40:56.528312Z",
     "iopub.status.idle": "2024-04-20T16:40:56.675148Z",
     "shell.execute_reply": "2024-04-20T16:40:56.674206Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.19289,
     "end_time": "2024-04-20T16:40:56.677676",
     "exception": false,
     "start_time": "2024-04-20T16:40:56.484786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "# written by Ali Hatamizadeh and Pavlo Molchanov from NVResearch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "from timm.models._registry import register_model\n",
    "from timm.models._builder import build_model_with_cfg\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {'url': url,\n",
    "            'num_classes': num_classes, # set num_classes here\n",
    "            'input_size': (1, IMAGE_SIZE[0], IMAGE_SIZE[1]),  # adjust input image size here\n",
    "            'pool_size': None,\n",
    "            'crop_pct': 0.875, \n",
    "            'interpolation': 'bicubic', \n",
    "            'fixed_input_size': True,\n",
    "            'mean': (0.485, 0.456, 0.406), \n",
    "            'std': (0.229, 0.224, 0.225),\n",
    "            **kwargs\n",
    "            }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    'gc_vit_xxtiny': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_xxtiny.pth.tar',\n",
    "                          crop_pct=1.0, \n",
    "                          input_size=(3, 224, 224), \n",
    "                          crop_mode= 'center'),\n",
    "    'gc_vit_xtiny': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_xtiny.pth.tar',\n",
    "                         crop_pct=0.875, \n",
    "                         input_size=(3, 224, 224), \n",
    "                         crop_mode= 'center'),\n",
    "    'gc_vit_tiny': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_tiny.pth.tar',\n",
    "                        crop_pct=1.0, \n",
    "                        input_size=(3, 224, 224), \n",
    "                        crop_mode= 'center'),\n",
    "    'gc_vit_tiny2': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_tiny2.pth.tar',\n",
    "                         crop_pct=0.875, \n",
    "                         input_size=(3, 224, 224), \n",
    "                         crop_mode= 'center'),\n",
    "    'gc_vit_small': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_small.pth.tar',\n",
    "                         crop_pct=0.875, \n",
    "                         input_size=(3, 224, 224), \n",
    "                         crop_mode= 'center'),\n",
    "    'gc_vit_small2': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_small2.pth.tar',\n",
    "                          crop_pct=0.875, \n",
    "                          input_size=(3, 224, 224), \n",
    "                          crop_mode= 'center'),\n",
    "    'gc_vit_base': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_base.pth.tar',\n",
    "                        crop_pct=1.0, \n",
    "                        input_size=(3, 224, 224), \n",
    "                        crop_mode= 'center'),\n",
    "    'gc_vit_large': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_1k_large.pth.tar',\n",
    "                         crop_pct=1.0, \n",
    "                         input_size=(3, 224, 224), \n",
    "                         crop_mode= 'center'),\n",
    "    'gc_vit_large_224_21k': _cfg(url='https://drive.google.com/uc?export=download&id=1maGDr6mJkLyRTUkspMzCgSlhDzNRFGEf', \n",
    "                                 crop_pct=1.0, \n",
    "                                 input_size=(3, 224, 224), \n",
    "                                 crop_mode= 'center'),\n",
    "    'gc_vit_large_384_21k': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_21k_large_384.pth.tar', \n",
    "                                 crop_pct=1.0, \n",
    "                                 input_size=(3, 384, 384), \n",
    "                                 crop_mode='squash'),\n",
    "    'gc_vit_large_512_21k': _cfg(url='https://huggingface.co/nvidia/GCViT/resolve/main/gcvit_21k_large_512.pth.tar', \n",
    "                                 crop_pct=1.0, \n",
    "                                 input_size=(3, 512, 512), \n",
    "                                 crop_mode='squash'),                             \n",
    "}\n",
    "\n",
    "\n",
    "def _to_channel_last(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, C, H, W)\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    return x.permute(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "def _to_channel_first(x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "\n",
    "    Returns:\n",
    "        x: (B, C, H, W)\n",
    "    \"\"\"\n",
    "    return x.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def window_partition(x, window_size, h_w, w_w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size: window size\n",
    "\n",
    "    Returns:\n",
    "        local window features (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, h_w, window_size, w_w, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W, h_w, w_w, B):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: local window features (num_windows*B, window_size, window_size, C)\n",
    "        window_size: Window size\n",
    "        H: Height of image\n",
    "        W: Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    # B = int(windows.shape[0] // (H * W // window_size // window_size))\n",
    "    x = windows.view(B, h_w, w_w, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron (MLP) block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 hidden_features=None,\n",
    "                 out_features=None,\n",
    "                 act_layer=nn.GELU,\n",
    "                 drop=0.):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: input features dimension.\n",
    "            hidden_features: hidden features dimension.\n",
    "            out_features: output features dimension.\n",
    "            act_layer: activation function.\n",
    "            drop: dropout rate.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze and excitation block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 inp,\n",
    "                 oup,\n",
    "                 expansion=0.25):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inp: input features dimension.\n",
    "            oup: output features dimension.\n",
    "            expansion: expansion ratio.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class ReduceSize(nn.Module):\n",
    "    \"\"\"\n",
    "    Down-sampling block based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 keep_dim=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            norm_layer: normalization layer.\n",
    "            keep_dim: bool argument for maintaining the resolution.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1,\n",
    "                      groups=dim, bias=False),\n",
    "            nn.GELU(),\n",
    "            SE(dim, dim),\n",
    "            nn.Conv2d(dim, dim, 1, 1, 0, bias=False),\n",
    "        )\n",
    "        if keep_dim:\n",
    "            dim_out = dim\n",
    "        else:\n",
    "            dim_out = 2 * dim\n",
    "        self.reduction = nn.Conv2d(dim, dim_out, 3, 2, 1, bias=False)\n",
    "        self.norm2 = norm_layer(dim_out)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous()\n",
    "        x = self.norm1(x)\n",
    "        x = _to_channel_first(x)\n",
    "        x = x + self.conv(x)\n",
    "        x = self.reduction(x)\n",
    "        x = _to_channel_last(x)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch embedding block based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_chans=3, dim=96):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: number of input channels.\n",
    "            dim: feature size dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(in_chans, dim, 3, 2, 1)\n",
    "        self.conv_down = ReduceSize(dim=dim, keep_dim=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = _to_channel_last(x)\n",
    "        x = self.conv_down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatExtract(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature extraction block based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, keep_dim=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            keep_dim: bool argument for maintaining the resolution.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1,\n",
    "                      groups=dim, bias=False),\n",
    "            nn.GELU(),\n",
    "            SE(dim, dim),\n",
    "            nn.Conv2d(dim, dim, 1, 1, 0, bias=False),\n",
    "        )\n",
    "        if not keep_dim:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.keep_dim = keep_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous()\n",
    "        x = x + self.conv(x)\n",
    "        if not self.keep_dim:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Local window attention based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            num_heads: number of attention head.\n",
    "            window_size: window size.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            attn_drop: attention dropout rate.\n",
    "            proj_drop: output dropout rate.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        window_size = (window_size, window_size)\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = torch.div(dim, num_heads, rounding_mode='floor')\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, q_global):\n",
    "        B_, N, C = x.shape\n",
    "        head_dim = torch.div(C, self.num_heads, rounding_mode='floor')\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WindowAttentionGlobal(nn.Module):\n",
    "    \"\"\"\n",
    "    Global window attention based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop=0.,\n",
    "                 proj_drop=0.,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            num_heads: number of attention head.\n",
    "            window_size: window size.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            attn_drop: attention dropout rate.\n",
    "            proj_drop: output dropout rate.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        window_size = (window_size, window_size)\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = torch.div(dim, num_heads, rounding_mode='floor')\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, q_global):\n",
    "        B_, N, C = x.shape\n",
    "        B = q_global.shape[0]\n",
    "        head_dim = torch.div(C, self.num_heads, rounding_mode='floor')\n",
    "        B_dim = torch.div(B_, B, rounding_mode='floor')\n",
    "        kv = self.qkv(x).reshape(B_, N, 2, self.num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "        q_global = q_global.repeat(1, B_dim, 1, 1, 1)\n",
    "        q = q_global.reshape(B_, self.num_heads, N, head_dim)\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCViTBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    GCViT block based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 attention=WindowAttentionGlobal,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 layer_scale=None,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            input_resolution: input image resolution.\n",
    "            num_heads: number of attention head.\n",
    "            window_size: window size.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: drop path rate.\n",
    "            act_layer: activation function.\n",
    "            attention: attention block type.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        self.attn = attention(dim,\n",
    "                              num_heads=num_heads,\n",
    "                              window_size=window_size,\n",
    "                              qkv_bias=qkv_bias,\n",
    "                              qk_scale=qk_scale,\n",
    "                              attn_drop=attn_drop,\n",
    "                              proj_drop=drop,\n",
    "                              )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "        self.layer_scale = False\n",
    "        if layer_scale is not None and type(layer_scale) in [int, float]:\n",
    "            self.layer_scale = True\n",
    "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)\n",
    "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)\n",
    "        else:\n",
    "            self.gamma1 = 1.0\n",
    "            self.gamma2 = 1.0\n",
    "\n",
    "        inp_w = torch.div(input_resolution, window_size, rounding_mode='floor')\n",
    "        self.num_windows = int(inp_w * inp_w)\n",
    "\n",
    "    def forward(self, x, q_global):\n",
    "        B, H, W, C = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        h_w = torch.div(H, self.window_size, rounding_mode='floor')\n",
    "        w_w = torch.div(W, self.window_size, rounding_mode='floor')\n",
    "        x_windows = window_partition(x, self.window_size, h_w, w_w)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "        attn_windows = self.attn(x_windows, q_global)\n",
    "        x = window_reverse(attn_windows, self.window_size, H, W, h_w, w_w, B)\n",
    "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
    "        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalQueryGen(nn.Module):\n",
    "    \"\"\"\n",
    "    Global query generator based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 image_resolution,\n",
    "                 window_size,\n",
    "                 num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            input_resolution: input image resolution.\n",
    "            window_size: window size.\n",
    "            num_heads: number of heads.\n",
    "\n",
    "        For instance, repeating log(56/7) = 3 blocks, with input window dimension 56 and output window dimension 7 at\n",
    "        down-sampling ratio 2. Please check Fig.5 of GC ViT paper for details.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if input_resolution == image_resolution//4:\n",
    "            self.to_q_global = nn.Sequential(\n",
    "                FeatExtract(dim, keep_dim=False),\n",
    "                FeatExtract(dim, keep_dim=False),\n",
    "                FeatExtract(dim, keep_dim=False),\n",
    "            )\n",
    "\n",
    "        elif input_resolution == image_resolution//8:\n",
    "            self.to_q_global = nn.Sequential(\n",
    "                FeatExtract(dim, keep_dim=False),\n",
    "                FeatExtract(dim, keep_dim=False),\n",
    "            )\n",
    "\n",
    "        elif input_resolution == image_resolution//16:\n",
    "\n",
    "            if window_size == input_resolution:\n",
    "                self.to_q_global = nn.Sequential(\n",
    "                    FeatExtract(dim, keep_dim=True)\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                self.to_q_global = nn.Sequential(\n",
    "                    FeatExtract(dim, keep_dim=True)\n",
    "                )\n",
    "\n",
    "        elif input_resolution == image_resolution//32:\n",
    "            self.to_q_global = nn.Sequential(\n",
    "                FeatExtract(dim, keep_dim=True)\n",
    "            )\n",
    "\n",
    "        self.resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.N = window_size * window_size\n",
    "        self.dim_head = torch.div(dim, self.num_heads, rounding_mode='floor')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = _to_channel_last(self.to_q_global(x))\n",
    "        B = x.shape[0]\n",
    "        x = x.reshape(B, 1, self.N, self.num_heads, self.dim_head).permute(0, 1, 3, 2, 4)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GCViTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GCViT layer based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 input_resolution,\n",
    "                 image_resolution,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 downsample=True,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 layer_scale=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            depth: number of layers in each stage.\n",
    "            input_resolution: input image resolution.\n",
    "            window_size: window size in each stage.\n",
    "            downsample: bool argument for down-sampling.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            num_heads: number of heads in each stage.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: drop path rate.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            GCViTBlock(dim=dim,\n",
    "                       num_heads=num_heads,\n",
    "                       window_size=window_size,\n",
    "                       mlp_ratio=mlp_ratio,\n",
    "                       qkv_bias=qkv_bias,\n",
    "                       qk_scale=qk_scale,\n",
    "                       attention=WindowAttention if (i % 2 == 0) else WindowAttentionGlobal,\n",
    "                       drop=drop,\n",
    "                       attn_drop=attn_drop,\n",
    "                       drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                       norm_layer=norm_layer,\n",
    "                       layer_scale=layer_scale,\n",
    "                       input_resolution=input_resolution)\n",
    "            for i in range(depth)])\n",
    "        self.downsample = None if not downsample else ReduceSize(dim=dim, norm_layer=norm_layer)\n",
    "        self.q_global_gen = GlobalQueryGen(dim, input_resolution, image_resolution, window_size, num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q_global = self.q_global_gen(_to_channel_first(x))\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, q_global)\n",
    "        if self.downsample is None:\n",
    "            return x\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class GCViT(nn.Module):\n",
    "    \"\"\"\n",
    "    GCViT based on: \"Hatamizadeh et al.,\n",
    "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depths,\n",
    "                 window_size,\n",
    "                 mlp_ratio,\n",
    "                 num_heads,\n",
    "                 resolution=224,\n",
    "                 drop_path_rate=0.2,\n",
    "                 in_chans=3,\n",
    "                 num_classes=1000,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 layer_scale=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: feature size dimension.\n",
    "            depths: number of layers in each stage.\n",
    "            window_size: window size in each stage.\n",
    "            mlp_ratio: MLP ratio.\n",
    "            num_heads: number of heads in each stage.\n",
    "            resolution: input image resolution.\n",
    "            drop_path_rate: drop path rate.\n",
    "            in_chans: number of input channels.\n",
    "            num_classes: number of classes.\n",
    "            qkv_bias: bool argument for query, key, value learnable bias.\n",
    "            qk_scale: bool argument to scaling query, key.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            norm_layer: normalization layer.\n",
    "            layer_scale: layer scaling coefficient.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        num_features = int(dim * 2 ** (len(depths) - 1))\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_embed = PatchEmbed(in_chans=in_chans, dim=dim)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        self.levels = nn.ModuleList()\n",
    "        for i in range(len(depths)):\n",
    "            level = GCViTLayer(dim=int(dim * 2 ** i),\n",
    "                               depth=depths[i],\n",
    "                               num_heads=num_heads[i],\n",
    "                               window_size=window_size[i],\n",
    "                               mlp_ratio=mlp_ratio,\n",
    "                               qkv_bias=qkv_bias,\n",
    "                               qk_scale=qk_scale,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=(i < len(depths) - 1),\n",
    "                               layer_scale=layer_scale,\n",
    "                               input_resolution=int(2 ** (-2 - i) * resolution),\n",
    "                               image_resolution=resolution)\n",
    "            self.levels.append(level)\n",
    "        self.norm = norm_layer(num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head = nn.Linear(num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'rpb'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for level in self.levels:\n",
    "            x = level(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = _to_channel_first(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def _create_gc_vit(variant, pretrained=False, **kwargs):\n",
    "\n",
    "    return build_model_with_cfg(\n",
    "        GCViT,\n",
    "        variant,\n",
    "        pretrained,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_xxtiny(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
    "    model_kwargs = dict(depths=[2, 2, 6, 2],\n",
    "                        num_heads=[2, 4, 8, 16],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=64,\n",
    "                        mlp_ratio=3,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_xxtiny', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_xtiny(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
    "    model_kwargs = dict(depths=[3, 4, 6, 5],\n",
    "                        num_heads=[2, 4, 8, 16],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=64,\n",
    "                        mlp_ratio=3,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_xtiny', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_tiny(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.2)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[2, 4, 8, 16],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=64,\n",
    "                        mlp_ratio=3,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_tiny', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_tiny2(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.25)\n",
    "    model_kwargs = dict(depths=[3, 4, 29, 5],\n",
    "                        num_heads=[2, 4, 8, 16],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=64,\n",
    "                        mlp_ratio=3,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_tiny2', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_small(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.3)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[3, 6, 12, 24],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=96,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_small', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_small2(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.35)\n",
    "    model_kwargs = dict(depths=[3, 4, 23, 5],\n",
    "                        num_heads=[3, 6, 12, 24],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=96,\n",
    "                        mlp_ratio=3,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_small2', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_base(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.5)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[4, 8, 16, 32],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=128,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    model = _create_gc_vit('gc_vit_base', pretrained=pretrained, **model_kwargs)\n",
    "#     in_features = model.head.in_features \n",
    "#     out_features = 5 # num_classes , set here\n",
    "    \n",
    "#     model.head = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_large(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.5)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[6, 12, 24, 48],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=192,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_large', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_large_224_21k(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.5)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[6, 12, 24, 48],\n",
    "                        window_size=[7, 7, 14, 7],\n",
    "                        dim=192,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_large_224_21k', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_large_384_21k(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.1)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[6, 12, 24, 48],\n",
    "                        window_size=[12, 12, 24, 12],\n",
    "                        dim=192,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_large_384_21k', pretrained=pretrained, **model_kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "@register_model\n",
    "def gc_vit_large_512_21k(pretrained=False, **kwargs) -> GCViT:\n",
    "    drop_path_rate = kwargs.pop(\"drop_path_rate\", 0.1)\n",
    "    model_kwargs = dict(depths=[3, 4, 19, 5],\n",
    "                        num_heads=[6, 12, 24, 48],\n",
    "                        window_size=[16, 16, 32, 16],\n",
    "                        dim=192,\n",
    "                        mlp_ratio=2,\n",
    "                        drop_path_rate=drop_path_rate,\n",
    "                        layer_scale=1e-5,\n",
    "                        **kwargs\n",
    "                        )\n",
    "    model = _create_gc_vit('gc_vit_large_512_21k', pretrained=pretrained, **model_kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7103ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:56.764474Z",
     "iopub.status.busy": "2024-04-20T16:40:56.764075Z",
     "iopub.status.idle": "2024-04-20T16:40:58.839659Z",
     "shell.execute_reply": "2024-04-20T16:40:58.838474Z"
    },
    "papermill": {
     "duration": 2.121702,
     "end_time": "2024-04-20T16:40:58.842200",
     "exception": false,
     "start_time": "2024-04-20T16:40:56.720498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.1446, -0.0298, -0.1130, -0.2961, -0.3105,  0.1483,  0.3173, -0.1134,\n",
      "         -0.0269, -0.2434, -0.1346,  0.1015,  0.0903, -0.4193, -0.3536]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 89.31 M\n",
      "Number of trainable parameters in millions: 89.31 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 13.94G, Params: 89.02M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class GCViT_Model(nn.Module): \n",
    "    def __init__(self, num_classes):\n",
    "        super(GCViT_Model, self).__init__()\n",
    "        # Add a convolutional layer at the top\n",
    "        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Assuming input is grayscale (1 channel)\n",
    "\n",
    "#         self.gcvit = gc_vit_large_512_21k(pretrained=False) # for image_size 512 x 512\n",
    "        self.gcvit = gc_vit_base(pretrained=False) # for image_size 224 x 224\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.gcvit(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "model = GCViT_Model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a639b9",
   "metadata": {
    "papermill": {
     "duration": 0.041893,
     "end_time": "2024-04-20T16:40:58.926604",
     "exception": false,
     "start_time": "2024-04-20T16:40:58.884711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. EfficientViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "012a30e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:59.013101Z",
     "iopub.status.busy": "2024-04-20T16:40:59.012063Z",
     "iopub.status.idle": "2024-04-20T16:40:59.414539Z",
     "shell.execute_reply": "2024-04-20T16:40:59.413397Z"
    },
    "papermill": {
     "duration": 0.44783,
     "end_time": "2024-04-20T16:40:59.416852",
     "exception": false,
     "start_time": "2024-04-20T16:40:58.969022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67646f3a544a48d0ae834019b9a0bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/13.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.5227,  0.4816, -0.1879,  0.4157,  0.4021,  0.2044,  0.2763, -0.0605,\n",
      "         -0.4593, -0.0542, -0.1839, -0.3059, -0.0949, -0.5918, -0.9585]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 2.15 M\n",
      "Number of trainable parameters in millions: 1.47 M\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "FLOPs: 102.47M, Params: 2.15M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# efficientvit_b0.r224_in1k # is very fast \n",
    "# 'efficientvit_l3.r384_in1k # time consuming \n",
    "\n",
    "class EfficientViT(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(EfficientViT, self).__init__()\n",
    "        self.efficientvit = timm.create_model('efficientvit_b0.r224_in1k', \n",
    "           pretrained=pretrained,\n",
    "           num_classes=num_classes,\n",
    "           in_chans=1)\n",
    "        \n",
    "#         model.efficientvit.head\n",
    "        if not fine_tune:\n",
    "            for param in self.efficientvit.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.efficientvit.head.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientvit(x)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientViT(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f6d0f",
   "metadata": {
    "papermill": {
     "duration": 0.041688,
     "end_time": "2024-04-20T16:40:59.502619",
     "exception": false,
     "start_time": "2024-04-20T16:40:59.460931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. MaxViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f3ae758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:40:59.588337Z",
     "iopub.status.busy": "2024-04-20T16:40:59.587926Z",
     "iopub.status.idle": "2024-04-20T16:41:02.548290Z",
     "shell.execute_reply": "2024-04-20T16:41:02.546941Z"
    },
    "papermill": {
     "duration": 3.00591,
     "end_time": "2024-04-20T16:41:02.550648",
     "exception": false,
     "start_time": "2024-04-20T16:40:59.544738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d55ef7d3104ec3958464cead23929f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/124M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.1084,  0.0446,  0.2134, -0.2865,  0.1619, -0.2049,  0.5878, -0.1520,\n",
      "          0.0856,  0.0323,  0.0307, -0.1480, -0.2198,  0.0066,  0.0194]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 30.41 M\n",
      "Number of trainable parameters in millions: 0.27 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "FLOPs: 5.33G, Params: 30.28M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# maxvit_tiny_tf_224.in1k # for image size 224 x 224\n",
    "# maxvit_tiny_tf_512.in1k # for image size 512 x 512\n",
    "\n",
    "\n",
    "\n",
    "class MaxVit(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(MaxVit, self).__init__()\n",
    "        self.maxvit = timm.create_model('maxvit_tiny_tf_224.in1k', \n",
    "           pretrained=pretrained,\n",
    "           num_classes=num_classes,\n",
    "           in_chans=1)\n",
    "        \n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.maxvit.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            for param in self.maxvit.head.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxvit(x)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MaxVit(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17df1d",
   "metadata": {
    "papermill": {
     "duration": 0.043267,
     "end_time": "2024-04-20T16:41:02.637403",
     "exception": false,
     "start_time": "2024-04-20T16:41:02.594136",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Proposed #1 DaViT_Unetr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63822576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:41:02.724711Z",
     "iopub.status.busy": "2024-04-20T16:41:02.723762Z",
     "iopub.status.idle": "2024-04-20T16:41:19.960067Z",
     "shell.execute_reply": "2024-04-20T16:41:19.958643Z"
    },
    "papermill": {
     "duration": 17.282627,
     "end_time": "2024-04-20T16:41:19.962643",
     "exception": false,
     "start_time": "2024-04-20T16:41:02.680016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai\r\n",
      "  Downloading monai-1.3.0-202310121228-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n",
      "Downloading monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: monai\r\n",
      "Successfully installed monai-1.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd2a4aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:41:20.053855Z",
     "iopub.status.busy": "2024-04-20T16:41:20.053449Z",
     "iopub.status.idle": "2024-04-20T16:42:12.859933Z",
     "shell.execute_reply": "2024-04-20T16:42:12.858781Z"
    },
    "papermill": {
     "duration": 52.854817,
     "end_time": "2024-04-20T16:42:12.862319",
     "exception": false,
     "start_time": "2024-04-20T16:41:20.007502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 16:41:57.781980: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-20 16:41:57.782145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-20 16:41:57.874898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0e6a5a8e6340b089a91ca2c0ec3ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/199M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.1955,  0.0616,  0.0698, -0.0919,  0.2435,  0.1863,  0.0584, -0.0458,\n",
      "         -0.0207, -0.0806, -0.0196,  0.0023,  0.0011, -0.0114,  0.1238]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 78.91 M\n",
      "Number of trainable parameters in millions: 29.95 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 27.02G, Params: 78.87M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "\n",
    "\n",
    "class DaViT_UnetR_Model(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(DaViT_UnetR_Model, self).__init__()\n",
    "        \n",
    "#         self.davit = timm.create_model('davit_small.msft_in1k', pretrained=pretrained, num_classes=num_classes)\n",
    "        \n",
    "        self.davit = timm.create_model('davit_small.msft_in1k', pretrained=pretrained, features_only=True, in_chans=1)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.davit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        spatial_dims = 2 \n",
    "        in_channels = 1 # R,G,B\n",
    "        feature_size = 96\n",
    "        norm_name = \"instance\"\n",
    "        hidden_size = 96\n",
    "        res_block = True\n",
    "        conv_block = False\n",
    "\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*2,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*4,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size * 8,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(feature_size, 78, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(78, 50, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Classifier layer with convolution\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2450, 1024),  # (DYNAMIC)Adjust the input size based on the output size of the convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        hidden_states_out = self.davit(x_in) # returns 4 lists\n",
    "#         print(\"Length of hidden states from DaViT:\", len(hidden_states_out))\n",
    "#         for i in hidden_states_out:\n",
    "#             print(i.shape)\n",
    "#         print()\n",
    "\n",
    "\n",
    "        enc1 = self.encoder1(x_in)\n",
    "#         print(\"output from encoder1:\", enc1.shape)\n",
    "        \n",
    "        x2 = hidden_states_out[0]\n",
    "        enc2 = self.encoder2(x2)\n",
    "#         print(\"output from encoder2:\", enc2.shape)\n",
    "        \n",
    "        x3 = hidden_states_out[1]\n",
    "        enc3 = self.encoder3(x3)\n",
    "#         print(\"output from encoder3:\", enc3.shape)\n",
    "        \n",
    "        \n",
    "        x4 = hidden_states_out[2]\n",
    "        enc4 = self.encoder4(x4)\n",
    "#         print(\"output from encoder4:\", enc4.shape)\n",
    "        \n",
    "#         print(\"All encoders OK\\n\")\n",
    "        \n",
    "        dec4 = hidden_states_out[3]\n",
    "#         print(\"Input to decoder5:\", dec4.shape, enc4.shape)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "#         print(\"output from decoder5:\", dec3.shape)\n",
    "        \n",
    "#         print(\"Input to decoder4:\", dec3.shape, enc3.shape)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "#         print(\"output from decoder4:\", dec2.shape)\n",
    "        \n",
    "#         print(\"Input to decoder3:\", dec2.shape, enc2.shape)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "#         print(\"output from decoder3:\", dec1.shape)\n",
    "        \n",
    "#         print(\"Input to decoder2:\", dec1.shape, enc1.shape)\n",
    "        out = self.decoder2(dec1, enc1) \n",
    "#         print(\"output from decoder2:\", out.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        conv_out = self.conv(out)\n",
    "#         print(f\"conv_out_shape:{conv_out.shape}\")\n",
    "\n",
    "        return self.classifier(conv_out)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DaViT_UnetR_Model(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cadbd0",
   "metadata": {
    "papermill": {
     "duration": 0.043014,
     "end_time": "2024-04-20T16:42:12.949770",
     "exception": false,
     "start_time": "2024-04-20T16:42:12.906756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Proposed #2 GWA_DaViT Model (10 cells) # works only with 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6d50bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.040211Z",
     "iopub.status.busy": "2024-04-20T16:42:13.039380Z",
     "iopub.status.idle": "2024-04-20T16:42:13.102681Z",
     "shell.execute_reply": "2024-04-20T16:42:13.101618Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.110954,
     "end_time": "2024-04-20T16:42:13.105243",
     "exception": false,
     "start_time": "2024-04-20T16:42:12.994289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" PyTorch Feature Extraction Helpers\n",
    "\n",
    "A collection of classes, functions, modules to help extract features from models\n",
    "and provide a common interface for describing them.\n",
    "\n",
    "The return_layers, module re-writing idea inspired by torchvision IntermediateLayerGetter\n",
    "https://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "from collections import OrderedDict, defaultdict\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Dict, List, Sequence, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from timm.layers import Format\n",
    "\n",
    "\n",
    "__all__ = ['FeatureInfo', 'FeatureHooks', 'FeatureDictNet', 'FeatureListNet', 'FeatureHookNet']\n",
    "\n",
    "\n",
    "class FeatureInfo:\n",
    "\n",
    "    def __init__(self, feature_info: List[Dict], out_indices: Tuple[int]):\n",
    "        prev_reduction = 1\n",
    "        for i, fi in enumerate(feature_info):\n",
    "            # sanity check the mandatory fields, there may be additional fields depending on the model\n",
    "            assert 'num_chs' in fi and fi['num_chs'] > 0\n",
    "            assert 'reduction' in fi and fi['reduction'] >= prev_reduction\n",
    "            prev_reduction = fi['reduction']\n",
    "            assert 'module' in fi\n",
    "            fi.setdefault('index', i)\n",
    "        self.out_indices = out_indices\n",
    "        self.info = feature_info\n",
    "\n",
    "    def from_other(self, out_indices: Tuple[int]):\n",
    "        return FeatureInfo(deepcopy(self.info), out_indices)\n",
    "\n",
    "    def get(self, key, idx=None):\n",
    "        \"\"\" Get value by key at specified index (indices)\n",
    "        if idx == None, returns value for key at each output index\n",
    "        if idx is an integer, return value for that feature module index (ignoring output indices)\n",
    "        if idx is a list/tupple, return value for each module index (ignoring output indices)\n",
    "        \"\"\"\n",
    "        if idx is None:\n",
    "            return [self.info[i][key] for i in self.out_indices]\n",
    "        if isinstance(idx, (tuple, list)):\n",
    "            return [self.info[i][key] for i in idx]\n",
    "        else:\n",
    "            return self.info[idx][key]\n",
    "\n",
    "    def get_dicts(self, keys=None, idx=None):\n",
    "        \"\"\" return info dicts for specified keys (or all if None) at specified indices (or out_indices if None)\n",
    "        \"\"\"\n",
    "        if idx is None:\n",
    "            if keys is None:\n",
    "                return [self.info[i] for i in self.out_indices]\n",
    "            else:\n",
    "                return [{k: self.info[i][k] for k in keys} for i in self.out_indices]\n",
    "        if isinstance(idx, (tuple, list)):\n",
    "            return [self.info[i] if keys is None else {k: self.info[i][k] for k in keys} for i in idx]\n",
    "        else:\n",
    "            return self.info[idx] if keys is None else {k: self.info[idx][k] for k in keys}\n",
    "\n",
    "    def channels(self, idx=None):\n",
    "        \"\"\" feature channels accessor\n",
    "        \"\"\"\n",
    "        return self.get('num_chs', idx)\n",
    "\n",
    "    def reduction(self, idx=None):\n",
    "        \"\"\" feature reduction (output stride) accessor\n",
    "        \"\"\"\n",
    "        return self.get('reduction', idx)\n",
    "\n",
    "    def module_name(self, idx=None):\n",
    "        \"\"\" feature module name accessor\n",
    "        \"\"\"\n",
    "        return self.get('module', idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.info[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "\n",
    "class FeatureHooks:\n",
    "    \"\"\" Feature Hook Helper\n",
    "\n",
    "    This module helps with the setup and extraction of hooks for extracting features from\n",
    "    internal nodes in a model by node name.\n",
    "\n",
    "    FIXME This works well in eager Python but needs redesign for torchscript.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hooks: Sequence[str],\n",
    "            named_modules: dict,\n",
    "            out_map: Sequence[Union[int, str]] = None,\n",
    "            default_hook_type: str = 'forward',\n",
    "    ):\n",
    "        # setup feature hooks\n",
    "        self._feature_outputs = defaultdict(OrderedDict)\n",
    "        modules = {k: v for k, v in named_modules}\n",
    "        for i, h in enumerate(hooks):\n",
    "            hook_name = h['module']\n",
    "            m = modules[hook_name]\n",
    "            hook_id = out_map[i] if out_map else hook_name\n",
    "            hook_fn = partial(self._collect_output_hook, hook_id)\n",
    "            hook_type = h.get('hook_type', default_hook_type)\n",
    "            if hook_type == 'forward_pre':\n",
    "                m.register_forward_pre_hook(hook_fn)\n",
    "            elif hook_type == 'forward':\n",
    "                m.register_forward_hook(hook_fn)\n",
    "            else:\n",
    "                assert False, \"Unsupported hook type\"\n",
    "\n",
    "    def _collect_output_hook(self, hook_id, *args):\n",
    "        x = args[-1]  # tensor we want is last argument, output for fwd, input for fwd_pre\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # unwrap input tuple\n",
    "        self._feature_outputs[x.device][hook_id] = x\n",
    "\n",
    "    def get_output(self, device) -> Dict[str, torch.tensor]:\n",
    "        output = self._feature_outputs[device]\n",
    "        self._feature_outputs[device] = OrderedDict()  # clear after reading\n",
    "        return output\n",
    "\n",
    "\n",
    "def _module_list(module, flatten_sequential=False):\n",
    "    # a yield/iter would be better for this but wouldn't be compatible with torchscript\n",
    "    ml = []\n",
    "    for name, module in module.named_children():\n",
    "        if flatten_sequential and isinstance(module, nn.Sequential):\n",
    "            # first level of Sequential containers is flattened into containing model\n",
    "            for child_name, child_module in module.named_children():\n",
    "                combined = [name, child_name]\n",
    "                ml.append(('_'.join(combined), '.'.join(combined), child_module))\n",
    "        else:\n",
    "            ml.append((name, name, module))\n",
    "    return ml\n",
    "\n",
    "\n",
    "def _get_feature_info(net, out_indices):\n",
    "    feature_info = getattr(net, 'feature_info')\n",
    "    if isinstance(feature_info, FeatureInfo):\n",
    "        return feature_info.from_other(out_indices)\n",
    "    elif isinstance(feature_info, (list, tuple)):\n",
    "        return FeatureInfo(net.feature_info, out_indices)\n",
    "    else:\n",
    "        assert False, \"Provided feature_info is not valid\"\n",
    "\n",
    "\n",
    "def _get_return_layers(feature_info, out_map):\n",
    "    module_names = feature_info.module_name()\n",
    "    return_layers = {}\n",
    "    for i, name in enumerate(module_names):\n",
    "        return_layers[name] = out_map[i] if out_map is not None else feature_info.out_indices[i]\n",
    "    return return_layers\n",
    "\n",
    "\n",
    "class FeatureDictNet(nn.ModuleDict):\n",
    "    \"\"\" Feature extractor with OrderedDict return\n",
    "\n",
    "    Wrap a model and extract features as specified by the out indices, the network is\n",
    "    partially re-built from contained modules.\n",
    "\n",
    "    There is a strong assumption that the modules have been registered into the model in the same\n",
    "    order as they are used. There should be no reuse of the same nn.Module more than once, including\n",
    "    trivial modules like `self.relu = nn.ReLU`.\n",
    "\n",
    "    Only submodules that are directly assigned to the model class (`model.feature1`) or at most\n",
    "    one Sequential container deep (`model.features.1`, with flatten_sequent=True) can be captured.\n",
    "    All Sequential containers that are directly assigned to the original model will have their\n",
    "    modules assigned to this module with the name `model.features.1` being changed to `model.features_1`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n",
    "            out_map: Sequence[Union[int, str]] = None,\n",
    "            output_fmt: str = 'NCHW',\n",
    "            feature_concat: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            out_map: Return id mapping for each output index, otherwise str(index) is used.\n",
    "            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n",
    "                first element e.g. `x[0]`\n",
    "            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n",
    "        \"\"\"\n",
    "        super(FeatureDictNet, self).__init__()\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        self.output_fmt = Format(output_fmt)\n",
    "        self.concat = feature_concat\n",
    "        self.grad_checkpointing = False\n",
    "        self.return_layers = {}\n",
    "\n",
    "        return_layers = _get_return_layers(self.feature_info, out_map)\n",
    "        modules = _module_list(model, flatten_sequential=flatten_sequential)\n",
    "        remaining = set(return_layers.keys())\n",
    "        layers = OrderedDict()\n",
    "        for new_name, old_name, module in modules:\n",
    "            layers[new_name] = module\n",
    "            if old_name in remaining:\n",
    "                # return id has to be consistently str type for torchscript\n",
    "                self.return_layers[new_name] = str(return_layers[old_name])\n",
    "                remaining.remove(old_name)\n",
    "            if not remaining:\n",
    "                break\n",
    "        assert not remaining and len(self.return_layers) == len(return_layers), \\\n",
    "            f'Return layers ({remaining}) are not present in model'\n",
    "        self.update(layers)\n",
    "\n",
    "    def set_grad_checkpointing(self, enable: bool = True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    def _collect(self, x) -> (Dict[str, torch.Tensor]):\n",
    "        out = OrderedDict()\n",
    "        for i, (name, module) in enumerate(self.items()):\n",
    "            if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                # Skipping checkpoint of first module because need a gradient at input\n",
    "                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n",
    "                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n",
    "                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n",
    "                x = module(x) if first_or_last_module else checkpoint(module, x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "\n",
    "            if name in self.return_layers:\n",
    "                out_id = self.return_layers[name]\n",
    "                if isinstance(x, (tuple, list)):\n",
    "                    # If model tap is a tuple or list, concat or select first element\n",
    "                    # FIXME this may need to be more generic / flexible for some nets\n",
    "                    out[out_id] = torch.cat(x, 1) if self.concat else x[0]\n",
    "                else:\n",
    "                    out[out_id] = x\n",
    "        return out\n",
    "\n",
    "    def forward(self, x) -> Dict[str, torch.Tensor]:\n",
    "        return self._collect(x)\n",
    "\n",
    "\n",
    "class FeatureListNet(FeatureDictNet):\n",
    "    \"\"\" Feature extractor with list return\n",
    "\n",
    "    A specialization of FeatureDictNet that always returns features as a list (values() of dict).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n",
    "            output_fmt: str = 'NCHW',\n",
    "            feature_concat: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n",
    "                first element e.g. `x[0]`\n",
    "            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model,\n",
    "            out_indices=out_indices,\n",
    "            output_fmt=output_fmt,\n",
    "            feature_concat=feature_concat,\n",
    "            flatten_sequential=flatten_sequential,\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> (List[torch.Tensor]):\n",
    "        return list(self._collect(x).values())\n",
    "\n",
    "\n",
    "class FeatureHookNet(nn.ModuleDict):\n",
    "    \"\"\" FeatureHookNet\n",
    "\n",
    "    Wrap a model and extract features specified by the out indices using forward/forward-pre hooks.\n",
    "\n",
    "    If `no_rewrite` is True, features are extracted via hooks without modifying the underlying\n",
    "    network in any way.\n",
    "\n",
    "    If `no_rewrite` is False, the model will be re-written as in the\n",
    "    FeatureList/FeatureDict case by folding first to second (Sequential only) level modules into this one.\n",
    "\n",
    "    FIXME this does not currently work with Torchscript, see FeatureHooks class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: Tuple[int, ...] = (0, 1, 2, 3, 4),\n",
    "            out_map: Sequence[Union[int, str]] = None,\n",
    "            return_dict: bool = False,\n",
    "            output_fmt: str = 'NCHW',\n",
    "            no_rewrite: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "            default_hook_type: str = 'forward',\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            out_map: Return id mapping for each output index, otherwise str(index) is used.\n",
    "            return_dict: Output features as a dict.\n",
    "            no_rewrite: Enforce that model is not re-written if True, ie no modules are removed / changed.\n",
    "                flatten_sequential arg must also be False if this is set True.\n",
    "            flatten_sequential: Re-write modules by flattening first two levels of nn.Sequential containers.\n",
    "            default_hook_type: The default hook type to use if not specified in model.feature_info.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert not torch.jit.is_scripting()\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        self.return_dict = return_dict\n",
    "        self.output_fmt = Format(output_fmt)\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        hooks = []\n",
    "        if no_rewrite:\n",
    "            assert not flatten_sequential\n",
    "            if hasattr(model, 'reset_classifier'):  # make sure classifier is removed?\n",
    "                model.reset_classifier(0)\n",
    "            layers['body'] = model\n",
    "            hooks.extend(self.feature_info.get_dicts())\n",
    "        else:\n",
    "            modules = _module_list(model, flatten_sequential=flatten_sequential)\n",
    "            remaining = {\n",
    "                f['module']: f['hook_type'] if 'hook_type' in f else default_hook_type\n",
    "                for f in self.feature_info.get_dicts()\n",
    "            }\n",
    "            for new_name, old_name, module in modules:\n",
    "                layers[new_name] = module\n",
    "                for fn, fm in module.named_modules(prefix=old_name):\n",
    "                    if fn in remaining:\n",
    "                        hooks.append(dict(module=fn, hook_type=remaining[fn]))\n",
    "                        del remaining[fn]\n",
    "                if not remaining:\n",
    "                    break\n",
    "            assert not remaining, f'Return layers ({remaining}) are not present in model'\n",
    "        self.update(layers)\n",
    "        self.hooks = FeatureHooks(hooks, model.named_modules(), out_map=out_map)\n",
    "\n",
    "    def set_grad_checkpointing(self, enable: bool = True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (name, module) in enumerate(self.items()):\n",
    "            if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                # Skipping checkpoint of first module because need a gradient at input\n",
    "                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n",
    "                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n",
    "                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n",
    "                x = module(x) if first_or_last_module else checkpoint(module, x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        out = self.hooks.get_output(x.device)\n",
    "        return out if self.return_dict else list(out.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d97c517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.192993Z",
     "iopub.status.busy": "2024-04-20T16:42:13.192341Z",
     "iopub.status.idle": "2024-04-20T16:42:13.214261Z",
     "shell.execute_reply": "2024-04-20T16:42:13.213254Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.067537,
     "end_time": "2024-04-20T16:42:13.216422",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.148885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" PyTorch FX Based Feature Extraction Helpers\n",
    "Using https://pytorch.org/vision/stable/feature_extraction.html\n",
    "\"\"\"\n",
    "from typing import Callable, List, Dict, Union, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from ._features import _get_feature_info, _get_return_layers\n",
    "\n",
    "try:\n",
    "    from torchvision.models.feature_extraction import create_feature_extractor as _create_feature_extractor\n",
    "    has_fx_feature_extraction = True\n",
    "except ImportError:\n",
    "    has_fx_feature_extraction = False\n",
    "\n",
    "# Layers we went to treat as leaf modules\n",
    "from timm.layers import Conv2dSame, ScaledStdConv2dSame, CondConv2d, StdConv2dSame\n",
    "from timm.layers.non_local_attn import BilinearAttnTransform\n",
    "from timm.layers.pool2d_same import MaxPool2dSame, AvgPool2dSame\n",
    "from timm.layers.norm_act import (\n",
    "    BatchNormAct2d,\n",
    "    SyncBatchNormAct,\n",
    "    FrozenBatchNormAct2d,\n",
    "    GroupNormAct,\n",
    "    GroupNorm1Act,\n",
    "    LayerNormAct,\n",
    "    LayerNormAct2d\n",
    ")\n",
    "\n",
    "__all__ = ['register_notrace_module', 'is_notrace_module', 'get_notrace_modules',\n",
    "           'register_notrace_function', 'is_notrace_function', 'get_notrace_functions',\n",
    "           'create_feature_extractor', 'FeatureGraphNet', 'GraphExtractNet']\n",
    "\n",
    "\n",
    "# NOTE: By default, any modules from timm.models.layers that we want to treat as leaf modules go here\n",
    "# BUT modules from timm.models should use the registration mechanism below\n",
    "_leaf_modules = {\n",
    "    BilinearAttnTransform,  # reason: flow control t <= 1\n",
    "    # Reason: get_same_padding has a max which raises a control flow error\n",
    "    Conv2dSame, MaxPool2dSame, ScaledStdConv2dSame, StdConv2dSame, AvgPool2dSame,\n",
    "    CondConv2d,  # reason: TypeError: F.conv2d received Proxy in groups=self.groups * B (because B = x.shape[0]),\n",
    "    BatchNormAct2d,\n",
    "    SyncBatchNormAct,\n",
    "    FrozenBatchNormAct2d,\n",
    "    GroupNormAct,\n",
    "    GroupNorm1Act,\n",
    "    LayerNormAct,\n",
    "    LayerNormAct2d,\n",
    "}\n",
    "\n",
    "try:\n",
    "    from timm.layers import InplaceAbn\n",
    "    _leaf_modules.add(InplaceAbn)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def register_notrace_module(module: Type[nn.Module]):\n",
    "    \"\"\"\n",
    "    Any module not under timm.models.layers should get this decorator if we don't want to trace through it.\n",
    "    \"\"\"\n",
    "    _leaf_modules.add(module)\n",
    "    return module\n",
    "\n",
    "\n",
    "def is_notrace_module(module: Type[nn.Module]):\n",
    "    return module in _leaf_modules\n",
    "\n",
    "\n",
    "def get_notrace_modules():\n",
    "    return list(_leaf_modules)\n",
    "\n",
    "\n",
    "# Functions we want to autowrap (treat them as leaves)\n",
    "_autowrap_functions = set()\n",
    "\n",
    "\n",
    "def register_notrace_function(func: Callable):\n",
    "    \"\"\"\n",
    "    Decorator for functions which ought not to be traced through\n",
    "    \"\"\"\n",
    "    _autowrap_functions.add(func)\n",
    "    return func\n",
    "\n",
    "\n",
    "def is_notrace_function(func: Callable):\n",
    "    return func in _autowrap_functions\n",
    "\n",
    "\n",
    "def get_notrace_functions():\n",
    "    return list(_autowrap_functions)\n",
    "\n",
    "\n",
    "def create_feature_extractor(model: nn.Module, return_nodes: Union[Dict[str, str], List[str]]):\n",
    "    assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n",
    "    return _create_feature_extractor(\n",
    "        model, return_nodes,\n",
    "        tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)}\n",
    "    )\n",
    "\n",
    "\n",
    "class FeatureGraphNet(nn.Module):\n",
    "    \"\"\" A FX Graph based feature extractor that works with the model feature_info metadata\n",
    "    \"\"\"\n",
    "    def __init__(self, model, out_indices, out_map=None):\n",
    "        super().__init__()\n",
    "        assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        if out_map is not None:\n",
    "            assert len(out_map) == len(out_indices)\n",
    "        return_nodes = _get_return_layers(self.feature_info, out_map)\n",
    "        self.graph_module = create_feature_extractor(model, return_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return list(self.graph_module(x).values())\n",
    "\n",
    "\n",
    "class GraphExtractNet(nn.Module):\n",
    "    \"\"\" A standalone feature extraction wrapper that maps dict -> list or single tensor\n",
    "    NOTE:\n",
    "      * one can use feature_extractor directly if dictionary output is desired\n",
    "      * unlike FeatureGraphNet, this is intended to be used standalone and not with model feature_info\n",
    "      metadata for builtin feature extraction mode\n",
    "      * create_feature_extractor can be used directly if dictionary output is acceptable\n",
    "\n",
    "    Args:\n",
    "        model: model to extract features from\n",
    "        return_nodes: node names to return features from (dict or list)\n",
    "        squeeze_out: if only one output, and output in list format, flatten to single tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, model, return_nodes: Union[Dict[str, str], List[str]], squeeze_out: bool = True):\n",
    "        super().__init__()\n",
    "        self.squeeze_out = squeeze_out\n",
    "        self.graph_module = create_feature_extractor(model, return_nodes)\n",
    "\n",
    "    def forward(self, x) -> Union[List[torch.Tensor], torch.Tensor]:\n",
    "        out = list(self.graph_module(x).values())\n",
    "        if self.squeeze_out and len(out) == 1:\n",
    "            return out[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09763a69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.305045Z",
     "iopub.status.busy": "2024-04-20T16:42:13.304603Z",
     "iopub.status.idle": "2024-04-20T16:42:13.332933Z",
     "shell.execute_reply": "2024-04-20T16:42:13.331928Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.076289,
     "end_time": "2024-04-20T16:42:13.335101",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.258812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Model creation / weight loading / state_dict helpers\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable, Dict, Optional, Union\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "__all__ = ['clean_state_dict', 'load_state_dict', 'load_checkpoint', 'remap_state_dict', 'resume_checkpoint']\n",
    "\n",
    "\n",
    "def clean_state_dict(state_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    # 'clean' checkpoint by removing .module prefix from state dict if it exists from parallel training\n",
    "    cleaned_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] if k.startswith('module.') else k\n",
    "        cleaned_state_dict[name] = v\n",
    "    return cleaned_state_dict\n",
    "\n",
    "\n",
    "def load_state_dict(\n",
    "        checkpoint_path: str,\n",
    "        use_ema: bool = True,\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    ") -> Dict[str, Any]:\n",
    "    if checkpoint_path and os.path.isfile(checkpoint_path):\n",
    "        # Check if safetensors or not and load weights accordingly\n",
    "        if str(checkpoint_path).endswith(\".safetensors\"):\n",
    "            assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n",
    "            checkpoint = safetensors.torch.load_file(checkpoint_path, device=device)\n",
    "        else:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "        state_dict_key = ''\n",
    "        if isinstance(checkpoint, dict):\n",
    "            if use_ema and checkpoint.get('state_dict_ema', None) is not None:\n",
    "                state_dict_key = 'state_dict_ema'\n",
    "            elif use_ema and checkpoint.get('model_ema', None) is not None:\n",
    "                state_dict_key = 'model_ema'\n",
    "            elif 'state_dict' in checkpoint:\n",
    "                state_dict_key = 'state_dict'\n",
    "            elif 'model' in checkpoint:\n",
    "                state_dict_key = 'model'\n",
    "        state_dict = clean_state_dict(checkpoint[state_dict_key] if state_dict_key else checkpoint)\n",
    "        _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n",
    "        return state_dict\n",
    "    else:\n",
    "        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n",
    "        raise FileNotFoundError()\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "        model: torch.nn.Module,\n",
    "        checkpoint_path: str,\n",
    "        use_ema: bool = True,\n",
    "        device: Union[str, torch.device] = 'cpu',\n",
    "        strict: bool = True,\n",
    "        remap: bool = False,\n",
    "        filter_fn: Optional[Callable] = None,\n",
    "):\n",
    "    if os.path.splitext(checkpoint_path)[-1].lower() in ('.npz', '.npy'):\n",
    "        # numpy checkpoint, try to load via model specific load_pretrained fn\n",
    "        if hasattr(model, 'load_pretrained'):\n",
    "            model.load_pretrained(checkpoint_path)\n",
    "        else:\n",
    "            raise NotImplementedError('Model cannot load numpy checkpoint')\n",
    "        return\n",
    "\n",
    "    state_dict = load_state_dict(checkpoint_path, use_ema, device=device)\n",
    "    if remap:\n",
    "        state_dict = remap_state_dict(state_dict, model)\n",
    "    elif filter_fn:\n",
    "        state_dict = filter_fn(state_dict, model)\n",
    "    incompatible_keys = model.load_state_dict(state_dict, strict=strict)\n",
    "    return incompatible_keys\n",
    "\n",
    "\n",
    "def remap_state_dict(\n",
    "        state_dict: Dict[str, Any],\n",
    "        model: torch.nn.Module,\n",
    "        allow_reshape: bool = True\n",
    "):\n",
    "    \"\"\" remap checkpoint by iterating over state dicts in order (ignoring original keys).\n",
    "    This assumes models (and originating state dict) were created with params registered in same order.\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for (ka, va), (kb, vb) in zip(model.state_dict().items(), state_dict.items()):\n",
    "        assert va.numel() == vb.numel(), f'Tensor size mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'\n",
    "        if va.shape != vb.shape:\n",
    "            if allow_reshape:\n",
    "                vb = vb.reshape(va.shape)\n",
    "            else:\n",
    "                assert False,  f'Tensor shape mismatch {ka}: {va.shape} vs {kb}: {vb.shape}. Remap failed.'\n",
    "        out_dict[ka] = vb\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def resume_checkpoint(\n",
    "        model: torch.nn.Module,\n",
    "        checkpoint_path: str,\n",
    "        optimizer: torch.optim.Optimizer = None,\n",
    "        loss_scaler: Any = None,\n",
    "        log_info: bool = True,\n",
    "):\n",
    "    resume_epoch = None\n",
    "    if os.path.isfile(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:\n",
    "            if log_info:\n",
    "                _logger.info('Restoring model state from checkpoint...')\n",
    "            state_dict = clean_state_dict(checkpoint['state_dict'])\n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "            if optimizer is not None and 'optimizer' in checkpoint:\n",
    "                if log_info:\n",
    "                    _logger.info('Restoring optimizer state from checkpoint...')\n",
    "                optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "            if loss_scaler is not None and loss_scaler.state_dict_key in checkpoint:\n",
    "                if log_info:\n",
    "                    _logger.info('Restoring AMP loss scaler state from checkpoint...')\n",
    "                loss_scaler.load_state_dict(checkpoint[loss_scaler.state_dict_key])\n",
    "\n",
    "            if 'epoch' in checkpoint:\n",
    "                resume_epoch = checkpoint['epoch']\n",
    "                if 'version' in checkpoint and checkpoint['version'] > 1:\n",
    "                    resume_epoch += 1  # start at the next epoch, old checkpoints incremented before save\n",
    "\n",
    "                if log_info:\n",
    "                    _logger.info(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint['epoch']))\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "            if log_info:\n",
    "                _logger.info(\"Loaded checkpoint '{}'\".format(checkpoint_path))\n",
    "        return resume_epoch\n",
    "    else:\n",
    "        _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n",
    "        raise FileNotFoundError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9207a0ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.424515Z",
     "iopub.status.busy": "2024-04-20T16:42:13.424121Z",
     "iopub.status.idle": "2024-04-20T16:42:13.489043Z",
     "shell.execute_reply": "2024-04-20T16:42:13.487913Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.11292,
     "end_time": "2024-04-20T16:42:13.491639",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.378719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Iterable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch.hub import HASH_REGEX, download_url_to_file, urlparse\n",
    "\n",
    "try:\n",
    "    from torch.hub import get_dir\n",
    "except ImportError:\n",
    "    from torch.hub import _get_torch_home as get_dir\n",
    "\n",
    "try:\n",
    "    import safetensors.torch\n",
    "    _has_safetensors = True\n",
    "except ImportError:\n",
    "    _has_safetensors = False\n",
    "\n",
    "try:\n",
    "    from typing import Literal\n",
    "except ImportError:\n",
    "    from typing_extensions import Literal\n",
    "\n",
    "from timm import __version__\n",
    "# from timm.models._pretrained import filter_pretrained_cfg\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import (\n",
    "        create_repo, get_hf_file_metadata,\n",
    "        hf_hub_download, hf_hub_url,\n",
    "        repo_type_and_id_from_hf_id, upload_folder)\n",
    "    from huggingface_hub.utils import EntryNotFoundError\n",
    "    hf_hub_download = partial(hf_hub_download, library_name=\"timm\", library_version=__version__)\n",
    "    _has_hf_hub = True\n",
    "except ImportError:\n",
    "    hf_hub_download = None\n",
    "    _has_hf_hub = False\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "__all__ = ['get_cache_dir', 'download_cached_file', 'has_hf_hub', 'hf_split', 'load_model_config_from_hf',\n",
    "           'load_state_dict_from_hf', 'save_for_hf', 'push_to_hf_hub']\n",
    "\n",
    "# Default name for a weights file hosted on the Huggingface Hub.\n",
    "HF_WEIGHTS_NAME = \"pytorch_model.bin\"  # default pytorch pkl\n",
    "HF_SAFE_WEIGHTS_NAME = \"model.safetensors\"  # safetensors version\n",
    "HF_OPEN_CLIP_WEIGHTS_NAME = \"open_clip_pytorch_model.bin\"  # default pytorch pkl\n",
    "HF_OPEN_CLIP_SAFE_WEIGHTS_NAME = \"open_clip_model.safetensors\"  # safetensors version\n",
    "\n",
    "\n",
    "def get_cache_dir(child_dir=''):\n",
    "    \"\"\"\n",
    "    Returns the location of the directory where models are cached (and creates it if necessary).\n",
    "    \"\"\"\n",
    "    # Issue warning to move data if old env is set\n",
    "    if os.getenv('TORCH_MODEL_ZOO'):\n",
    "        _logger.warning('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n",
    "\n",
    "    hub_dir = get_dir()\n",
    "    child_dir = () if not child_dir else (child_dir,)\n",
    "    model_dir = os.path.join(hub_dir, 'checkpoints', *child_dir)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def download_cached_file(url, check_hash=True, progress=False):\n",
    "    if isinstance(url, (list, tuple)):\n",
    "        url, filename = url\n",
    "    else:\n",
    "        parts = urlparse(url)\n",
    "        filename = os.path.basename(parts.path)\n",
    "    cached_file = os.path.join(get_cache_dir(), filename)\n",
    "    if not os.path.exists(cached_file):\n",
    "        _logger.info('Downloading: \"{}\" to {}\\n'.format(url, cached_file))\n",
    "        hash_prefix = None\n",
    "        if check_hash:\n",
    "            r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n",
    "            hash_prefix = r.group(1) if r else None\n",
    "        download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
    "    return cached_file\n",
    "\n",
    "\n",
    "def check_cached_file(url, check_hash=True):\n",
    "    if isinstance(url, (list, tuple)):\n",
    "        url, filename = url\n",
    "    else:\n",
    "        parts = urlparse(url)\n",
    "        filename = os.path.basename(parts.path)\n",
    "    cached_file = os.path.join(get_cache_dir(), filename)\n",
    "    if os.path.exists(cached_file):\n",
    "        if check_hash:\n",
    "            r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n",
    "            hash_prefix = r.group(1) if r else None\n",
    "            if hash_prefix:\n",
    "                with open(cached_file, 'rb') as f:\n",
    "                    hd = hashlib.sha256(f.read()).hexdigest()\n",
    "                    if hd[:len(hash_prefix)] != hash_prefix:\n",
    "                        return False\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def has_hf_hub(necessary=False):\n",
    "    if not _has_hf_hub and necessary:\n",
    "        # if no HF Hub module installed, and it is necessary to continue, raise error\n",
    "        raise RuntimeError(\n",
    "            'Hugging Face hub model specified but package not installed. Run `pip install huggingface_hub`.')\n",
    "    return _has_hf_hub\n",
    "\n",
    "\n",
    "def hf_split(hf_id: str):\n",
    "    # FIXME I may change @ -> # and be parsed as fragment in a URI model name scheme\n",
    "    rev_split = hf_id.split('@')\n",
    "    assert 0 < len(rev_split) <= 2, 'hf_hub id should only contain one @ character to identify revision.'\n",
    "    hf_model_id = rev_split[0]\n",
    "    hf_revision = rev_split[-1] if len(rev_split) > 1 else None\n",
    "    return hf_model_id, hf_revision\n",
    "\n",
    "\n",
    "def load_cfg_from_json(json_file: Union[str, os.PathLike]):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        text = reader.read()\n",
    "    return json.loads(text)\n",
    "\n",
    "\n",
    "def download_from_hf(model_id: str, filename: str):\n",
    "    hf_model_id, hf_revision = hf_split(model_id)\n",
    "    return hf_hub_download(hf_model_id, filename, revision=hf_revision)\n",
    "\n",
    "\n",
    "def load_model_config_from_hf(model_id: str):\n",
    "    assert has_hf_hub(True)\n",
    "    cached_file = download_from_hf(model_id, 'config.json')\n",
    "\n",
    "    hf_config = load_cfg_from_json(cached_file)\n",
    "    if 'pretrained_cfg' not in hf_config:\n",
    "        # old form, pull pretrain_cfg out of the base dict\n",
    "        pretrained_cfg = hf_config\n",
    "        hf_config = {}\n",
    "        hf_config['architecture'] = pretrained_cfg.pop('architecture')\n",
    "        hf_config['num_features'] = pretrained_cfg.pop('num_features', None)\n",
    "        if 'labels' in pretrained_cfg:  # deprecated name for 'label_names'\n",
    "            pretrained_cfg['label_names'] = pretrained_cfg.pop('labels')\n",
    "        hf_config['pretrained_cfg'] = pretrained_cfg\n",
    "\n",
    "    # NOTE currently discarding parent config as only arch name and pretrained_cfg used in timm right now\n",
    "    pretrained_cfg = hf_config['pretrained_cfg']\n",
    "    pretrained_cfg['hf_hub_id'] = model_id  # insert hf_hub id for pretrained weight load during model creation\n",
    "    pretrained_cfg['source'] = 'hf-hub'\n",
    "\n",
    "    # model should be created with base config num_classes if its exist\n",
    "    if 'num_classes' in hf_config:\n",
    "        pretrained_cfg['num_classes'] = hf_config['num_classes']\n",
    "\n",
    "    # label meta-data in base config overrides saved pretrained_cfg on load\n",
    "    if 'label_names' in hf_config:\n",
    "        pretrained_cfg['label_names'] = hf_config.pop('label_names')\n",
    "    if 'label_descriptions' in hf_config:\n",
    "        pretrained_cfg['label_descriptions'] = hf_config.pop('label_descriptions')\n",
    "\n",
    "    model_args = hf_config.get('model_args', {})\n",
    "    model_name = hf_config['architecture']\n",
    "    return pretrained_cfg, model_name, model_args\n",
    "\n",
    "\n",
    "def load_state_dict_from_hf(model_id: str, filename: str = HF_WEIGHTS_NAME):\n",
    "    assert has_hf_hub(True)\n",
    "    hf_model_id, hf_revision = hf_split(model_id)\n",
    "\n",
    "    # Look for .safetensors alternatives and load from it if it exists\n",
    "    if _has_safetensors:\n",
    "        for safe_filename in _get_safe_alternatives(filename):\n",
    "            try:\n",
    "                cached_safe_file = hf_hub_download(repo_id=hf_model_id, filename=safe_filename, revision=hf_revision)\n",
    "                _logger.info(\n",
    "                    f\"[{model_id}] Safe alternative available for '{filename}' \"\n",
    "                    f\"(as '{safe_filename}'). Loading weights using safetensors.\")\n",
    "                return safetensors.torch.load_file(cached_safe_file, device=\"cpu\")\n",
    "            except EntryNotFoundError:\n",
    "                pass\n",
    "\n",
    "    # Otherwise, load using pytorch.load\n",
    "    cached_file = hf_hub_download(hf_model_id, filename=filename, revision=hf_revision)\n",
    "    _logger.debug(f\"[{model_id}] Safe alternative not found for '{filename}'. Loading weights using default pytorch.\")\n",
    "    return torch.load(cached_file, map_location='cpu')\n",
    "\n",
    "\n",
    "def save_config_for_hf(\n",
    "        model,\n",
    "        config_path: str,\n",
    "        model_config: Optional[dict] = None,\n",
    "        model_args: Optional[dict] = None\n",
    "):\n",
    "    model_config = model_config or {}\n",
    "    hf_config = {}\n",
    "    pretrained_cfg = filter_pretrained_cfg(model.pretrained_cfg, remove_source=True, remove_null=True)\n",
    "    # set some values at root config level\n",
    "    hf_config['architecture'] = pretrained_cfg.pop('architecture')\n",
    "    hf_config['num_classes'] = model_config.pop('num_classes', model.num_classes)\n",
    "\n",
    "    # NOTE these attr saved for informational purposes, do not impact model build\n",
    "    hf_config['num_features'] = model_config.pop('num_features', model.num_features)\n",
    "    global_pool_type = model_config.pop('global_pool', getattr(model, 'global_pool', None))\n",
    "    if isinstance(global_pool_type, str) and global_pool_type:\n",
    "        hf_config['global_pool'] = global_pool_type\n",
    "\n",
    "    # Save class label info\n",
    "    if 'labels' in model_config:\n",
    "        _logger.warning(\n",
    "            \"'labels' as a config field for is deprecated. Please use 'label_names' and 'label_descriptions'.\"\n",
    "            \" Renaming provided 'labels' field to 'label_names'.\")\n",
    "        model_config.setdefault('label_names', model_config.pop('labels'))\n",
    "\n",
    "    label_names = model_config.pop('label_names', None)\n",
    "    if label_names:\n",
    "        assert isinstance(label_names, (dict, list, tuple))\n",
    "        # map label id (classifier index) -> unique label name (ie synset for ImageNet, MID for OpenImages)\n",
    "        # can be a dict id: name if there are id gaps, or tuple/list if no gaps.\n",
    "        hf_config['label_names'] = label_names\n",
    "\n",
    "    label_descriptions = model_config.pop('label_descriptions', None)\n",
    "    if label_descriptions:\n",
    "        assert isinstance(label_descriptions, dict)\n",
    "        # maps label names -> descriptions\n",
    "        hf_config['label_descriptions'] = label_descriptions\n",
    "\n",
    "    if model_args:\n",
    "        hf_config['model_args'] = model_args\n",
    "\n",
    "    hf_config['pretrained_cfg'] = pretrained_cfg\n",
    "    hf_config.update(model_config)\n",
    "\n",
    "    with config_path.open('w') as f:\n",
    "        json.dump(hf_config, f, indent=2)\n",
    "\n",
    "\n",
    "def save_for_hf(\n",
    "        model,\n",
    "        save_directory: str,\n",
    "        model_config: Optional[dict] = None,\n",
    "        model_args: Optional[dict] = None,\n",
    "        safe_serialization: Union[bool, Literal[\"both\"]] = False,\n",
    "):\n",
    "    assert has_hf_hub(True)\n",
    "    save_directory = Path(save_directory)\n",
    "    save_directory.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save model weights, either safely (using safetensors), or using legacy pytorch approach or both.\n",
    "    tensors = model.state_dict()\n",
    "    if safe_serialization is True or safe_serialization == \"both\":\n",
    "        assert _has_safetensors, \"`pip install safetensors` to use .safetensors\"\n",
    "        safetensors.torch.save_file(tensors, save_directory / HF_SAFE_WEIGHTS_NAME)\n",
    "    if safe_serialization is False or safe_serialization == \"both\":\n",
    "        torch.save(tensors, save_directory / HF_WEIGHTS_NAME)\n",
    "\n",
    "    config_path = save_directory / 'config.json'\n",
    "    save_config_for_hf(\n",
    "        model,\n",
    "        config_path,\n",
    "        model_config=model_config,\n",
    "        model_args=model_args,\n",
    "    )\n",
    "\n",
    "\n",
    "def push_to_hf_hub(\n",
    "        model: torch.nn.Module,\n",
    "        repo_id: str,\n",
    "        commit_message: str = 'Add model',\n",
    "        token: Optional[str] = None,\n",
    "        revision: Optional[str] = None,\n",
    "        private: bool = False,\n",
    "        create_pr: bool = False,\n",
    "        model_config: Optional[dict] = None,\n",
    "        model_card: Optional[dict] = None,\n",
    "        model_args: Optional[dict] = None,\n",
    "        safe_serialization: Union[bool, Literal[\"both\"]] = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        (...)\n",
    "        safe_serialization (`bool` or `\"both\"`, *optional*, defaults to `False`):\n",
    "            Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n",
    "            Can be set to `\"both\"` in order to push both safe and unsafe weights.\n",
    "    \"\"\"\n",
    "    # Create repo if it doesn't exist yet\n",
    "    repo_url = create_repo(repo_id, token=token, private=private, exist_ok=True)\n",
    "\n",
    "    # Infer complete repo_id from repo_url\n",
    "    # Can be different from the input `repo_id` if repo_owner was implicit\n",
    "    _, repo_owner, repo_name = repo_type_and_id_from_hf_id(repo_url)\n",
    "    repo_id = f\"{repo_owner}/{repo_name}\"\n",
    "\n",
    "    # Check if README file already exist in repo\n",
    "    try:\n",
    "        get_hf_file_metadata(hf_hub_url(repo_id=repo_id, filename=\"README.md\", revision=revision))\n",
    "        has_readme = True\n",
    "    except EntryNotFoundError:\n",
    "        has_readme = False\n",
    "\n",
    "    # Dump model and push to Hub\n",
    "    with TemporaryDirectory() as tmpdir:\n",
    "        # Save model weights and config.\n",
    "        save_for_hf(\n",
    "            model,\n",
    "            tmpdir,\n",
    "            model_config=model_config,\n",
    "            model_args=model_args,\n",
    "            safe_serialization=safe_serialization,\n",
    "        )\n",
    "\n",
    "        # Add readme if it does not exist\n",
    "        if not has_readme:\n",
    "            model_card = model_card or {}\n",
    "            model_name = repo_id.split('/')[-1]\n",
    "            readme_path = Path(tmpdir) / \"README.md\"\n",
    "            readme_text = generate_readme(model_card, model_name)\n",
    "            readme_path.write_text(readme_text)\n",
    "\n",
    "        # Upload model and return\n",
    "        return upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=tmpdir,\n",
    "            revision=revision,\n",
    "            create_pr=create_pr,\n",
    "            commit_message=commit_message,\n",
    "        )\n",
    "\n",
    "\n",
    "def generate_readme(model_card: dict, model_name: str):\n",
    "    readme_text = \"---\\n\"\n",
    "    readme_text += \"tags:\\n- image-classification\\n- timm\\n\"\n",
    "    readme_text += \"library_name: timm\\n\"\n",
    "    readme_text += f\"license: {model_card.get('license', 'apache-2.0')}\\n\"\n",
    "    if 'details' in model_card and 'Dataset' in model_card['details']:\n",
    "        readme_text += 'datasets:\\n'\n",
    "        if isinstance(model_card['details']['Dataset'], (tuple, list)):\n",
    "            for d in model_card['details']['Dataset']:\n",
    "                readme_text += f\"- {d.lower()}\\n\"\n",
    "        else:\n",
    "            readme_text += f\"- {model_card['details']['Dataset'].lower()}\\n\"\n",
    "        if 'Pretrain Dataset' in model_card['details']:\n",
    "            if isinstance(model_card['details']['Pretrain Dataset'], (tuple, list)):\n",
    "                for d in model_card['details']['Pretrain Dataset']:\n",
    "                    readme_text += f\"- {d.lower()}\\n\"\n",
    "            else:\n",
    "                readme_text += f\"- {model_card['details']['Pretrain Dataset'].lower()}\\n\"\n",
    "    readme_text += \"---\\n\"\n",
    "    readme_text += f\"# Model card for {model_name}\\n\"\n",
    "    if 'description' in model_card:\n",
    "        readme_text += f\"\\n{model_card['description']}\\n\"\n",
    "    if 'details' in model_card:\n",
    "        readme_text += f\"\\n## Model Details\\n\"\n",
    "        for k, v in model_card['details'].items():\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                readme_text += f\"- **{k}:**\\n\"\n",
    "                for vi in v:\n",
    "                    readme_text += f\"  - {vi}\\n\"\n",
    "            elif isinstance(v, dict):\n",
    "                readme_text += f\"- **{k}:**\\n\"\n",
    "                for ki, vi in v.items():\n",
    "                    readme_text += f\"  - {ki}: {vi}\\n\"\n",
    "            else:\n",
    "                readme_text += f\"- **{k}:** {v}\\n\"\n",
    "    if 'usage' in model_card:\n",
    "        readme_text += f\"\\n## Model Usage\\n\"\n",
    "        readme_text += model_card['usage']\n",
    "        readme_text += '\\n'\n",
    "\n",
    "    if 'comparison' in model_card:\n",
    "        readme_text += f\"\\n## Model Comparison\\n\"\n",
    "        readme_text += model_card['comparison']\n",
    "        readme_text += '\\n'\n",
    "\n",
    "    if 'citation' in model_card:\n",
    "        readme_text += f\"\\n## Citation\\n\"\n",
    "        if not isinstance(model_card['citation'], (list, tuple)):\n",
    "            citations = [model_card['citation']]\n",
    "        else:\n",
    "            citations = model_card['citation']\n",
    "        for c in citations:\n",
    "            readme_text += f\"```bibtex\\n{c}\\n```\\n\"\n",
    "    return readme_text\n",
    "\n",
    "\n",
    "def _get_safe_alternatives(filename: str) -> Iterable[str]:\n",
    "    \"\"\"Returns potential safetensors alternatives for a given filename.\n",
    "\n",
    "    Use case:\n",
    "        When downloading a model from the Huggingface Hub, we first look if a .safetensors file exists and if yes, we use it.\n",
    "        Main use case is filename \"pytorch_model.bin\" => check for \"model.safetensors\" or \"pytorch_model.safetensors\".\n",
    "    \"\"\"\n",
    "    if filename == HF_WEIGHTS_NAME:\n",
    "        yield HF_SAFE_WEIGHTS_NAME\n",
    "    if filename == HF_OPEN_CLIP_WEIGHTS_NAME:\n",
    "        yield HF_OPEN_CLIP_SAFE_WEIGHTS_NAME\n",
    "    if filename not in (HF_WEIGHTS_NAME, HF_OPEN_CLIP_WEIGHTS_NAME) and filename.endswith(\".bin\"):\n",
    "        yield filename[:-4] + \".safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46918e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.581249Z",
     "iopub.status.busy": "2024-04-20T16:42:13.580867Z",
     "iopub.status.idle": "2024-04-20T16:42:13.628171Z",
     "shell.execute_reply": "2024-04-20T16:42:13.627243Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.095006,
     "end_time": "2024-04-20T16:42:13.630369",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.535363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from typing import Any, Callable, Dict, Iterator, Tuple, Type, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "__all__ = ['model_parameters', 'named_apply', 'named_modules', 'named_modules_with_params', 'adapt_input_conv',\n",
    "           'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq']\n",
    "\n",
    "\n",
    "def model_parameters(model: nn.Module, exclude_head: bool = False):\n",
    "    if exclude_head:\n",
    "        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n",
    "        return [p for p in model.parameters()][:-2]\n",
    "    else:\n",
    "        return model.parameters()\n",
    "\n",
    "\n",
    "def named_apply(\n",
    "        fn: Callable,\n",
    "        module: nn.Module, name='',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    ") -> nn.Module:\n",
    "    if not depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    return module\n",
    "\n",
    "\n",
    "def named_modules(\n",
    "        module: nn.Module,\n",
    "        name: str = '',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    "):\n",
    "    if not depth_first and include_root:\n",
    "        yield name, module\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        yield from named_modules(\n",
    "            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        yield name, module\n",
    "\n",
    "\n",
    "def named_modules_with_params(\n",
    "        module: nn.Module,\n",
    "        name: str = '',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    "):\n",
    "    if module._parameters and not depth_first and include_root:\n",
    "        yield name, module\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        yield from named_modules_with_params(\n",
    "            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if module._parameters and depth_first and include_root:\n",
    "        yield name, module\n",
    "\n",
    "\n",
    "MATCH_PREV_GROUP = (99999,)\n",
    "\n",
    "\n",
    "def group_with_matcher(\n",
    "        named_objects: Iterator[Tuple[str, Any]],\n",
    "        group_matcher: Union[Dict, Callable],\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False\n",
    "):\n",
    "    if isinstance(group_matcher, dict):\n",
    "        # dictionary matcher contains a dict of raw-string regex expr that must be compiled\n",
    "        compiled = []\n",
    "        for group_ordinal, (group_name, mspec) in enumerate(group_matcher.items()):\n",
    "            if mspec is None:\n",
    "                continue\n",
    "            # map all matching specifications into 3-tuple (compiled re, prefix, suffix)\n",
    "            if isinstance(mspec, (tuple, list)):\n",
    "                # multi-entry match specifications require each sub-spec to be a 2-tuple (re, suffix)\n",
    "                for sspec in mspec:\n",
    "                    compiled += [(re.compile(sspec[0]), (group_ordinal,), sspec[1])]\n",
    "            else:\n",
    "                compiled += [(re.compile(mspec), (group_ordinal,), None)]\n",
    "        group_matcher = compiled\n",
    "\n",
    "    def _get_grouping(name):\n",
    "        if isinstance(group_matcher, (list, tuple)):\n",
    "            for match_fn, prefix, suffix in group_matcher:\n",
    "                r = match_fn.match(name)\n",
    "                if r:\n",
    "                    parts = (prefix, r.groups(), suffix)\n",
    "                    # map all tuple elem to int for numeric sort, filter out None entries\n",
    "                    return tuple(map(float, chain.from_iterable(filter(None, parts))))\n",
    "            return float('inf'),  # un-matched layers (neck, head) mapped to largest ordinal\n",
    "        else:\n",
    "            ord = group_matcher(name)\n",
    "            if not isinstance(ord, collections.abc.Iterable):\n",
    "                return ord,\n",
    "            return tuple(ord)\n",
    "\n",
    "    # map layers into groups via ordinals (ints or tuples of ints) from matcher\n",
    "    grouping = defaultdict(list)\n",
    "    for k, v in named_objects:\n",
    "        grouping[_get_grouping(k)].append(v if return_values else k)\n",
    "\n",
    "    # remap to integers\n",
    "    layer_id_to_param = defaultdict(list)\n",
    "    lid = -1\n",
    "    for k in sorted(filter(lambda x: x is not None, grouping.keys())):\n",
    "        if lid < 0 or k[-1] != MATCH_PREV_GROUP[0]:\n",
    "            lid += 1\n",
    "        layer_id_to_param[lid].extend(grouping[k])\n",
    "\n",
    "    if reverse:\n",
    "        assert not return_values, \"reverse mapping only sensible for name output\"\n",
    "        # output reverse mapping\n",
    "        param_to_layer_id = {}\n",
    "        for lid, lm in layer_id_to_param.items():\n",
    "            for n in lm:\n",
    "                param_to_layer_id[n] = lid\n",
    "        return param_to_layer_id\n",
    "\n",
    "    return layer_id_to_param\n",
    "\n",
    "\n",
    "def group_parameters(\n",
    "        module: nn.Module,\n",
    "        group_matcher,\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False,\n",
    "):\n",
    "    return group_with_matcher(\n",
    "        module.named_parameters(), group_matcher, return_values=return_values, reverse=reverse)\n",
    "\n",
    "\n",
    "def group_modules(\n",
    "        module: nn.Module,\n",
    "        group_matcher,\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False,\n",
    "):\n",
    "    return group_with_matcher(\n",
    "        named_modules_with_params(module), group_matcher, return_values=return_values, reverse=reverse)\n",
    "\n",
    "\n",
    "def flatten_modules(\n",
    "        named_modules: Iterator[Tuple[str, nn.Module]],\n",
    "        depth: int = 1,\n",
    "        prefix: Union[str, Tuple[str, ...]] = '',\n",
    "        module_types: Union[str, Tuple[Type[nn.Module]]] = 'sequential',\n",
    "):\n",
    "    prefix_is_tuple = isinstance(prefix, tuple)\n",
    "    if isinstance(module_types, str):\n",
    "        if module_types == 'container':\n",
    "            module_types = (nn.Sequential, nn.ModuleList, nn.ModuleDict)\n",
    "        else:\n",
    "            module_types = (nn.Sequential,)\n",
    "    for name, module in named_modules:\n",
    "        if depth and isinstance(module, module_types):\n",
    "            yield from flatten_modules(\n",
    "                module.named_children(),\n",
    "                depth - 1,\n",
    "                prefix=(name,) if prefix_is_tuple else name,\n",
    "                module_types=module_types,\n",
    "            )\n",
    "        else:\n",
    "            if prefix_is_tuple:\n",
    "                name = prefix + (name,)\n",
    "                yield name, module\n",
    "            else:\n",
    "                if prefix:\n",
    "                    name = '.'.join([prefix, name])\n",
    "                yield name, module\n",
    "\n",
    "\n",
    "def checkpoint_seq(\n",
    "        functions,\n",
    "        x,\n",
    "        every=1,\n",
    "        flatten=False,\n",
    "        skip_last=False,\n",
    "        preserve_rng_state=True\n",
    "):\n",
    "    r\"\"\"A helper function for checkpointing sequential models.\n",
    "\n",
    "    Sequential models execute a list of modules/functions in order\n",
    "    (sequentially). Therefore, we can divide such a sequence into segments\n",
    "    and checkpoint each segment. All segments except run in :func:`torch.no_grad`\n",
    "    manner, i.e., not storing the intermediate activations. The inputs of each\n",
    "    checkpointed segment will be saved for re-running the segment in the backward pass.\n",
    "\n",
    "    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n",
    "\n",
    "    .. warning::\n",
    "        Checkpointing currently only supports :func:`torch.autograd.backward`\n",
    "        and only if its `inputs` argument is not passed. :func:`torch.autograd.grad`\n",
    "        is not supported.\n",
    "\n",
    "    .. warning:\n",
    "        At least one of the inputs needs to have :code:`requires_grad=True` if\n",
    "        grads are needed for model inputs, otherwise the checkpointed part of the\n",
    "        model won't have gradients.\n",
    "\n",
    "    Args:\n",
    "        functions: A :class:`torch.nn.Sequential` or the list of modules or functions to run sequentially.\n",
    "        x: A Tensor that is input to :attr:`functions`\n",
    "        every: checkpoint every-n functions (default: 1)\n",
    "        flatten (bool): flatten nn.Sequential of nn.Sequentials\n",
    "        skip_last (bool): skip checkpointing the last function in the sequence if True\n",
    "        preserve_rng_state (bool, optional, default=True):  Omit stashing and restoring\n",
    "            the RNG state during each checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        Output of running :attr:`functions` sequentially on :attr:`*inputs`\n",
    "\n",
    "    Example:\n",
    "        >>> model = nn.Sequential(...)\n",
    "        >>> input_var = checkpoint_seq(model, input_var, every=2)\n",
    "    \"\"\"\n",
    "    def run_function(start, end, functions):\n",
    "        def forward(_x):\n",
    "            for j in range(start, end + 1):\n",
    "                _x = functions[j](_x)\n",
    "            return _x\n",
    "        return forward\n",
    "\n",
    "    if isinstance(functions, torch.nn.Sequential):\n",
    "        functions = functions.children()\n",
    "    if flatten:\n",
    "        functions = chain.from_iterable(functions)\n",
    "    if not isinstance(functions, (tuple, list)):\n",
    "        functions = tuple(functions)\n",
    "\n",
    "    num_checkpointed = len(functions)\n",
    "    if skip_last:\n",
    "        num_checkpointed -= 1\n",
    "    end = -1\n",
    "    for start in range(0, num_checkpointed, every):\n",
    "        end = min(start + every - 1, num_checkpointed - 1)\n",
    "        x = checkpoint(run_function(start, end, functions), x, preserve_rng_state=preserve_rng_state)\n",
    "    if skip_last:\n",
    "        return run_function(end + 1, len(functions) - 1, functions)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def adapt_input_conv(in_chans, conv_weight):\n",
    "    conv_type = conv_weight.dtype\n",
    "    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n",
    "    O, I, J, K = conv_weight.shape\n",
    "    if in_chans == 1:\n",
    "        if I > 3:\n",
    "            assert conv_weight.shape[1] % 3 == 0\n",
    "            # For models with space2depth stems\n",
    "            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)\n",
    "            conv_weight = conv_weight.sum(dim=2, keepdim=False)\n",
    "        else:\n",
    "            conv_weight = conv_weight.sum(dim=1, keepdim=True)\n",
    "    elif in_chans != 3:\n",
    "        if I != 3:\n",
    "            raise NotImplementedError('Weight format not supported by conversion.')\n",
    "        else:\n",
    "            # NOTE this strategy should be better than random init, but there could be other combinations of\n",
    "            # the original RGB input layer weights that'd work better for specific cases.\n",
    "            repeat = int(math.ceil(in_chans / 3))\n",
    "            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n",
    "            conv_weight *= (3 / float(in_chans))\n",
    "    conv_weight = conv_weight.to(conv_type)\n",
    "    return conv_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9b80b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.720153Z",
     "iopub.status.busy": "2024-04-20T16:42:13.719734Z",
     "iopub.status.idle": "2024-04-20T16:42:13.744576Z",
     "shell.execute_reply": "2024-04-20T16:42:13.743580Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.072728,
     "end_time": "2024-04-20T16:42:13.746855",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.674127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass, field, replace, asdict\n",
    "from typing import Any, Deque, Dict, Tuple, Optional, Union\n",
    "\n",
    "\n",
    "__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PretrainedCfg:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # weight source locations\n",
    "    url: Optional[Union[str, Tuple[str, str]]] = None  # remote URL\n",
    "    file: Optional[str] = None  # local / shared filesystem path\n",
    "    state_dict: Optional[Dict[str, Any]] = None  # in-memory state dict\n",
    "    hf_hub_id: Optional[str] = None  # Hugging Face Hub model id ('organization/model')\n",
    "    hf_hub_filename: Optional[str] = None  # Hugging Face Hub filename (overrides default)\n",
    "\n",
    "    source: Optional[str] = None  # source of cfg / weight location used (url, file, hf-hub)\n",
    "    architecture: Optional[str] = None  # architecture variant can be set when not implicit\n",
    "    tag: Optional[str] = None  # pretrained tag of source\n",
    "    custom_load: bool = False  # use custom model specific model.load_pretrained() (ie for npz files)\n",
    "\n",
    "    # input / data config\n",
    "    input_size: Tuple[int, int, int] = (3, 224, 224)\n",
    "    test_input_size: Optional[Tuple[int, int, int]] = None\n",
    "    min_input_size: Optional[Tuple[int, int, int]] = None\n",
    "    fixed_input_size: bool = False\n",
    "    interpolation: str = 'bicubic'\n",
    "    crop_pct: float = 0.875\n",
    "    test_crop_pct: Optional[float] = None\n",
    "    crop_mode: str = 'center'\n",
    "    mean: Tuple[float, ...] = (0.485, 0.456, 0.406)\n",
    "    std: Tuple[float, ...] = (0.229, 0.224, 0.225)\n",
    "\n",
    "    # head / classifier config and meta-data\n",
    "    num_classes: int = 1000\n",
    "    label_offset: Optional[int] = None\n",
    "    label_names: Optional[Tuple[str]] = None\n",
    "    label_descriptions: Optional[Dict[str, str]] = None\n",
    "\n",
    "    # model attributes that vary with above or required for pretrained adaptation\n",
    "    pool_size: Optional[Tuple[int, ...]] = None\n",
    "    test_pool_size: Optional[Tuple[int, ...]] = None\n",
    "    first_conv: Optional[str] = None\n",
    "    classifier: Optional[str] = None\n",
    "\n",
    "    license: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    origin_url: Optional[str] = None\n",
    "    paper_name: Optional[str] = None\n",
    "    paper_ids: Optional[Union[str, Tuple[str]]] = None\n",
    "    notes: Optional[Tuple[str]] = None\n",
    "\n",
    "    @property\n",
    "    def has_weights(self):\n",
    "        return self.url or self.file or self.hf_hub_id\n",
    "\n",
    "    def to_dict(self, remove_source=False, remove_null=True):\n",
    "        return filter_pretrained_cfg(\n",
    "            asdict(self),\n",
    "            remove_source=remove_source,\n",
    "            remove_null=remove_null\n",
    "        )\n",
    "\n",
    "\n",
    "def filter_pretrained_cfg(cfg, remove_source=False, remove_null=True):\n",
    "    filtered_cfg = {}\n",
    "    keep_null = {'pool_size', 'first_conv', 'classifier'}  # always keep these keys, even if none\n",
    "    for k, v in cfg.items():\n",
    "        if remove_source and k in {'url', 'file', 'hf_hub_id', 'hf_hub_id', 'hf_hub_filename', 'source'}:\n",
    "            continue\n",
    "        if remove_null and v is None and k not in keep_null:\n",
    "            continue\n",
    "        filtered_cfg[k] = v\n",
    "    return filtered_cfg\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultCfg:\n",
    "    tags: Deque[str] = field(default_factory=deque)  # priority queue of tags (first is default)\n",
    "    cfgs: Dict[str, PretrainedCfg] = field(default_factory=dict)  # pretrained cfgs by tag\n",
    "    is_pretrained: bool = False  # at least one of the configs has a pretrained source set\n",
    "\n",
    "    @property\n",
    "    def default(self):\n",
    "        return self.cfgs[self.tags[0]]\n",
    "\n",
    "    @property\n",
    "    def default_with_tag(self):\n",
    "        tag = self.tags[0]\n",
    "        return tag, self.cfgs[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24f3bd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.835515Z",
     "iopub.status.busy": "2024-04-20T16:42:13.834538Z",
     "iopub.status.idle": "2024-04-20T16:42:13.859352Z",
     "shell.execute_reply": "2024-04-20T16:42:13.858333Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.072287,
     "end_time": "2024-04-20T16:42:13.861576",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.789289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pkgutil\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch import nn as nn\n",
    "\n",
    "from timm.layers import Conv2dSame, BatchNormAct2d, Linear\n",
    "\n",
    "__all__ = ['extract_layer', 'set_layer', 'adapt_model_from_string', 'adapt_model_from_file']\n",
    "\n",
    "\n",
    "def extract_layer(model, layer):\n",
    "    layer = layer.split('.')\n",
    "    module = model\n",
    "    if hasattr(model, 'module') and layer[0] != 'module':\n",
    "        module = model.module\n",
    "    if not hasattr(model, 'module') and layer[0] == 'module':\n",
    "        layer = layer[1:]\n",
    "    for l in layer:\n",
    "        if hasattr(module, l):\n",
    "            if not l.isdigit():\n",
    "                module = getattr(module, l)\n",
    "            else:\n",
    "                module = module[int(l)]\n",
    "        else:\n",
    "            return module\n",
    "    return module\n",
    "\n",
    "\n",
    "def set_layer(model, layer, val):\n",
    "    layer = layer.split('.')\n",
    "    module = model\n",
    "    if hasattr(model, 'module') and layer[0] != 'module':\n",
    "        module = model.module\n",
    "    lst_index = 0\n",
    "    module2 = module\n",
    "    for l in layer:\n",
    "        if hasattr(module2, l):\n",
    "            if not l.isdigit():\n",
    "                module2 = getattr(module2, l)\n",
    "            else:\n",
    "                module2 = module2[int(l)]\n",
    "            lst_index += 1\n",
    "    lst_index -= 1\n",
    "    for l in layer[:lst_index]:\n",
    "        if not l.isdigit():\n",
    "            module = getattr(module, l)\n",
    "        else:\n",
    "            module = module[int(l)]\n",
    "    l = layer[lst_index]\n",
    "    setattr(module, l, val)\n",
    "\n",
    "\n",
    "def adapt_model_from_string(parent_module, model_string):\n",
    "    separator = '***'\n",
    "    state_dict = {}\n",
    "    lst_shape = model_string.split(separator)\n",
    "    for k in lst_shape:\n",
    "        k = k.split(':')\n",
    "        key = k[0]\n",
    "        shape = k[1][1:-1].split(',')\n",
    "        if shape[0] != '':\n",
    "            state_dict[key] = [int(i) for i in shape]\n",
    "\n",
    "    new_module = deepcopy(parent_module)\n",
    "    for n, m in parent_module.named_modules():\n",
    "        old_module = extract_layer(parent_module, n)\n",
    "        if isinstance(old_module, nn.Conv2d) or isinstance(old_module, Conv2dSame):\n",
    "            if isinstance(old_module, Conv2dSame):\n",
    "                conv = Conv2dSame\n",
    "            else:\n",
    "                conv = nn.Conv2d\n",
    "            s = state_dict[n + '.weight']\n",
    "            in_channels = s[1]\n",
    "            out_channels = s[0]\n",
    "            g = 1\n",
    "            if old_module.groups > 1:\n",
    "                in_channels = out_channels\n",
    "                g = in_channels\n",
    "            new_conv = conv(\n",
    "                in_channels=in_channels, out_channels=out_channels, kernel_size=old_module.kernel_size,\n",
    "                bias=old_module.bias is not None, padding=old_module.padding, dilation=old_module.dilation,\n",
    "                groups=g, stride=old_module.stride)\n",
    "            set_layer(new_module, n, new_conv)\n",
    "        elif isinstance(old_module, BatchNormAct2d):\n",
    "            new_bn = BatchNormAct2d(\n",
    "                state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n",
    "                affine=old_module.affine, track_running_stats=True)\n",
    "            new_bn.drop = old_module.drop\n",
    "            new_bn.act = old_module.act\n",
    "            set_layer(new_module, n, new_bn)\n",
    "        elif isinstance(old_module, nn.BatchNorm2d):\n",
    "            new_bn = nn.BatchNorm2d(\n",
    "                num_features=state_dict[n + '.weight'][0], eps=old_module.eps, momentum=old_module.momentum,\n",
    "                affine=old_module.affine, track_running_stats=True)\n",
    "            set_layer(new_module, n, new_bn)\n",
    "        elif isinstance(old_module, nn.Linear):\n",
    "            # FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?\n",
    "            num_features = state_dict[n + '.weight'][1]\n",
    "            new_fc = Linear(\n",
    "                in_features=num_features, out_features=old_module.out_features, bias=old_module.bias is not None)\n",
    "            set_layer(new_module, n, new_fc)\n",
    "            if hasattr(new_module, 'num_features'):\n",
    "                new_module.num_features = num_features\n",
    "    new_module.eval()\n",
    "    parent_module.eval()\n",
    "\n",
    "    return new_module\n",
    "\n",
    "\n",
    "def adapt_model_from_file(parent_module, model_variant):\n",
    "    adapt_data = pkgutil.get_data(__name__, os.path.join('_pruned', model_variant + '.txt'))\n",
    "    return adapt_model_from_string(parent_module, adapt_data.decode('utf-8').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c7eb1cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:13.952020Z",
     "iopub.status.busy": "2024-04-20T16:42:13.951597Z",
     "iopub.status.idle": "2024-04-20T16:42:14.016560Z",
     "shell.execute_reply": "2024-04-20T16:42:14.015424Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.113722,
     "end_time": "2024-04-20T16:42:14.019201",
     "exception": false,
     "start_time": "2024-04-20T16:42:13.905479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Dict, Callable, Any, Tuple\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from timm.models._features import FeatureListNet, FeatureHookNet\n",
    "from timm.models._features_fx import FeatureGraphNet\n",
    "from timm.models._helpers import load_state_dict\n",
    "from timm.models._hub import has_hf_hub, download_cached_file, check_cached_file, load_state_dict_from_hf\n",
    "from timm.models._manipulate import adapt_input_conv\n",
    "from timm.models._pretrained import PretrainedCfg\n",
    "from timm.models._prune import adapt_model_from_file\n",
    "from timm.models._registry import get_pretrained_cfg\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables for rarely used pretrained checkpoint download progress and hash check.\n",
    "# Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.\n",
    "_DOWNLOAD_PROGRESS = False\n",
    "_CHECK_HASH = False\n",
    "_USE_OLD_CACHE = int(os.environ.get('TIMM_USE_OLD_CACHE', 0)) > 0\n",
    "\n",
    "__all__ = ['set_pretrained_download_progress', 'set_pretrained_check_hash', 'load_custom_pretrained', 'load_pretrained',\n",
    "           'pretrained_cfg_for_features', 'resolve_pretrained_cfg', 'build_model_with_cfg']\n",
    "\n",
    "\n",
    "def _resolve_pretrained_source(pretrained_cfg):\n",
    "    cfg_source = pretrained_cfg.get('source', '')\n",
    "    pretrained_url = pretrained_cfg.get('url', None)\n",
    "    pretrained_file = pretrained_cfg.get('file', None)\n",
    "    pretrained_sd = pretrained_cfg.get('state_dict', None)\n",
    "    hf_hub_id = pretrained_cfg.get('hf_hub_id', None)\n",
    "\n",
    "    # resolve where to load pretrained weights from\n",
    "    load_from = ''\n",
    "    pretrained_loc = ''\n",
    "    if cfg_source == 'hf-hub' and has_hf_hub(necessary=True):\n",
    "        # hf-hub specified as source via model identifier\n",
    "        load_from = 'hf-hub'\n",
    "        assert hf_hub_id\n",
    "        pretrained_loc = hf_hub_id\n",
    "    else:\n",
    "        # default source == timm or unspecified\n",
    "        if pretrained_sd:\n",
    "            # direct state_dict pass through is the highest priority\n",
    "            load_from = 'state_dict'\n",
    "            pretrained_loc = pretrained_sd\n",
    "            assert isinstance(pretrained_loc, dict)\n",
    "        elif pretrained_file:\n",
    "            # file load override is the second-highest priority if set\n",
    "            load_from = 'file'\n",
    "            pretrained_loc = pretrained_file\n",
    "        else:\n",
    "            old_cache_valid = False\n",
    "            if _USE_OLD_CACHE:\n",
    "                # prioritized old cached weights if exists and env var enabled\n",
    "                old_cache_valid = check_cached_file(pretrained_url) if pretrained_url else False\n",
    "            if not old_cache_valid and hf_hub_id and has_hf_hub(necessary=True):\n",
    "                # hf-hub available as alternate weight source in default_cfg\n",
    "                load_from = 'hf-hub'\n",
    "                pretrained_loc = hf_hub_id\n",
    "            elif pretrained_url:\n",
    "                load_from = 'url'\n",
    "                pretrained_loc = pretrained_url\n",
    "\n",
    "    if load_from == 'hf-hub' and pretrained_cfg.get('hf_hub_filename', None):\n",
    "        # if a filename override is set, return tuple for location w/ (hub_id, filename)\n",
    "        pretrained_loc = pretrained_loc, pretrained_cfg['hf_hub_filename']\n",
    "    return load_from, pretrained_loc\n",
    "\n",
    "\n",
    "def set_pretrained_download_progress(enable=True):\n",
    "    \"\"\" Set download progress for pretrained weights on/off (globally). \"\"\"\n",
    "    global _DOWNLOAD_PROGRESS\n",
    "    _DOWNLOAD_PROGRESS = enable\n",
    "\n",
    "\n",
    "def set_pretrained_check_hash(enable=True):\n",
    "    \"\"\" Set hash checking for pretrained weights on/off (globally). \"\"\"\n",
    "    global _CHECK_HASH\n",
    "    _CHECK_HASH = enable\n",
    "\n",
    "\n",
    "def load_custom_pretrained(\n",
    "        model: nn.Module,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        load_fn: Optional[Callable] = None,\n",
    "):\n",
    "    r\"\"\"Loads a custom (read non .pth) weight file\n",
    "\n",
    "    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n",
    "    a passed in custom load fun, or the `load_pretrained` model member fn.\n",
    "\n",
    "    If the object is already present in `model_dir`, it's deserialized and returned.\n",
    "    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n",
    "    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n",
    "\n",
    "    Args:\n",
    "        model: The instantiated model to load weights into\n",
    "        pretrained_cfg (dict): Default pretrained model cfg\n",
    "        load_fn: An external standalone fn that loads weights into provided model, otherwise a fn named\n",
    "            'laod_pretrained' on the model will be called if it exists\n",
    "    \"\"\"\n",
    "    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n",
    "    if not pretrained_cfg:\n",
    "        _logger.warning(\"Invalid pretrained config, cannot load weights.\")\n",
    "        return\n",
    "\n",
    "    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n",
    "    if not load_from:\n",
    "        _logger.warning(\"No pretrained weights exist for this model. Using random initialization.\")\n",
    "        return\n",
    "    if load_from == 'hf-hub':\n",
    "        _logger.warning(\"Hugging Face hub not currently supported for custom load pretrained models.\")\n",
    "    elif load_from == 'url':\n",
    "        pretrained_loc = download_cached_file(\n",
    "            pretrained_loc,\n",
    "            check_hash=_CHECK_HASH,\n",
    "            progress=_DOWNLOAD_PROGRESS,\n",
    "        )\n",
    "\n",
    "    if load_fn is not None:\n",
    "        load_fn(model, pretrained_loc)\n",
    "    elif hasattr(model, 'load_pretrained'):\n",
    "        model.load_pretrained(pretrained_loc)\n",
    "    else:\n",
    "        _logger.warning(\"Valid function to load pretrained weights is not available, using random initialization.\")\n",
    "\n",
    "\n",
    "def load_pretrained(\n",
    "        model: nn.Module,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        num_classes: int = 1000,\n",
    "        in_chans: int = 3,\n",
    "        filter_fn: Optional[Callable] = None,\n",
    "        strict: bool = True,\n",
    "):\n",
    "    \"\"\" Load pretrained checkpoint\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module) : PyTorch model module\n",
    "        pretrained_cfg (Optional[Dict]): configuration for pretrained weights / target dataset\n",
    "        num_classes (int): num_classes for target model\n",
    "        in_chans (int): in_chans for target model\n",
    "        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n",
    "        strict (bool): strict load of checkpoint\n",
    "\n",
    "    \"\"\"\n",
    "    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n",
    "    if not pretrained_cfg:\n",
    "        raise RuntimeError(\"Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.\")\n",
    "\n",
    "    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n",
    "    if load_from == 'state_dict':\n",
    "        _logger.info(f'Loading pretrained weights from state dict')\n",
    "        state_dict = pretrained_loc  # pretrained_loc is the actual state dict for this override\n",
    "    elif load_from == 'file':\n",
    "        _logger.info(f'Loading pretrained weights from file ({pretrained_loc})')\n",
    "        if pretrained_cfg.get('custom_load', False):\n",
    "            model.load_pretrained(pretrained_loc)\n",
    "            return\n",
    "        else:\n",
    "            state_dict = load_state_dict(pretrained_loc)\n",
    "    elif load_from == 'url':\n",
    "        _logger.info(f'Loading pretrained weights from url ({pretrained_loc})')\n",
    "        if pretrained_cfg.get('custom_load', False):\n",
    "            pretrained_loc = download_cached_file(\n",
    "                pretrained_loc,\n",
    "                progress=_DOWNLOAD_PROGRESS,\n",
    "                check_hash=_CHECK_HASH,\n",
    "            )\n",
    "            model.load_pretrained(pretrained_loc)\n",
    "            return\n",
    "        else:\n",
    "            state_dict = load_state_dict_from_url(\n",
    "                pretrained_loc,\n",
    "                map_location='cpu',\n",
    "                progress=_DOWNLOAD_PROGRESS,\n",
    "                check_hash=_CHECK_HASH,\n",
    "            )\n",
    "    elif load_from == 'hf-hub':\n",
    "        _logger.info(f'Loading pretrained weights from Hugging Face hub ({pretrained_loc})')\n",
    "        if isinstance(pretrained_loc, (list, tuple)):\n",
    "            state_dict = load_state_dict_from_hf(*pretrained_loc)\n",
    "        else:\n",
    "            state_dict = load_state_dict_from_hf(pretrained_loc)\n",
    "    else:\n",
    "        model_name = pretrained_cfg.get('architecture', 'this model')\n",
    "        raise RuntimeError(f\"No pretrained weights exist for {model_name}. Use `pretrained=False` for random init.\")\n",
    "\n",
    "    if filter_fn is not None:\n",
    "        try:\n",
    "            state_dict = filter_fn(state_dict, model)\n",
    "        except TypeError as e:\n",
    "            # for backwards compat with filter fn that take one arg\n",
    "            state_dict = filter_fn(state_dict)\n",
    "\n",
    "    input_convs = pretrained_cfg.get('first_conv', None)\n",
    "    if input_convs is not None and in_chans != 3:\n",
    "        if isinstance(input_convs, str):\n",
    "            input_convs = (input_convs,)\n",
    "        for input_conv_name in input_convs:\n",
    "            weight_name = input_conv_name + '.weight'\n",
    "            try:\n",
    "                state_dict[weight_name] = adapt_input_conv(in_chans, state_dict[weight_name])\n",
    "                _logger.info(\n",
    "                    f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)')\n",
    "            except NotImplementedError as e:\n",
    "                del state_dict[weight_name]\n",
    "                strict = False\n",
    "                _logger.warning(\n",
    "                    f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.')\n",
    "\n",
    "    classifiers = pretrained_cfg.get('classifier', None)\n",
    "    label_offset = pretrained_cfg.get('label_offset', 0)\n",
    "    if classifiers is not None:\n",
    "        if isinstance(classifiers, str):\n",
    "            classifiers = (classifiers,)\n",
    "        if num_classes != pretrained_cfg['num_classes']:\n",
    "            for classifier_name in classifiers:\n",
    "                # completely discard fully connected if model num_classes doesn't match pretrained weights\n",
    "                state_dict.pop(classifier_name + '.weight', None)\n",
    "                state_dict.pop(classifier_name + '.bias', None)\n",
    "            strict = False\n",
    "        elif label_offset > 0:\n",
    "            for classifier_name in classifiers:\n",
    "                # special case for pretrained weights with an extra background class in pretrained weights\n",
    "                classifier_weight = state_dict[classifier_name + '.weight']\n",
    "                state_dict[classifier_name + '.weight'] = classifier_weight[label_offset:]\n",
    "                classifier_bias = state_dict[classifier_name + '.bias']\n",
    "                state_dict[classifier_name + '.bias'] = classifier_bias[label_offset:]\n",
    "\n",
    "    load_result = model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_result.missing_keys:\n",
    "        _logger.info(\n",
    "            f'Missing keys ({\", \".join(load_result.missing_keys)}) discovered while loading pretrained weights.'\n",
    "            f' This is expected if model is being adapted.')\n",
    "    if load_result.unexpected_keys:\n",
    "        _logger.warning(\n",
    "            f'Unexpected keys ({\", \".join(load_result.unexpected_keys)}) found while loading pretrained weights.'\n",
    "            f' This may be expected if model is being adapted.')\n",
    "\n",
    "\n",
    "def pretrained_cfg_for_features(pretrained_cfg):\n",
    "    pretrained_cfg = deepcopy(pretrained_cfg)\n",
    "    # remove default pretrained cfg fields that don't have much relevance for feature backbone\n",
    "    to_remove = ('num_classes', 'classifier', 'global_pool')  # add default final pool size?\n",
    "    for tr in to_remove:\n",
    "        pretrained_cfg.pop(tr, None)\n",
    "    return pretrained_cfg\n",
    "\n",
    "\n",
    "def _filter_kwargs(kwargs, names):\n",
    "    if not kwargs or not names:\n",
    "        return\n",
    "    for n in names:\n",
    "        kwargs.pop(n, None)\n",
    "\n",
    "\n",
    "def _update_default_model_kwargs(pretrained_cfg, kwargs, kwargs_filter):\n",
    "    \"\"\" Update the default_cfg and kwargs before passing to model\n",
    "\n",
    "    Args:\n",
    "        pretrained_cfg: input pretrained cfg (updated in-place)\n",
    "        kwargs: keyword args passed to model build fn (updated in-place)\n",
    "        kwargs_filter: keyword arg keys that must be removed before model __init__\n",
    "    \"\"\"\n",
    "    # Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)\n",
    "    default_kwarg_names = ('num_classes', 'global_pool', 'in_chans')\n",
    "    if pretrained_cfg.get('fixed_input_size', False):\n",
    "        # if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size\n",
    "        default_kwarg_names += ('img_size',)\n",
    "\n",
    "    for n in default_kwarg_names:\n",
    "        # for legacy reasons, model __init__args uses img_size + in_chans as separate args while\n",
    "        # pretrained_cfg has one input_size=(C, H ,W) entry\n",
    "        if n == 'img_size':\n",
    "            input_size = pretrained_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[-2:])\n",
    "        elif n == 'in_chans':\n",
    "            input_size = pretrained_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[0])\n",
    "        elif n == 'num_classes':\n",
    "            default_val = pretrained_cfg.get(n, None)\n",
    "            # if default is < 0, don't pass through to model\n",
    "            if default_val is not None and default_val >= 0:\n",
    "                kwargs.setdefault(n, pretrained_cfg[n])\n",
    "        else:\n",
    "            default_val = pretrained_cfg.get(n, None)\n",
    "            if default_val is not None:\n",
    "                kwargs.setdefault(n, pretrained_cfg[n])\n",
    "\n",
    "    # Filter keyword args for task specific model variants (some 'features only' models, etc.)\n",
    "    _filter_kwargs(kwargs, names=kwargs_filter)\n",
    "\n",
    "\n",
    "def resolve_pretrained_cfg(\n",
    "        variant: str,\n",
    "        pretrained_cfg=None,\n",
    "        pretrained_cfg_overlay=None,\n",
    ") -> PretrainedCfg:\n",
    "    model_with_tag = variant\n",
    "    pretrained_tag = None\n",
    "    if pretrained_cfg:\n",
    "        if isinstance(pretrained_cfg, dict):\n",
    "            # pretrained_cfg dict passed as arg, validate by converting to PretrainedCfg\n",
    "            pretrained_cfg = PretrainedCfg(**pretrained_cfg)\n",
    "        elif isinstance(pretrained_cfg, str):\n",
    "            pretrained_tag = pretrained_cfg\n",
    "            pretrained_cfg = None\n",
    "\n",
    "    # fallback to looking up pretrained cfg in model registry by variant identifier\n",
    "    if not pretrained_cfg:\n",
    "        if pretrained_tag:\n",
    "            model_with_tag = '.'.join([variant, pretrained_tag])\n",
    "        pretrained_cfg = get_pretrained_cfg(model_with_tag)\n",
    "\n",
    "    if not pretrained_cfg:\n",
    "        _logger.warning(\n",
    "            f\"No pretrained configuration specified for {model_with_tag} model. Using a default.\"\n",
    "            f\" Please add a config to the model pretrained_cfg registry or pass explicitly.\")\n",
    "        pretrained_cfg = PretrainedCfg()  # instance with defaults\n",
    "\n",
    "    pretrained_cfg_overlay = pretrained_cfg_overlay or {}\n",
    "    if not pretrained_cfg.architecture:\n",
    "        pretrained_cfg_overlay.setdefault('architecture', variant)\n",
    "    pretrained_cfg = dataclasses.replace(pretrained_cfg, **pretrained_cfg_overlay)\n",
    "\n",
    "    return pretrained_cfg\n",
    "\n",
    "\n",
    "def build_model_with_cfg(\n",
    "        model_cls: Callable,\n",
    "        variant: str,\n",
    "        pretrained: bool,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        pretrained_cfg_overlay: Optional[Dict] = None,\n",
    "        model_cfg: Optional[Any] = None,\n",
    "        feature_cfg: Optional[Dict] = None,\n",
    "        pretrained_strict: bool = True,\n",
    "        pretrained_filter_fn: Optional[Callable] = None,\n",
    "        kwargs_filter: Optional[Tuple[str]] = None,\n",
    "        **kwargs,\n",
    "):\n",
    "    \"\"\" Build model with specified default_cfg and optional model_cfg\n",
    "\n",
    "    This helper fn aids in the construction of a model including:\n",
    "      * handling default_cfg and associated pretrained weight loading\n",
    "      * passing through optional model_cfg for models with config based arch spec\n",
    "      * features_only model adaptation\n",
    "      * pruning config / model adaptation\n",
    "\n",
    "    Args:\n",
    "        model_cls (nn.Module): model class\n",
    "        variant (str): model variant name\n",
    "        pretrained (bool): load pretrained weights\n",
    "        pretrained_cfg (dict): model's pretrained weight/task config\n",
    "        model_cfg (Optional[Dict]): model's architecture config\n",
    "        feature_cfg (Optional[Dict]: feature extraction adapter config\n",
    "        pretrained_strict (bool): load pretrained weights strictly\n",
    "        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n",
    "        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n",
    "        **kwargs: model args passed through to model __init__\n",
    "    \"\"\"\n",
    "    pruned = kwargs.pop('pruned', False)\n",
    "    features = False\n",
    "    feature_cfg = feature_cfg or {}\n",
    "\n",
    "    # resolve and update model pretrained config and model kwargs\n",
    "    pretrained_cfg = resolve_pretrained_cfg(\n",
    "        variant,\n",
    "        pretrained_cfg=pretrained_cfg,\n",
    "        pretrained_cfg_overlay=pretrained_cfg_overlay\n",
    "    )\n",
    "\n",
    "    # FIXME converting back to dict, PretrainedCfg use should be propagated further, but not into model\n",
    "    pretrained_cfg = pretrained_cfg.to_dict()\n",
    "\n",
    "    _update_default_model_kwargs(pretrained_cfg, kwargs, kwargs_filter)\n",
    "\n",
    "    # Setup for feature extraction wrapper done at end of this fn\n",
    "    if kwargs.pop('features_only', False):\n",
    "        features = True\n",
    "        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n",
    "        if 'out_indices' in kwargs:\n",
    "            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n",
    "\n",
    "    # Instantiate the model\n",
    "    if model_cfg is None:\n",
    "        model = model_cls(**kwargs)\n",
    "    else:\n",
    "        model = model_cls(cfg=model_cfg, **kwargs)\n",
    "    model.pretrained_cfg = pretrained_cfg\n",
    "    model.default_cfg = model.pretrained_cfg  # alias for backwards compat\n",
    "\n",
    "    if pruned:\n",
    "        model = adapt_model_from_file(model, variant)\n",
    "\n",
    "    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            model,\n",
    "            pretrained_cfg=pretrained_cfg,\n",
    "            num_classes=num_classes_pretrained,\n",
    "            in_chans=kwargs.get('in_chans', 3),\n",
    "            filter_fn=pretrained_filter_fn,\n",
    "            strict=pretrained_strict,\n",
    "        )\n",
    "\n",
    "    # Wrap the model in a feature extraction module if enabled\n",
    "    if features:\n",
    "        feature_cls = FeatureListNet\n",
    "        output_fmt = getattr(model, 'output_fmt', None)\n",
    "        if output_fmt is not None:\n",
    "            feature_cfg.setdefault('output_fmt', output_fmt)\n",
    "        if 'feature_cls' in feature_cfg:\n",
    "            feature_cls = feature_cfg.pop('feature_cls')\n",
    "            if isinstance(feature_cls, str):\n",
    "                feature_cls = feature_cls.lower()\n",
    "                if 'hook' in feature_cls:\n",
    "                    feature_cls = FeatureHookNet\n",
    "                elif feature_cls == 'fx':\n",
    "                    feature_cls = FeatureGraphNet\n",
    "                else:\n",
    "                    assert False, f'Unknown feature class {feature_cls}'\n",
    "        model = feature_cls(model, **feature_cfg)\n",
    "        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back pretrained cfg\n",
    "        model.default_cfg = model.pretrained_cfg  # alias for rename backwards compat (default_cfg -> pretrained_cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "481c21ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:14.111222Z",
     "iopub.status.busy": "2024-04-20T16:42:14.110834Z",
     "iopub.status.idle": "2024-04-20T16:42:14.170345Z",
     "shell.execute_reply": "2024-04-20T16:42:14.169357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.108927,
     "end_time": "2024-04-20T16:42:14.172718",
     "exception": false,
     "start_time": "2024-04-20T16:42:14.063791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Model Registry\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "\n",
    "import fnmatch\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import defaultdict, deque\n",
    "from copy import deepcopy\n",
    "from dataclasses import replace\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Sequence, Union, Tuple\n",
    "\n",
    "# from ._pretrained import PretrainedCfg, DefaultCfg\n",
    "\n",
    "__all__ = [\n",
    "    'split_model_name_tag', 'get_arch_name', 'register_model', 'generate_default_cfgs',\n",
    "    'list_models', 'list_pretrained', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n",
    "    'get_pretrained_cfg_value', 'is_model_pretrained'\n",
    "]\n",
    "\n",
    "_module_to_models: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module\n",
    "_model_to_module: Dict[str, str] = {}  # mapping of model names to module names\n",
    "_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to architecture entrypoint fns\n",
    "_model_has_pretrained: Set[str] = set()  # set of model names that have pretrained weight url present\n",
    "_model_default_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch -> default cfg objects\n",
    "_model_pretrained_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch.tag -> pretrained cfgs\n",
    "_model_with_tags: Dict[str, List[str]] = defaultdict(list)  # shortcut to map each model arch to all model + tag names\n",
    "_module_to_deprecated_models: Dict[str, Dict[str, Optional[str]]] = defaultdict(dict)\n",
    "_deprecated_models: Dict[str, Optional[str]] = {}\n",
    "\n",
    "\n",
    "def split_model_name_tag(model_name: str, no_tag: str = '') -> Tuple[str, str]:\n",
    "    model_name, *tag_list = model_name.split('.', 1)\n",
    "    tag = tag_list[0] if tag_list else no_tag\n",
    "    return model_name, tag\n",
    "\n",
    "\n",
    "def get_arch_name(model_name: str) -> str:\n",
    "    return split_model_name_tag(model_name)[0]\n",
    "\n",
    "\n",
    "def generate_default_cfgs(cfgs: Dict[str, Union[Dict[str, Any], PretrainedCfg]]):\n",
    "    out = defaultdict(DefaultCfg)\n",
    "    default_set = set()  # no tag and tags ending with * are prioritized as default\n",
    "\n",
    "    for k, v in cfgs.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = PretrainedCfg(**v)\n",
    "        has_weights = v.has_weights\n",
    "\n",
    "        model, tag = split_model_name_tag(k)\n",
    "        is_default_set = model in default_set\n",
    "        priority = (has_weights and not tag) or (tag.endswith('*') and not is_default_set)\n",
    "        tag = tag.strip('*')\n",
    "\n",
    "        default_cfg = out[model]\n",
    "\n",
    "        if priority:\n",
    "            default_cfg.tags.appendleft(tag)\n",
    "            default_set.add(model)\n",
    "        elif has_weights and not default_cfg.is_pretrained:\n",
    "            default_cfg.tags.appendleft(tag)\n",
    "        else:\n",
    "            default_cfg.tags.append(tag)\n",
    "\n",
    "        if has_weights:\n",
    "            default_cfg.is_pretrained = True\n",
    "\n",
    "        default_cfg.cfgs[tag] = v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def register_model(fn: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    # lookup containing module\n",
    "    mod = sys.modules[fn.__module__]\n",
    "    module_name_split = fn.__module__.split('.')\n",
    "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
    "\n",
    "    # add model to __all__ in module\n",
    "    model_name = fn.__name__\n",
    "    if hasattr(mod, '__all__'):\n",
    "        mod.__all__.append(model_name)\n",
    "    else:\n",
    "        mod.__all__ = [model_name]  # type: ignore\n",
    "\n",
    "    # add entries to registry dict/sets\n",
    "    if model_name in _model_entrypoints:\n",
    "        warnings.warn(\n",
    "            f'Overwriting {model_name} in registry with {fn.__module__}.{model_name}. This is because the name being '\n",
    "            'registered conflicts with an existing name. Please check if this is not expected.',\n",
    "            stacklevel=2,\n",
    "        )\n",
    "    _model_entrypoints[model_name] = fn\n",
    "    _model_to_module[model_name] = module_name\n",
    "    _module_to_models[module_name].add(model_name)\n",
    "    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:\n",
    "        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n",
    "        # entrypoints or non-matching combos\n",
    "        default_cfg = mod.default_cfgs[model_name]\n",
    "        if not isinstance(default_cfg, DefaultCfg):\n",
    "            # new style default cfg dataclass w/ multiple entries per model-arch\n",
    "            assert isinstance(default_cfg, dict)\n",
    "            # old style cfg dict per model-arch\n",
    "            pretrained_cfg = PretrainedCfg(**default_cfg)\n",
    "            default_cfg = DefaultCfg(tags=deque(['']), cfgs={'': pretrained_cfg})\n",
    "\n",
    "        for tag_idx, tag in enumerate(default_cfg.tags):\n",
    "            is_default = tag_idx == 0\n",
    "            pretrained_cfg = default_cfg.cfgs[tag]\n",
    "            model_name_tag = '.'.join([model_name, tag]) if tag else model_name\n",
    "            replace_items = dict(architecture=model_name, tag=tag if tag else None)\n",
    "            if pretrained_cfg.hf_hub_id and pretrained_cfg.hf_hub_id == 'timm/':\n",
    "                # auto-complete hub name w/ architecture.tag\n",
    "                replace_items['hf_hub_id'] = pretrained_cfg.hf_hub_id + model_name_tag\n",
    "            pretrained_cfg = replace(pretrained_cfg, **replace_items)\n",
    "\n",
    "            if is_default:\n",
    "                _model_pretrained_cfgs[model_name] = pretrained_cfg\n",
    "                if pretrained_cfg.has_weights:\n",
    "                    # add tagless entry if it's default and has weights\n",
    "                    _model_has_pretrained.add(model_name)\n",
    "\n",
    "            if tag:\n",
    "                _model_pretrained_cfgs[model_name_tag] = pretrained_cfg\n",
    "                if pretrained_cfg.has_weights:\n",
    "                    # add model w/ tag if tag is valid\n",
    "                    _model_has_pretrained.add(model_name_tag)\n",
    "                _model_with_tags[model_name].append(model_name_tag)\n",
    "            else:\n",
    "                _model_with_tags[model_name].append(model_name)  # has empty tag (to slowly remove these instances)\n",
    "\n",
    "        _model_default_cfgs[model_name] = default_cfg\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def _deprecated_model_shim(deprecated_name: str, current_fn: Callable = None, current_tag: str = ''):\n",
    "    def _fn(pretrained=False, **kwargs):\n",
    "        assert current_fn is not None,  f'Model {deprecated_name} has been removed with no replacement.'\n",
    "        current_name = '.'.join([current_fn.__name__, current_tag]) if current_tag else current_fn.__name__\n",
    "        warnings.warn(f'Mapping deprecated model name {deprecated_name} to current {current_name}.', stacklevel=2)\n",
    "        pretrained_cfg = kwargs.pop('pretrained_cfg', None)\n",
    "        return current_fn(pretrained=pretrained, pretrained_cfg=pretrained_cfg or current_tag, **kwargs)\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def register_model_deprecations(module_name: str, deprecation_map: Dict[str, Optional[str]]):\n",
    "    mod = sys.modules[module_name]\n",
    "    module_name_split = module_name.split('.')\n",
    "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
    "\n",
    "    for deprecated, current in deprecation_map.items():\n",
    "        if hasattr(mod, '__all__'):\n",
    "            mod.__all__.append(deprecated)\n",
    "        current_fn = None\n",
    "        current_tag = ''\n",
    "        if current:\n",
    "            current_name, current_tag = split_model_name_tag(current)\n",
    "            current_fn = getattr(mod, current_name)\n",
    "        deprecated_entrypoint_fn = _deprecated_model_shim(deprecated, current_fn, current_tag)\n",
    "        setattr(mod, deprecated, deprecated_entrypoint_fn)\n",
    "        _model_entrypoints[deprecated] = deprecated_entrypoint_fn\n",
    "        _model_to_module[deprecated] = module_name\n",
    "        _module_to_models[module_name].add(deprecated)\n",
    "        _deprecated_models[deprecated] = current\n",
    "        _module_to_deprecated_models[module_name][deprecated] = current\n",
    "\n",
    "\n",
    "def _natural_key(string_: str) -> List[Union[int, str]]:\n",
    "    \"\"\"See https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\"\"\"\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n",
    "\n",
    "\n",
    "def _expand_filter(filter: str):\n",
    "    \"\"\" expand a 'base_filter' to 'base_filter.*' if no tag portion\"\"\"\n",
    "    filter_base, filter_tag = split_model_name_tag(filter)\n",
    "    if not filter_tag:\n",
    "        return ['.'.join([filter_base, '*']), filter]\n",
    "    else:\n",
    "        return [filter]\n",
    "\n",
    "\n",
    "def list_models(\n",
    "        filter: Union[str, List[str]] = '',\n",
    "        module: str = '',\n",
    "        pretrained: bool = False,\n",
    "        exclude_filters: Union[str, List[str]] = '',\n",
    "        name_matches_cfg: bool = False,\n",
    "        include_tags: Optional[bool] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\" Return list of available model names, sorted alphabetically\n",
    "\n",
    "    Args:\n",
    "        filter - Wildcard filter string that works with fnmatch\n",
    "        module - Limit model selection to a specific submodule (ie 'vision_transformer')\n",
    "        pretrained - Include only models with valid pretrained weights if True\n",
    "        exclude_filters - Wildcard filters to exclude models after including them with filter\n",
    "        name_matches_cfg - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n",
    "        include_tags - Include pretrained tags in model names (model.tag). If None, defaults\n",
    "            set to True when pretrained=True else False (default: None)\n",
    "\n",
    "    Returns:\n",
    "        models - The sorted list of models\n",
    "\n",
    "    Example:\n",
    "        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n",
    "        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n",
    "    \"\"\"\n",
    "    if filter:\n",
    "        include_filters = filter if isinstance(filter, (tuple, list)) else [filter]\n",
    "    else:\n",
    "        include_filters = []\n",
    "\n",
    "    if include_tags is None:\n",
    "        # FIXME should this be default behaviour? or default to include_tags=True?\n",
    "        include_tags = pretrained\n",
    "\n",
    "    all_models: Set[str] = _module_to_models[module] if module else set(_model_entrypoints.keys())\n",
    "    all_models = all_models - _deprecated_models.keys()  # remove deprecated models from listings\n",
    "\n",
    "    if include_tags:\n",
    "        # expand model names to include names w/ pretrained tags\n",
    "        models_with_tags: Set[str] = set()\n",
    "        for m in all_models:\n",
    "            models_with_tags.update(_model_with_tags[m])\n",
    "        all_models = models_with_tags\n",
    "        # expand include and exclude filters to include a '.*' for proper match if no tags in filter\n",
    "        include_filters = [ef for f in include_filters for ef in _expand_filter(f)]\n",
    "        exclude_filters = [ef for f in exclude_filters for ef in _expand_filter(f)]\n",
    "\n",
    "    if include_filters:\n",
    "        models: Set[str] = set()\n",
    "        for f in include_filters:\n",
    "            include_models = fnmatch.filter(all_models, f)  # include these models\n",
    "            if len(include_models):\n",
    "                models = models.union(include_models)\n",
    "    else:\n",
    "        models = all_models\n",
    "\n",
    "    if exclude_filters:\n",
    "        if not isinstance(exclude_filters, (tuple, list)):\n",
    "            exclude_filters = [exclude_filters]\n",
    "        for xf in exclude_filters:\n",
    "            exclude_models = fnmatch.filter(models, xf)  # exclude these models\n",
    "            if len(exclude_models):\n",
    "                models = models.difference(exclude_models)\n",
    "\n",
    "    if pretrained:\n",
    "        models = _model_has_pretrained.intersection(models)\n",
    "\n",
    "    if name_matches_cfg:\n",
    "        models = set(_model_pretrained_cfgs).intersection(models)\n",
    "\n",
    "    return sorted(models, key=_natural_key)\n",
    "\n",
    "\n",
    "def list_pretrained(\n",
    "        filter: Union[str, List[str]] = '',\n",
    "        exclude_filters: str = '',\n",
    ") -> List[str]:\n",
    "    return list_models(\n",
    "        filter=filter,\n",
    "        pretrained=True,\n",
    "        exclude_filters=exclude_filters,\n",
    "        include_tags=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_deprecated_models(module: str = '') -> Dict[str, str]:\n",
    "    all_deprecated = _module_to_deprecated_models[module] if module else _deprecated_models\n",
    "    return deepcopy(all_deprecated)\n",
    "\n",
    "\n",
    "def is_model(model_name: str) -> bool:\n",
    "    \"\"\" Check if a model name exists\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    return arch_name in _model_entrypoints\n",
    "\n",
    "\n",
    "def model_entrypoint(model_name: str, module_filter: Optional[str] = None) -> Callable[..., Any]:\n",
    "    \"\"\"Fetch a model entrypoint for specified model name\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    if module_filter and arch_name not in _module_to_models.get(module_filter, {}):\n",
    "        raise RuntimeError(f'Model ({model_name} not found in module {module_filter}.')\n",
    "    return _model_entrypoints[arch_name]\n",
    "\n",
    "\n",
    "def list_modules() -> List[str]:\n",
    "    \"\"\" Return list of module names that contain models / model entrypoints\n",
    "    \"\"\"\n",
    "    modules = _module_to_models.keys()\n",
    "    return sorted(modules)\n",
    "\n",
    "\n",
    "def is_model_in_modules(\n",
    "        model_name: str, module_names: Union[Tuple[str, ...], List[str], Set[str]]\n",
    ") -> bool:\n",
    "    \"\"\"Check if a model exists within a subset of modules\n",
    "\n",
    "    Args:\n",
    "        model_name - name of model to check\n",
    "        module_names - names of modules to search in\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    assert isinstance(module_names, (tuple, list, set))\n",
    "    return any(arch_name in _module_to_models[n] for n in module_names)\n",
    "\n",
    "\n",
    "def is_model_pretrained(model_name: str) -> bool:\n",
    "    return model_name in _model_has_pretrained\n",
    "\n",
    "\n",
    "def get_pretrained_cfg(model_name: str, allow_unregistered: bool = True) -> Optional[PretrainedCfg]:\n",
    "    if model_name in _model_pretrained_cfgs:\n",
    "        return deepcopy(_model_pretrained_cfgs[model_name])\n",
    "    arch_name, tag = split_model_name_tag(model_name)\n",
    "    if arch_name in _model_default_cfgs:\n",
    "        # if model arch exists, but the tag is wrong, error out\n",
    "        raise RuntimeError(f'Invalid pretrained tag ({tag}) for {arch_name}.')\n",
    "    if allow_unregistered:\n",
    "        # if model arch doesn't exist, it has no pretrained_cfg registered, allow a default to be created\n",
    "        return None\n",
    "    raise RuntimeError(f'Model architecture ({arch_name}) has no pretrained cfg registered.')\n",
    "\n",
    "\n",
    "def get_pretrained_cfg_value(model_name: str, cfg_key: str) -> Optional[Any]:\n",
    "    \"\"\" Get a specific model default_cfg value by key. None if key doesn't exist.\n",
    "    \"\"\"\n",
    "    cfg = get_pretrained_cfg(model_name, allow_unregistered=False)\n",
    "    return getattr(cfg, cfg_key, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34fa13f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:14.264311Z",
     "iopub.status.busy": "2024-04-20T16:42:14.263763Z",
     "iopub.status.idle": "2024-04-20T16:42:14.392747Z",
     "shell.execute_reply": "2024-04-20T16:42:14.391731Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.177554,
     "end_time": "2024-04-20T16:42:14.395278",
     "exception": false,
     "start_time": "2024-04-20T16:42:14.217724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" DaViT: Dual Attention Vision Transformers\n",
    "\n",
    "As described in https://arxiv.org/abs/2204.03645\n",
    "\n",
    "Input size invariant transformer architecture that combines channel and spacial\n",
    "attention in each block. The attention mechanisms used are linear in complexity.\n",
    "\n",
    "DaViT model defs and weights adapted from https://github.com/dingmyu/davit, original copyright below\n",
    "\n",
    "\"\"\"\n",
    "# Copyright (c) 2022 Mingyu Ding\n",
    "# All rights reserved.\n",
    "# This source code is licensed under the MIT license\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers import DropPath, to_2tuple, trunc_normal_, Mlp, LayerNorm2d, get_norm_layer, use_fused_attn\n",
    "from timm.layers import NormMlpClassifierHead, ClassifierHead, RelPosBias, get_attn\n",
    "# from ._builder import build_model_with_cfg\n",
    "# from ._features_fx import register_notrace_function\n",
    "# from ._manipulate import checkpoint_seq\n",
    "# from ._registry import generate_default_cfgs, register_model\n",
    "\n",
    "__all__ = ['DaVit']\n",
    "\n",
    "\n",
    "class ConvPosEnc(nn.Module):\n",
    "    def __init__(self, dim: int, k: int = 3, act: bool = False):\n",
    "        super(ConvPosEnc, self).__init__()\n",
    "\n",
    "        self.proj = nn.Conv2d(dim, dim, k, 1, k // 2, groups=dim)\n",
    "        self.act = nn.GELU() if act else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        feat = self.proj(x)\n",
    "        x = x + self.act(feat)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "    \"\"\" Size-agnostic implementation of 2D image to patch embedding,\n",
    "        allowing input size to be adjusted during model forward operation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs=3,\n",
    "            out_chs=96,\n",
    "            stride=4,\n",
    "            norm_layer=LayerNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        stride = to_2tuple(stride)\n",
    "        self.stride = stride\n",
    "        self.in_chs = in_chs\n",
    "        self.out_chs = out_chs\n",
    "        assert stride[0] == 4  # only setup for stride==4\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chs,\n",
    "            out_chs,\n",
    "            kernel_size=7,\n",
    "            stride=stride,\n",
    "            padding=3,\n",
    "        )\n",
    "        self.norm = norm_layer(out_chs)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "        x = F.pad(x, (0, (self.stride[1] - W % self.stride[1]) % self.stride[1]))\n",
    "        x = F.pad(x, (0, 0, 0, (self.stride[0] - H % self.stride[0]) % self.stride[0]))\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs,\n",
    "            out_chs,\n",
    "            norm_layer=LayerNorm2d,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_chs = in_chs\n",
    "        self.out_chs = out_chs\n",
    "\n",
    "        self.norm = norm_layer(in_chs)\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chs,\n",
    "            out_chs,\n",
    "            kernel_size=2,\n",
    "            stride=2,\n",
    "            padding=0,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.norm(x)\n",
    "        x = F.pad(x, (0, (2 - W % 2) % 2))\n",
    "        x = F.pad(x, (0, 0, 0, (2 - H % 2) % 2))\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        k = k * self.scale\n",
    "        attention = k.transpose(-1, -2) @ v\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        x = (attention @ q.transpose(-1, -2)).transpose(-1, -2)\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ChannelBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            ffn=True,\n",
    "            cpe_act=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n",
    "        self.ffn = ffn\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = ChannelAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n",
    "\n",
    "        if self.ffn:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            self.mlp = Mlp(\n",
    "                in_features=dim,\n",
    "                hidden_features=int(dim * mlp_ratio),\n",
    "                act_layer=act_layer,\n",
    "            )\n",
    "            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        else:\n",
    "            self.norm2 = None\n",
    "            self.mlp = None\n",
    "            self.drop_path2 = None\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = self.cpe1(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        cur = self.norm1(x)\n",
    "        cur = self.attn(cur)\n",
    "        x = x + self.drop_path1(cur)\n",
    "\n",
    "        x = self.cpe2(x.transpose(1, 2).view(B, C, H, W))\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "            x = x.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x: Tensor, window_size: Tuple[int, int]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "@register_notrace_function  # reason: int argument is a Proxy\n",
    "def window_reverse(windows: Tensor, window_size: Tuple[int, int], H: int, W: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    C = windows.shape[-1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "    \"\"\"\n",
    "    fused_attn: torch.jit.Final[bool]\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "        self.rel_pos = RelPosBias(window_size=window_size, num_heads=num_heads)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        \n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: Tensor, q_global: Optional[torch.Tensor] = None):\n",
    "        B_, N, C = x.shape\n",
    "#         print(x.shape, \"x.shape\")\n",
    "#         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv.unbind(0)\n",
    "\n",
    "#         if self.fused_attn:\n",
    "#             x = F.scaled_dot_product_attention(q, k, v)\n",
    "#         else:\n",
    "#             q = q * self.scale\n",
    "#             attn = (q @ k.transpose(-2, -1))\n",
    "#             attn = self.softmax(attn)\n",
    "#             x = attn @ v\n",
    "#         print(q.shape, k.shape, v.shape, x.shape, \"q.shape, k.shape, v.shape, x.shape\")    \n",
    "#         q_global = torch.rand(1, 7, 7, 128 )\n",
    "#         print(x.shape, global_query.shape)\n",
    "#         q_global = global_query\n",
    "#         print(x.shape, q_global.shape, global_query.shape)\n",
    "        kv = self.qkv(x)\n",
    "#         print(kv.shape, \"kv.shape\")\n",
    "        kv = kv.reshape(B_, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        _, k, v = kv.unbind(0)\n",
    "#         print(k.shape, \"k shape\", v.shape, \"v shape\")\n",
    "        q = q_global.repeat(B_ // q_global.shape[0], 1, 1, 1)\n",
    "#         print(q.shape, \"q shape\", B_, N, self.num_heads, self.head_dim)\n",
    "        q = q.reshape(B_, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "#         print(q.shape, \"q.shape after reshape\")  \n",
    "        \n",
    "        q = q * self.scale\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1).contiguous()  # NOTE contiguous() fixes an odd jit bug in PyTorch 2.0\n",
    "        attn = self.rel_pos(attn)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "        \n",
    "#         x = x.transpose(1, 2).reshape(B_, N, C)\n",
    "#         x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "# global_query = torch.rand(1, 7, 7, 128 )\n",
    "class MbConvBlock(nn.Module):\n",
    "    \"\"\" A depthwise separable / fused mbconv style residual block with SE, `no norm.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs,\n",
    "            out_chs=None,\n",
    "            expand_ratio=1.0,\n",
    "            attn_layer='se',\n",
    "            bias=False,\n",
    "            act_layer=nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        attn_kwargs = dict(act_layer=act_layer)\n",
    "        if isinstance(attn_layer, str) and attn_layer == 'se' or attn_layer == 'eca':\n",
    "            attn_kwargs['rd_ratio'] = 0.25\n",
    "            attn_kwargs['bias'] = False\n",
    "        attn_layer = get_attn(attn_layer)\n",
    "        out_chs = out_chs or in_chs\n",
    "        mid_chs = int(expand_ratio * in_chs)\n",
    "#         print(in_chs, out_chs, mid_chs, \"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        self.conv_dw = nn.Conv2d(in_chs, mid_chs, 3, 1, 1, groups=in_chs, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.se = attn_layer(mid_chs, **attn_kwargs)\n",
    "        self.conv_pw = nn.Conv2d(mid_chs, out_chs, 1, 1, 0, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "#         print(x.shape, \"MBConvBlock.................................\")\n",
    "        x = self.conv_dw(x)\n",
    "        \n",
    "        x = self.act(x)\n",
    "        x = self.se(x)\n",
    "        x = self.conv_pw(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class FeatureBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            levels=0,\n",
    "            reduction='max',\n",
    "            act_layer=nn.GELU,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        reductions = levels\n",
    "        levels = max(1, levels)\n",
    "        if reduction == 'avg':\n",
    "            pool_fn = partial(nn.AvgPool2d, kernel_size=2)\n",
    "        else:\n",
    "            pool_fn = partial(nn.MaxPool2d, kernel_size=3, stride=2, padding=1)\n",
    "        self.blocks = nn.Sequential()\n",
    "        for i in range(levels):\n",
    "            self.blocks.add_module(f'conv{i+1}', MbConvBlock(dim, act_layer=act_layer))\n",
    "            if reductions:\n",
    "                self.blocks.add_module(f'pool{i+1}', pool_fn())\n",
    "                reductions -= 1\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(x.shape, \"................................................................................\")\n",
    "        return self.blocks(x)\n",
    "\n",
    "class SpatialBlock(nn.Module):\n",
    "    r\"\"\" Windows Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            window_size=7,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=True,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            ffn=True,\n",
    "            cpe_act=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.ffn = ffn\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = to_2tuple(window_size)\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "        )\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act)\n",
    "        if self.ffn:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(\n",
    "                in_features=dim,\n",
    "                hidden_features=mlp_hidden_dim,\n",
    "                act_layer=act_layer,\n",
    "            )\n",
    "            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        else:\n",
    "            self.norm2 = None\n",
    "            self.mlp = None\n",
    "            self.drop_path1 = None\n",
    "\n",
    "    def forward(self, x: Tensor, q_global: Optional[torch.Tensor] = None):\n",
    "        B, C, H, W = x.shape\n",
    "#         print(x.shape, \"In spatial block\")\n",
    "        shortcut = self.cpe1(x).flatten(2).transpose(1, 2)\n",
    "\n",
    "        x = self.norm1(shortcut)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]\n",
    "        pad_b = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        x_windows = window_partition(x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size[0] * self.window_size[1], C)\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, q_global)\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size[0], self.window_size[1], C)\n",
    "        x = window_reverse(attn_windows, self.window_size, Hp, Wp)\n",
    "\n",
    "        # if pad_r > 0 or pad_b > 0:\n",
    "        x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path1(x)\n",
    "\n",
    "        x = self.cpe2(x.transpose(1, 2).view(B, C, H, W))\n",
    "\n",
    "        if self.mlp is not None:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "            x = x.transpose(1, 2).view(B, C, H, W)\n",
    "#         print(x.shape, \"In spatial block\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class DaVitStage(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs,\n",
    "            out_chs,\n",
    "            dim,\n",
    "            feat_size: Tuple[int, int],\n",
    "            depth=1,\n",
    "            downsample=True,\n",
    "            attn_types=('spatial', 'channel'),\n",
    "            num_heads=3,\n",
    "            window_size=7,\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            drop_path_rates=(0, 0),\n",
    "            norm_layer=LayerNorm2d,\n",
    "            norm_layer_cl=nn.LayerNorm,\n",
    "            ffn=True,\n",
    "            cpe_act=False,\n",
    "            global_norm= False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        # downsample embedding layer at the beginning of each stage\n",
    "        if downsample:\n",
    "            self.downsample = Downsample(in_chs, out_chs, norm_layer=norm_layer)\n",
    "            dim = dim * 2\n",
    "            feat_size = (feat_size[0] // 2, feat_size[1] // 2)\n",
    "        else:\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "        '''\n",
    "         repeating alternating attention blocks in each stage\n",
    "         default: (spatial -> channel) x depth\n",
    "         \n",
    "         potential opportunity to integrate with a more general version of ByobNet/ByoaNet\n",
    "         since the logic is similar\n",
    "        '''\n",
    "        window_size = to_2tuple(window_size)\n",
    "        self.feat_size = feat_size\n",
    "#         print(feat_size, window_size, \"feat_size, window_size,\")\n",
    "        feat_levels = int(math.log2(min(feat_size) / min(window_size)))\n",
    "        self.global_norm = norm_layer_cl(dim) if global_norm else nn.Identity()\n",
    "#         print(dim, feat_levels, \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        \n",
    "        self.global_block = FeatureBlock(dim, feat_levels)\n",
    "        stage_blocks = []\n",
    "        self.spatialBlock = SpatialBlock(\n",
    "                        dim=out_chs,\n",
    "                        num_heads=num_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=drop_path_rates[0],\n",
    "                        norm_layer=norm_layer_cl,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act,\n",
    "                        window_size=window_size,\n",
    "                    )\n",
    "        self.channelBlock = ChannelBlock(\n",
    "                        dim=out_chs,\n",
    "                        num_heads=num_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=drop_path_rates[0],\n",
    "                        norm_layer=norm_layer_cl,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act\n",
    "                    )\n",
    "        for block_idx in range(depth):\n",
    "            dual_attention_block = []\n",
    "            for attn_idx, attn_type in enumerate(attn_types):\n",
    "                if attn_type == 'spatial':\n",
    "                    dual_attention_block.append(SpatialBlock(\n",
    "                        dim=out_chs,\n",
    "                        num_heads=num_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=drop_path_rates[block_idx],\n",
    "                        norm_layer=norm_layer_cl,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act,\n",
    "                        window_size=window_size,\n",
    "                    ))\n",
    "                elif attn_type == 'channel':\n",
    "                    dual_attention_block.append(ChannelBlock(\n",
    "                        dim=out_chs,\n",
    "                        num_heads=num_heads,\n",
    "                        mlp_ratio=mlp_ratio,\n",
    "                        qkv_bias=qkv_bias,\n",
    "                        drop_path=drop_path_rates[block_idx],\n",
    "                        norm_layer=norm_layer_cl,\n",
    "                        ffn=ffn,\n",
    "                        cpe_act=cpe_act\n",
    "                    ))\n",
    "            stage_blocks.append(nn.Sequential(*dual_attention_block))\n",
    "        self.blocks = nn.Sequential(*stage_blocks)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "#         print(x.shape)\n",
    "        x = self.downsample(x)\n",
    "#         print(x.shape, \"after downsample\")\n",
    "        global_query = self.global_block(x)\n",
    "\n",
    "        # reshape NCHW --> NHWC for transformer blocks\n",
    "#         x = x.permute(0, 2, 3, 1)\n",
    "        global_query = self.global_norm(global_query.permute(0, 2, 3, 1))\n",
    "#         print(global_query.shape, \"global_query.shape\")\n",
    "        \n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.spatialBlock(x, global_query)\n",
    "            x = self.channelBlock(x)\n",
    "#             x = self.blocks(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DaVit(nn.Module):\n",
    "    r\"\"\" DaViT\n",
    "        A PyTorch implementation of `DaViT: Dual Attention Vision Transformers`  - https://arxiv.org/abs/2204.03645\n",
    "        Supports arbitrary input sizes and pyramid feature extraction\n",
    "        \n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks in each stage. Default: (1, 1, 3, 1)\n",
    "        embed_dims (tuple(int)): Patch embedding dimension. Default: (96, 192, 384, 768)\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers. Default: (3, 6, 12, 24)\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chans=3,\n",
    "            depths=(1, 1, 3, 1),\n",
    "            embed_dim: int = 128,\n",
    "            embed_dims=(96, 192, 384, 768),\n",
    "            num_heads=(3, 6, 12, 24),\n",
    "            img_size: Tuple[int, int] = 224,\n",
    "            window_size=7,\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            norm_layer='layernorm2d',\n",
    "            norm_layer_cl='layernorm',\n",
    "            norm_eps=1e-5,\n",
    "            attn_types=('spatial', 'channel'),\n",
    "            ffn=True,\n",
    "            cpe_act=False,\n",
    "            drop_rate=0.,\n",
    "            drop_path_rate=0.,\n",
    "            num_classes=1000,\n",
    "            global_pool='avg',\n",
    "            head_norm_first=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_stages = len(embed_dims)\n",
    "        assert num_stages == len(num_heads) == len(depths)\n",
    "        norm_layer = partial(get_norm_layer(norm_layer), eps=norm_eps)\n",
    "        norm_layer_cl = partial(get_norm_layer(norm_layer_cl), eps=norm_eps)\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = embed_dims[-1]\n",
    "        self.drop_rate = drop_rate\n",
    "        self.grad_checkpointing = False\n",
    "        self.feature_info = []\n",
    "        img_size = to_2tuple(img_size)\n",
    "        feat_size = tuple(d // 4 for d in img_size)  # stem reduction by 4\n",
    "\n",
    "        self.stem = Stem(in_chans, embed_dims[0], norm_layer=norm_layer)\n",
    "        in_chs = embed_dims[0]\n",
    "\n",
    "        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(depths)).split(depths)]\n",
    "        stages = []\n",
    "        for stage_idx in range(num_stages):\n",
    "            out_chs = embed_dims[stage_idx]\n",
    "            \n",
    "            stage_scale = 2 ** max(stage_idx - 1, 0)\n",
    "            dim=embed_dim * stage_scale\n",
    "#             print(feat_size[0], feat_size[1], embed_dim, stage_scale, feat_size[0] // stage_scale, feat_size[1] // stage_scale, \":::::::::::::::::::::::::::::::::::::::\")\n",
    "            \n",
    "            stage = DaVitStage(\n",
    "                in_chs,\n",
    "                out_chs,\n",
    "                dim=dim,\n",
    "                feat_size=(feat_size[0] // stage_scale, feat_size[1] // stage_scale),\n",
    "                depth=depths[stage_idx],\n",
    "                downsample=stage_idx > 0,\n",
    "                attn_types=attn_types,\n",
    "                num_heads=num_heads[stage_idx],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop_path_rates=dpr[stage_idx],\n",
    "                norm_layer=norm_layer,\n",
    "                norm_layer_cl=norm_layer_cl,\n",
    "                ffn=ffn,\n",
    "                cpe_act=cpe_act,\n",
    "            )\n",
    "            in_chs = out_chs\n",
    "            stages.append(stage)\n",
    "            self.feature_info += [dict(num_chs=out_chs, reduction=2, module=f'stages.{stage_idx}')]\n",
    "\n",
    "        self.stages = nn.Sequential(*stages)\n",
    "\n",
    "        # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets\n",
    "        # otherwise pool -> norm -> fc, the default DaViT order, similar to ConvNeXt\n",
    "        # FIXME generalize this structure to ClassifierHead\n",
    "        if head_norm_first:\n",
    "            self.norm_pre = norm_layer(self.num_features)\n",
    "            self.head = ClassifierHead(\n",
    "                self.num_features,\n",
    "                num_classes,\n",
    "                pool_type=global_pool,\n",
    "                drop_rate=self.drop_rate,\n",
    "            )\n",
    "        else:\n",
    "            self.norm_pre = nn.Identity()\n",
    "            self.head = NormMlpClassifierHead(\n",
    "                self.num_features,\n",
    "                num_classes,\n",
    "                pool_type=global_pool,\n",
    "                drop_rate=self.drop_rate,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        self.grad_checkpointing = enable\n",
    "        for stage in self.stages:\n",
    "            stage.set_grad_checkpointing(enable=enable)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head.fc\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=None):\n",
    "        self.head.reset(num_classes, global_pool=global_pool)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.stem(x)\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.stages, x)\n",
    "        else:\n",
    "            x = self.stages(x)\n",
    "        x = self.norm_pre(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        return self.head(x, pre_logits=True) if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" Remap MSFT checkpoints -> timm \"\"\"\n",
    "    if 'head.fc.weight' in state_dict:\n",
    "        return state_dict  # non-MSFT checkpoint\n",
    "\n",
    "    if 'state_dict' in state_dict:\n",
    "        state_dict = state_dict['state_dict']\n",
    "\n",
    "    import re\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k = re.sub(r'patch_embeds.([0-9]+)', r'stages.\\1.downsample', k)\n",
    "        k = re.sub(r'main_blocks.([0-9]+)', r'stages.\\1.blocks', k)\n",
    "        k = k.replace('downsample.proj', 'downsample.conv')\n",
    "        k = k.replace('stages.0.downsample', 'stem')\n",
    "        k = k.replace('head.', 'head.fc.')\n",
    "        k = k.replace('norms.', 'head.norm.')\n",
    "        k = k.replace('cpe.0', 'cpe1')\n",
    "        k = k.replace('cpe.1', 'cpe2')\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_davit(variant, pretrained=False, **kwargs):\n",
    "    default_out_indices = tuple(i for i, _ in enumerate(kwargs.get('depths', (1, 1, 3, 1))))\n",
    "    out_indices = kwargs.pop('out_indices', default_out_indices)\n",
    "\n",
    "    model = build_model_with_cfg(\n",
    "        DaVit,\n",
    "        variant,\n",
    "        pretrained,\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),\n",
    "        **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n",
    "        'crop_pct': 0.95, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'stem.conv', 'classifier': 'head.fc',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "# TODO contact authors to get larger pretrained models\n",
    "default_cfgs = generate_default_cfgs({\n",
    "    # official microsoft weights from https://github.com/dingmyu/davit\n",
    "    'davit_tiny.msft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'davit_small.msft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'davit_base.msft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'davit_large': _cfg(),\n",
    "    'davit_huge': _cfg(),\n",
    "    'davit_giant': _cfg(),\n",
    "})\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_tiny(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 3, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24))\n",
    "    return _create_davit('davit_tiny', pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_small(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 9, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24))\n",
    "    return _create_davit('davit_small', pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_base(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 9, 1), embed_dims=(128, 256, 512, 1024), num_heads=(4, 8, 16, 32))\n",
    "    return _create_davit('davit_base', pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_large(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 9, 1), embed_dims=(192, 384, 768, 1536), num_heads=(6, 12, 24, 48))\n",
    "    return _create_davit('davit_large', pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_huge(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 9, 1), embed_dims=(256, 512, 1024, 2048), num_heads=(8, 16, 32, 64))\n",
    "    return _create_davit('davit_huge', pretrained=pretrained, **dict(model_args, **kwargs))\n",
    "\n",
    "\n",
    "@register_model\n",
    "def davit_giant(pretrained=False, **kwargs) -> DaVit:\n",
    "    model_args = dict(depths=(1, 1, 12, 3), embed_dims=(384, 768, 1536, 3072), num_heads=(12, 24, 48, 96))\n",
    "    return _create_davit('davit_giant', pretrained=pretrained, **dict(model_args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b00ea6a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:14.486553Z",
     "iopub.status.busy": "2024-04-20T16:42:14.485726Z",
     "iopub.status.idle": "2024-04-20T16:42:17.850659Z",
     "shell.execute_reply": "2024-04-20T16:42:17.849566Z"
    },
    "papermill": {
     "duration": 3.414391,
     "end_time": "2024-04-20T16:42:17.854171",
     "exception": false,
     "start_time": "2024-04-20T16:42:14.439780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.2911,  0.0299,  0.1014,  0.2131, -0.0794, -0.2590, -0.0380,  0.0067,\n",
      "          0.1378,  0.0958,  0.2344, -0.5217, -0.1359, -0.4020, -0.1893]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 122.78 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "FLOPs: 5.53G, Params: 38.57M\n"
     ]
    }
   ],
   "source": [
    "# with fine-tuning option\n",
    "class GWA_DaViT(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(GWA_DaViT, self).__init__()\n",
    "        \n",
    "        self.davit = davit_base(pretrained=pretrained, num_classes=num_classes, in_chans=1)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            # Freeze all layers except classifier layers\n",
    "            for param in self.davit.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze the classifier layers\n",
    "            for param in self.davit.head.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.davit(x)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GWA_DaViT(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits    \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93a973",
   "metadata": {
    "papermill": {
     "duration": 0.047788,
     "end_time": "2024-04-20T16:42:17.954981",
     "exception": false,
     "start_time": "2024-04-20T16:42:17.907193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Proposed 3 DaViT(BASE) + Unetr (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84fc2ff4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:18.055945Z",
     "iopub.status.busy": "2024-04-20T16:42:18.054689Z",
     "iopub.status.idle": "2024-04-20T16:42:32.242902Z",
     "shell.execute_reply": "2024-04-20T16:42:32.241651Z"
    },
    "papermill": {
     "duration": 14.239013,
     "end_time": "2024-04-20T16:42:32.245548",
     "exception": false,
     "start_time": "2024-04-20T16:42:18.006535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /opt/conda/lib/python3.10/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e1a4e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:32.336593Z",
     "iopub.status.busy": "2024-04-20T16:42:32.336177Z",
     "iopub.status.idle": "2024-04-20T16:42:36.066713Z",
     "shell.execute_reply": "2024-04-20T16:42:36.065501Z"
    },
    "papermill": {
     "duration": 3.778663,
     "end_time": "2024-04-20T16:42:36.069320",
     "exception": false,
     "start_time": "2024-04-20T16:42:32.290657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.0553,  0.1761,  0.0403, -0.2128,  0.0069,  0.2055,  0.0741, -0.0338,\n",
      "          0.1973,  0.2444,  0.2451,  0.0858,  0.0332,  0.0037, -0.0713]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 138.13 M\n",
      "Number of trainable parameters in millions: 51.21 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 47.89G, Params: 138.08M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "\n",
    "\n",
    "class DaViT_UnetR_Modelv2(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(DaViT_UnetR_Modelv2, self).__init__()\n",
    "        \n",
    "        self.davit = timm.create_model('davit_base.msft_in1k', pretrained=pretrained, features_only=True, in_chans=1)\n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.davit.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        spatial_dims = 2 \n",
    "        in_channels = 1 # R,G,B\n",
    "        feature_size = 128\n",
    "        norm_name = \"instance\"\n",
    "        hidden_size = 128\n",
    "        res_block = True\n",
    "        conv_block = False\n",
    "\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*2,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*4,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size * 8,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(feature_size, 78, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(78, 50, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Classifier layer with convolution\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2450, 1024),  # (DYNAMIC)Adjust the input size based on the output size of the convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        hidden_states_out = self.davit(x_in) # returns 4 lists\n",
    "#         print(\"Length of hidden states from DaViT:\", len(hidden_states_out))\n",
    "#         for i in hidden_states_out:\n",
    "#             print(i.shape)\n",
    "#         print()\n",
    "\n",
    "\n",
    "        enc1 = self.encoder1(x_in)\n",
    "#         print(\"output from encoder1:\", enc1.shape)\n",
    "        \n",
    "        x2 = hidden_states_out[0]\n",
    "        enc2 = self.encoder2(x2)\n",
    "#         print(\"output from encoder2:\", enc2.shape)\n",
    "        \n",
    "        x3 = hidden_states_out[1]\n",
    "        enc3 = self.encoder3(x3)\n",
    "#         print(\"output from encoder3:\", enc3.shape)\n",
    "        \n",
    "        \n",
    "        x4 = hidden_states_out[2]\n",
    "        enc4 = self.encoder4(x4)\n",
    "#         print(\"output from encoder4:\", enc4.shape)\n",
    "        \n",
    "#         print(\"All encoders OK\\n\")\n",
    "        \n",
    "        dec4 = hidden_states_out[3]\n",
    "#         print(\"Input to decoder5:\", dec4.shape, enc4.shape)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "#         print(\"output from decoder5:\", dec3.shape)\n",
    "        \n",
    "#         print(\"Input to decoder4:\", dec3.shape, enc3.shape)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "#         print(\"output from decoder4:\", dec2.shape)\n",
    "        \n",
    "#         print(\"Input to decoder3:\", dec2.shape, enc2.shape)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "#         print(\"output from decoder3:\", dec1.shape)\n",
    "        \n",
    "#         print(\"Input to decoder2:\", dec1.shape, enc1.shape)\n",
    "        out = self.decoder2(dec1, enc1) \n",
    "#         print(\"output from decoder2:\", out.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        conv_out = self.conv(out)\n",
    "#         print(f\"conv_out_shape:{conv_out.shape}\")\n",
    "\n",
    "        return self.classifier(conv_out)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DaViT_UnetR_Modelv2(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6b3b9",
   "metadata": {
    "papermill": {
     "duration": 0.044645,
     "end_time": "2024-04-20T16:42:36.159304",
     "exception": false,
     "start_time": "2024-04-20T16:42:36.114659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Proposed Model 4: Swin_Unetr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ece3404b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:36.250830Z",
     "iopub.status.busy": "2024-04-20T16:42:36.249657Z",
     "iopub.status.idle": "2024-04-20T16:42:50.438194Z",
     "shell.execute_reply": "2024-04-20T16:42:50.436971Z"
    },
    "papermill": {
     "duration": 14.237223,
     "end_time": "2024-04-20T16:42:50.441069",
     "exception": false,
     "start_time": "2024-04-20T16:42:36.203846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /opt/conda/lib/python3.10/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05d2b039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:50.535287Z",
     "iopub.status.busy": "2024-04-20T16:42:50.534890Z",
     "iopub.status.idle": "2024-04-20T16:42:56.993012Z",
     "shell.execute_reply": "2024-04-20T16:42:56.991805Z"
    },
    "papermill": {
     "duration": 6.508035,
     "end_time": "2024-04-20T16:42:56.995434",
     "exception": false,
     "start_time": "2024-04-20T16:42:50.487399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[ 0.0679, -0.2037,  0.1124,  0.1723,  0.1938, -0.1984, -0.0166, -0.1033,\n",
      "          0.1193,  0.0890,  0.0989, -0.1060, -0.0095, -0.1039,  0.0462]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 306.95 M\n",
      "Number of trainable parameters in millions: 111.96 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.instancenorm.InstanceNorm2d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.ConvTranspose2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 107.39G, Params: 306.85M\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "from monai.networks.blocks.dynunet_block import UnetOutBlock\n",
    "from monai.networks.blocks.unetr_block import UnetrBasicBlock, UnetrPrUpBlock, UnetrUpBlock\n",
    "\n",
    "\n",
    "class Swin_Unetr(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, fine_tune=False):\n",
    "        super(Swin_Unetr, self).__init__()\n",
    "        \n",
    "        self.swin = timm.create_model(\n",
    "            'swin_large_patch4_window7_224.ms_in22k', \n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            in_chans=1,\n",
    "            features_only=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if not fine_tune:\n",
    "            for param in self.swin.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        spatial_dims = 2 \n",
    "        in_channels = 1 # R,G,B\n",
    "        feature_size = 192\n",
    "        norm_name = \"instance\"\n",
    "        hidden_size = 192\n",
    "        res_block = True\n",
    "        conv_block = False\n",
    "\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder2 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=feature_size * 2,\n",
    "            num_layer=2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder3 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*2,\n",
    "            out_channels=feature_size * 4,\n",
    "            num_layer=1,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.encoder4 = UnetrPrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size*4,\n",
    "            out_channels=feature_size * 8,\n",
    "            num_layer=0,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            upsample_kernel_size=1,\n",
    "            norm_name=norm_name,\n",
    "            conv_block=conv_block,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=hidden_size * 8,\n",
    "            out_channels=feature_size * 8,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(feature_size, 78, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(78, 50, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Classifier layer with convolution\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2450, 1024),  # (DYNAMIC)Adjust the input size based on the output size of the convolutional layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        \n",
    "        hidden_states_out = self.swin(x_in) # returns 4 lists\n",
    "#         print(\"Length of hidden states from Swin:\", len(hidden_states_out))\n",
    "#         for i in hidden_states_out:\n",
    "#             print(i.shape)\n",
    "#         print()\n",
    "        \n",
    "        ##we will permute each of the intermediate outputs from swin stages so that the channel comes first\n",
    "\n",
    "        hidden_states_out = [t.permute(0, 3, 1, 2) for t in hidden_states_out]\n",
    "        \n",
    "        \n",
    "        enc1 = self.encoder1(x_in)\n",
    "#         print(\"output from encoder1:\", enc1.shape)\n",
    "        \n",
    "        x2 = hidden_states_out[0]\n",
    "        enc2 = self.encoder2(x2)\n",
    "#         print(\"output from encoder2:\", enc2.shape)\n",
    "        \n",
    "        x3 = hidden_states_out[1]\n",
    "        enc3 = self.encoder3(x3)\n",
    "#         print(\"output from encoder3:\", enc3.shape)\n",
    "        \n",
    "        \n",
    "        x4 = hidden_states_out[2]\n",
    "        enc4 = self.encoder4(x4)\n",
    "#         print(\"output from encoder4:\", enc4.shape)\n",
    "        \n",
    "#         print(\"All encoders OK\\n\")\n",
    "        \n",
    "        dec4 = hidden_states_out[3]\n",
    "#         print(\"Input to decoder5:\", dec4.shape, enc4.shape)\n",
    "        dec3 = self.decoder5(dec4, enc4)\n",
    "#         print(\"output from decoder5:\", dec3.shape)\n",
    "        \n",
    "#         print(\"Input to decoder4:\", dec3.shape, enc3.shape)\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "#         print(\"output from decoder4:\", dec2.shape)\n",
    "        \n",
    "#         print(\"Input to decoder3:\", dec2.shape, enc2.shape)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "#         print(\"output from decoder3:\", dec1.shape)\n",
    "        \n",
    "#         print(\"Input to decoder2:\", dec1.shape, enc1.shape)\n",
    "        out = self.decoder2(dec1, enc1) \n",
    "#         print(\"output from decoder2:\", out.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        conv_out = self.conv(out)\n",
    "#         print(f\"conv_out_shape:{conv_out.shape}\")\n",
    "\n",
    "        return self.classifier(conv_out)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Swin_Unetr(num_classes, fine_tune=False)\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748989eb",
   "metadata": {
    "papermill": {
     "duration": 0.045091,
     "end_time": "2024-04-20T16:42:57.086496",
     "exception": false,
     "start_time": "2024-04-20T16:42:57.041405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec37c94e",
   "metadata": {
    "papermill": {
     "duration": 0.045296,
     "end_time": "2024-04-20T16:42:57.176753",
     "exception": false,
     "start_time": "2024-04-20T16:42:57.131457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Proposed Model 5: CoAtNet Multiscale Pyramidal Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa6ce7fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:42:57.267712Z",
     "iopub.status.busy": "2024-04-20T16:42:57.267006Z",
     "iopub.status.idle": "2024-04-20T16:43:11.542094Z",
     "shell.execute_reply": "2024-04-20T16:43:11.540953Z"
    },
    "papermill": {
     "duration": 14.323281,
     "end_time": "2024-04-20T16:43:11.544750",
     "exception": false,
     "start_time": "2024-04-20T16:42:57.221469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monai in /opt/conda/lib/python3.10/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from monai) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2024.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b8ceca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:11.638487Z",
     "iopub.status.busy": "2024-04-20T16:43:11.638073Z",
     "iopub.status.idle": "2024-04-20T16:43:11.722902Z",
     "shell.execute_reply": "2024-04-20T16:43:11.721938Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.135936,
     "end_time": "2024-04-20T16:43:11.725410",
     "exception": false,
     "start_time": "2024-04-20T16:43:11.589474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _features.py\n",
    "\n",
    "\"\"\" PyTorch Feature Extraction Helpers\n",
    "\n",
    "A collection of classes, functions, modules to help extract features from models\n",
    "and provide a common interface for describing them.\n",
    "\n",
    "The return_layers, module re-writing idea inspired by torchvision IntermediateLayerGetter\n",
    "https://github.com/pytorch/vision/blob/d88d8961ae51507d0cb680329d985b1488b1b76b/torchvision/models/_utils.py\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "from collections import OrderedDict, defaultdict\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Dict, List, Optional, Sequence, Set, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "from timm.layers import Format\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'FeatureInfo', 'FeatureHooks', 'FeatureDictNet', 'FeatureListNet', 'FeatureHookNet', 'FeatureGetterNet',\n",
    "    'feature_take_indices'\n",
    "]\n",
    "\n",
    "\n",
    "def _take_indices(\n",
    "        num_blocks: int,\n",
    "        n: Optional[Union[int, List[int], Tuple[int]]],\n",
    ") -> Tuple[Set[int], int]:\n",
    "    if isinstance(n, int):\n",
    "        assert n >= 0\n",
    "        take_indices = {x for x in range(num_blocks - n, num_blocks)}\n",
    "    else:\n",
    "        take_indices = {num_blocks + idx if idx < 0 else idx for idx in n}\n",
    "    return take_indices, max(take_indices)\n",
    "\n",
    "\n",
    "def _take_indices_jit(\n",
    "        num_blocks: int,\n",
    "        n: Union[int, List[int], Tuple[int]],\n",
    ") -> Tuple[List[int], int]:\n",
    "    if isinstance(n, int):\n",
    "        assert n >= 0\n",
    "        take_indices = [num_blocks - n + i for i in range(n)]\n",
    "    elif isinstance(n, tuple):\n",
    "        # splitting this up is silly, but needed for torchscript type resolution of n\n",
    "        take_indices = [num_blocks + idx if idx < 0 else idx for idx in n]\n",
    "    else:\n",
    "        take_indices = [num_blocks + idx if idx < 0 else idx for idx in n]\n",
    "    return take_indices, max(take_indices)\n",
    "\n",
    "\n",
    "def feature_take_indices(\n",
    "        num_blocks: int,\n",
    "        indices: Optional[Union[int, List[int], Tuple[int]]] = None,\n",
    ") -> Tuple[List[int], int]:\n",
    "    if indices is None:\n",
    "        indices = num_blocks  # all blocks if None\n",
    "    if torch.jit.is_scripting():\n",
    "        return _take_indices_jit(num_blocks, indices)\n",
    "    else:\n",
    "        # NOTE non-jit returns Set[int] instead of List[int] but torchscript can't handle that anno\n",
    "        return _take_indices(num_blocks, indices)\n",
    "\n",
    "\n",
    "def _out_indices_as_tuple(x: Union[int, Tuple[int, ...]]) -> Tuple[int, ...]:\n",
    "    if isinstance(x, int):\n",
    "        # if indices is an int, take last N features\n",
    "        return tuple(range(-x, 0))\n",
    "    return tuple(x)\n",
    "\n",
    "\n",
    "OutIndicesT = Union[int, Tuple[int, ...]]\n",
    "\n",
    "\n",
    "class FeatureInfo:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            feature_info: List[Dict],\n",
    "            out_indices: OutIndicesT,\n",
    "    ):\n",
    "        out_indices = _out_indices_as_tuple(out_indices)\n",
    "        prev_reduction = 1\n",
    "        for i, fi in enumerate(feature_info):\n",
    "            # sanity check the mandatory fields, there may be additional fields depending on the model\n",
    "            assert 'num_chs' in fi and fi['num_chs'] > 0\n",
    "            assert 'reduction' in fi and fi['reduction'] >= prev_reduction\n",
    "            prev_reduction = fi['reduction']\n",
    "            assert 'module' in fi\n",
    "            fi.setdefault('index', i)\n",
    "        self.out_indices = out_indices\n",
    "        self.info = feature_info\n",
    "\n",
    "    def from_other(self, out_indices: OutIndicesT):\n",
    "        out_indices = _out_indices_as_tuple(out_indices)\n",
    "        return FeatureInfo(deepcopy(self.info), out_indices)\n",
    "\n",
    "    def get(self, key: str, idx: Optional[Union[int, List[int]]] = None):\n",
    "        \"\"\" Get value by key at specified index (indices)\n",
    "        if idx == None, returns value for key at each output index\n",
    "        if idx is an integer, return value for that feature module index (ignoring output indices)\n",
    "        if idx is a list/tuple, return value for each module index (ignoring output indices)\n",
    "        \"\"\"\n",
    "        if idx is None:\n",
    "            return [self.info[i][key] for i in self.out_indices]\n",
    "        if isinstance(idx, (tuple, list)):\n",
    "            return [self.info[i][key] for i in idx]\n",
    "        else:\n",
    "            return self.info[idx][key]\n",
    "\n",
    "    def get_dicts(self, keys: Optional[List[str]] = None, idx: Optional[Union[int, List[int]]] = None):\n",
    "        \"\"\" return info dicts for specified keys (or all if None) at specified indices (or out_indices if None)\n",
    "        \"\"\"\n",
    "        if idx is None:\n",
    "            if keys is None:\n",
    "                return [self.info[i] for i in self.out_indices]\n",
    "            else:\n",
    "                return [{k: self.info[i][k] for k in keys} for i in self.out_indices]\n",
    "        if isinstance(idx, (tuple, list)):\n",
    "            return [self.info[i] if keys is None else {k: self.info[i][k] for k in keys} for i in idx]\n",
    "        else:\n",
    "            return self.info[idx] if keys is None else {k: self.info[idx][k] for k in keys}\n",
    "\n",
    "    def channels(self, idx: Optional[Union[int, List[int]]] = None):\n",
    "        \"\"\" feature channels accessor\n",
    "        \"\"\"\n",
    "        return self.get('num_chs', idx)\n",
    "\n",
    "    def reduction(self, idx: Optional[Union[int, List[int]]] = None):\n",
    "        \"\"\" feature reduction (output stride) accessor\n",
    "        \"\"\"\n",
    "        return self.get('reduction', idx)\n",
    "\n",
    "    def module_name(self, idx: Optional[Union[int, List[int]]] = None):\n",
    "        \"\"\" feature module name accessor\n",
    "        \"\"\"\n",
    "        return self.get('module', idx)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.info[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.info)\n",
    "\n",
    "\n",
    "class FeatureHooks:\n",
    "    \"\"\" Feature Hook Helper\n",
    "\n",
    "    This module helps with the setup and extraction of hooks for extracting features from\n",
    "    internal nodes in a model by node name.\n",
    "\n",
    "    FIXME This works well in eager Python but needs redesign for torchscript.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            hooks: Sequence[str],\n",
    "            named_modules: dict,\n",
    "            out_map: Sequence[Union[int, str]] = None,\n",
    "            default_hook_type: str = 'forward',\n",
    "    ):\n",
    "        # setup feature hooks\n",
    "        self._feature_outputs = defaultdict(OrderedDict)\n",
    "        modules = {k: v for k, v in named_modules}\n",
    "        for i, h in enumerate(hooks):\n",
    "            hook_name = h['module']\n",
    "            m = modules[hook_name]\n",
    "            hook_id = out_map[i] if out_map else hook_name\n",
    "            hook_fn = partial(self._collect_output_hook, hook_id)\n",
    "            hook_type = h.get('hook_type', default_hook_type)\n",
    "            if hook_type == 'forward_pre':\n",
    "                m.register_forward_pre_hook(hook_fn)\n",
    "            elif hook_type == 'forward':\n",
    "                m.register_forward_hook(hook_fn)\n",
    "            else:\n",
    "                assert False, \"Unsupported hook type\"\n",
    "\n",
    "    def _collect_output_hook(self, hook_id, *args):\n",
    "        x = args[-1]  # tensor we want is last argument, output for fwd, input for fwd_pre\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]  # unwrap input tuple\n",
    "        self._feature_outputs[x.device][hook_id] = x\n",
    "\n",
    "    def get_output(self, device) -> Dict[str, torch.tensor]:\n",
    "        output = self._feature_outputs[device]\n",
    "        self._feature_outputs[device] = OrderedDict()  # clear after reading\n",
    "        return output\n",
    "\n",
    "\n",
    "def _module_list(module, flatten_sequential=False):\n",
    "    # a yield/iter would be better for this but wouldn't be compatible with torchscript\n",
    "    ml = []\n",
    "    for name, module in module.named_children():\n",
    "        if flatten_sequential and isinstance(module, nn.Sequential):\n",
    "            # first level of Sequential containers is flattened into containing model\n",
    "            for child_name, child_module in module.named_children():\n",
    "                combined = [name, child_name]\n",
    "                ml.append(('_'.join(combined), '.'.join(combined), child_module))\n",
    "        else:\n",
    "            ml.append((name, name, module))\n",
    "    return ml\n",
    "\n",
    "\n",
    "def _get_feature_info(net, out_indices: OutIndicesT):\n",
    "    feature_info = getattr(net, 'feature_info')\n",
    "    if isinstance(feature_info, FeatureInfo):\n",
    "        return feature_info.from_other(out_indices)\n",
    "    elif isinstance(feature_info, (list, tuple)):\n",
    "        return FeatureInfo(net.feature_info, out_indices)\n",
    "    else:\n",
    "        assert False, \"Provided feature_info is not valid\"\n",
    "\n",
    "\n",
    "def _get_return_layers(feature_info, out_map):\n",
    "    module_names = feature_info.module_name()\n",
    "    return_layers = {}\n",
    "    for i, name in enumerate(module_names):\n",
    "        return_layers[name] = out_map[i] if out_map is not None else feature_info.out_indices[i]\n",
    "    return return_layers\n",
    "\n",
    "\n",
    "class FeatureDictNet(nn.ModuleDict):\n",
    "    \"\"\" Feature extractor with OrderedDict return\n",
    "\n",
    "    Wrap a model and extract features as specified by the out indices, the network is\n",
    "    partially re-built from contained modules.\n",
    "\n",
    "    There is a strong assumption that the modules have been registered into the model in the same\n",
    "    order as they are used. There should be no reuse of the same nn.Module more than once, including\n",
    "    trivial modules like `self.relu = nn.ReLU`.\n",
    "\n",
    "    Only submodules that are directly assigned to the model class (`model.feature1`) or at most\n",
    "    one Sequential container deep (`model.features.1`, with flatten_sequent=True) can be captured.\n",
    "    All Sequential containers that are directly assigned to the original model will have their\n",
    "    modules assigned to this module with the name `model.features.1` being changed to `model.features_1`\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: OutIndicesT = (0, 1, 2, 3, 4),\n",
    "            out_map: Sequence[Union[int, str]] = None,\n",
    "            output_fmt: str = 'NCHW',\n",
    "            feature_concat: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            out_map: Return id mapping for each output index, otherwise str(index) is used.\n",
    "            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n",
    "                first element e.g. `x[0]`\n",
    "            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n",
    "        \"\"\"\n",
    "        super(FeatureDictNet, self).__init__()\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        self.output_fmt = Format(output_fmt)\n",
    "        self.concat = feature_concat\n",
    "        self.grad_checkpointing = False\n",
    "        self.return_layers = {}\n",
    "\n",
    "        return_layers = _get_return_layers(self.feature_info, out_map)\n",
    "        modules = _module_list(model, flatten_sequential=flatten_sequential)\n",
    "        remaining = set(return_layers.keys())\n",
    "        layers = OrderedDict()\n",
    "        for new_name, old_name, module in modules:\n",
    "            layers[new_name] = module\n",
    "            if old_name in remaining:\n",
    "                # return id has to be consistently str type for torchscript\n",
    "                self.return_layers[new_name] = str(return_layers[old_name])\n",
    "                remaining.remove(old_name)\n",
    "            if not remaining:\n",
    "                break\n",
    "        assert not remaining and len(self.return_layers) == len(return_layers), \\\n",
    "            f'Return layers ({remaining}) are not present in model'\n",
    "        self.update(layers)\n",
    "\n",
    "    def set_grad_checkpointing(self, enable: bool = True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    def _collect(self, x) -> (Dict[str, torch.Tensor]):\n",
    "        out = OrderedDict()\n",
    "        for i, (name, module) in enumerate(self.items()):\n",
    "            if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                # Skipping checkpoint of first module because need a gradient at input\n",
    "                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n",
    "                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n",
    "                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n",
    "                x = module(x) if first_or_last_module else checkpoint(module, x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "\n",
    "            if name in self.return_layers:\n",
    "                out_id = self.return_layers[name]\n",
    "                if isinstance(x, (tuple, list)):\n",
    "                    # If model tap is a tuple or list, concat or select first element\n",
    "                    # FIXME this may need to be more generic / flexible for some nets\n",
    "                    out[out_id] = torch.cat(x, 1) if self.concat else x[0]\n",
    "                else:\n",
    "                    out[out_id] = x\n",
    "        return out\n",
    "\n",
    "    def forward(self, x) -> Dict[str, torch.Tensor]:\n",
    "        return self._collect(x)\n",
    "\n",
    "\n",
    "class FeatureListNet(FeatureDictNet):\n",
    "    \"\"\" Feature extractor with list return\n",
    "\n",
    "    A specialization of FeatureDictNet that always returns features as a list (values() of dict).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: OutIndicesT = (0, 1, 2, 3, 4),\n",
    "            output_fmt: str = 'NCHW',\n",
    "            feature_concat: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            feature_concat: Concatenate intermediate features that are lists or tuples instead of selecting\n",
    "                first element e.g. `x[0]`\n",
    "            flatten_sequential: Flatten first two-levels of sequential modules in model (re-writes model modules)\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model,\n",
    "            out_indices=out_indices,\n",
    "            output_fmt=output_fmt,\n",
    "            feature_concat=feature_concat,\n",
    "            flatten_sequential=flatten_sequential,\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> (List[torch.Tensor]):\n",
    "        return list(self._collect(x).values())\n",
    "\n",
    "\n",
    "class FeatureHookNet(nn.ModuleDict):\n",
    "    \"\"\" FeatureHookNet\n",
    "\n",
    "    Wrap a model and extract features specified by the out indices using forward/forward-pre hooks.\n",
    "\n",
    "    If `no_rewrite` is True, features are extracted via hooks without modifying the underlying\n",
    "    network in any way.\n",
    "\n",
    "    If `no_rewrite` is False, the model will be re-written as in the\n",
    "    FeatureList/FeatureDict case by folding first to second (Sequential only) level modules into this one.\n",
    "\n",
    "    FIXME this does not currently work with Torchscript, see FeatureHooks class\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: OutIndicesT = (0, 1, 2, 3, 4),\n",
    "            out_map: Optional[Sequence[Union[int, str]]] = None,\n",
    "            return_dict: bool = False,\n",
    "            output_fmt: str = 'NCHW',\n",
    "            no_rewrite: bool = False,\n",
    "            flatten_sequential: bool = False,\n",
    "            default_hook_type: str = 'forward',\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            model: Model from which to extract features.\n",
    "            out_indices: Output indices of the model features to extract.\n",
    "            out_map: Return id mapping for each output index, otherwise str(index) is used.\n",
    "            return_dict: Output features as a dict.\n",
    "            no_rewrite: Enforce that model is not re-written if True, ie no modules are removed / changed.\n",
    "                flatten_sequential arg must also be False if this is set True.\n",
    "            flatten_sequential: Re-write modules by flattening first two levels of nn.Sequential containers.\n",
    "            default_hook_type: The default hook type to use if not specified in model.feature_info.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert not torch.jit.is_scripting()\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        self.return_dict = return_dict\n",
    "        self.output_fmt = Format(output_fmt)\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        layers = OrderedDict()\n",
    "        hooks = []\n",
    "        if no_rewrite:\n",
    "            assert not flatten_sequential\n",
    "            if hasattr(model, 'reset_classifier'):  # make sure classifier is removed?\n",
    "                model.reset_classifier(0)\n",
    "            layers['body'] = model\n",
    "            hooks.extend(self.feature_info.get_dicts())\n",
    "        else:\n",
    "            modules = _module_list(model, flatten_sequential=flatten_sequential)\n",
    "            remaining = {\n",
    "                f['module']: f['hook_type'] if 'hook_type' in f else default_hook_type\n",
    "                for f in self.feature_info.get_dicts()\n",
    "            }\n",
    "            for new_name, old_name, module in modules:\n",
    "                layers[new_name] = module\n",
    "                for fn, fm in module.named_modules(prefix=old_name):\n",
    "                    if fn in remaining:\n",
    "                        hooks.append(dict(module=fn, hook_type=remaining[fn]))\n",
    "                        del remaining[fn]\n",
    "                if not remaining:\n",
    "                    break\n",
    "            assert not remaining, f'Return layers ({remaining}) are not present in model'\n",
    "        self.update(layers)\n",
    "        self.hooks = FeatureHooks(hooks, model.named_modules(), out_map=out_map)\n",
    "\n",
    "    def set_grad_checkpointing(self, enable: bool = True):\n",
    "        self.grad_checkpointing = enable\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (name, module) in enumerate(self.items()):\n",
    "            if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "                # Skipping checkpoint of first module because need a gradient at input\n",
    "                # Skipping last because networks with in-place ops might fail w/ checkpointing enabled\n",
    "                # NOTE: first_or_last module could be static, but recalc in is_scripting guard to avoid jit issues\n",
    "                first_or_last_module = i == 0 or i == max(len(self) - 1, 0)\n",
    "                x = module(x) if first_or_last_module else checkpoint(module, x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        out = self.hooks.get_output(x.device)\n",
    "        return out if self.return_dict else list(out.values())\n",
    "\n",
    "\n",
    "class FeatureGetterNet(nn.ModuleDict):\n",
    "    \"\"\" FeatureGetterNet\n",
    "\n",
    "    Wrap models with a feature getter method, like 'get_intermediate_layers'\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: OutIndicesT = 4,\n",
    "            out_map: Optional[Sequence[Union[int, str]]] = None,\n",
    "            return_dict: bool = False,\n",
    "            output_fmt: str = 'NCHW',\n",
    "            norm: bool = False,\n",
    "            prune: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            model: Model to wrap.\n",
    "            out_indices: Indices of features to extract.\n",
    "            out_map: Remap feature names for dict output (WIP, not supported).\n",
    "            return_dict: Return features as dictionary instead of list (WIP, not supported).\n",
    "            norm: Apply final model norm to all output features (if possible).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if prune and hasattr(model, 'prune_intermediate_layers'):\n",
    "            # replace out_indices after they've been normalized, -ve indices will be invalid after prune\n",
    "            out_indices = model.prune_intermediate_layers(\n",
    "                out_indices,\n",
    "                prune_norm=not norm,\n",
    "            )\n",
    "            out_indices = list(out_indices)\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        self.model = model\n",
    "        self.out_indices = out_indices\n",
    "        self.out_map = out_map\n",
    "        self.return_dict = return_dict\n",
    "        self.output_fmt = output_fmt\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model.forward_intermediates(\n",
    "            x,\n",
    "            indices=self.out_indices,\n",
    "            norm=self.norm,\n",
    "            output_fmt=self.output_fmt,\n",
    "            intermediates_only=True,\n",
    "        )\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f6459ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:11.819118Z",
     "iopub.status.busy": "2024-04-20T16:43:11.818670Z",
     "iopub.status.idle": "2024-04-20T16:43:11.842724Z",
     "shell.execute_reply": "2024-04-20T16:43:11.841684Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.074241,
     "end_time": "2024-04-20T16:43:11.845032",
     "exception": false,
     "start_time": "2024-04-20T16:43:11.770791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _features_fx.py\n",
    "\n",
    "\"\"\" PyTorch FX Based Feature Extraction Helpers\n",
    "Using https://pytorch.org/vision/stable/feature_extraction.html\n",
    "\"\"\"\n",
    "from typing import Callable, Dict, List, Optional, Union, Tuple, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# from ._features import _get_feature_info, _get_return_layers\n",
    "\n",
    "try:\n",
    "    from torchvision.models.feature_extraction import create_feature_extractor as _create_feature_extractor\n",
    "    has_fx_feature_extraction = True\n",
    "except ImportError:\n",
    "    has_fx_feature_extraction = False\n",
    "\n",
    "# Layers we went to treat as leaf modules\n",
    "from timm.layers import Conv2dSame, ScaledStdConv2dSame, CondConv2d, StdConv2dSame\n",
    "from timm.layers.non_local_attn import BilinearAttnTransform\n",
    "from timm.layers.pool2d_same import MaxPool2dSame, AvgPool2dSame\n",
    "from timm.layers.norm_act import (\n",
    "    BatchNormAct2d,\n",
    "    SyncBatchNormAct,\n",
    "    FrozenBatchNormAct2d,\n",
    "    GroupNormAct,\n",
    "    GroupNorm1Act,\n",
    "    LayerNormAct,\n",
    "    LayerNormAct2d\n",
    ")\n",
    "\n",
    "__all__ = ['register_notrace_module', 'is_notrace_module', 'get_notrace_modules',\n",
    "           'register_notrace_function', 'is_notrace_function', 'get_notrace_functions',\n",
    "           'create_feature_extractor', 'FeatureGraphNet', 'GraphExtractNet']\n",
    "\n",
    "\n",
    "# NOTE: By default, any modules from timm.models.layers that we want to treat as leaf modules go here\n",
    "# BUT modules from timm.models should use the registration mechanism below\n",
    "_leaf_modules = {\n",
    "    BilinearAttnTransform,  # reason: flow control t <= 1\n",
    "    # Reason: get_same_padding has a max which raises a control flow error\n",
    "    Conv2dSame, MaxPool2dSame, ScaledStdConv2dSame, StdConv2dSame, AvgPool2dSame,\n",
    "    CondConv2d,  # reason: TypeError: F.conv2d received Proxy in groups=self.groups * B (because B = x.shape[0]),\n",
    "    BatchNormAct2d,\n",
    "    SyncBatchNormAct,\n",
    "    FrozenBatchNormAct2d,\n",
    "    GroupNormAct,\n",
    "    GroupNorm1Act,\n",
    "    LayerNormAct,\n",
    "    LayerNormAct2d,\n",
    "}\n",
    "\n",
    "try:\n",
    "    from timm.layers import InplaceAbn\n",
    "    _leaf_modules.add(InplaceAbn)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def register_notrace_module(module: Type[nn.Module]):\n",
    "    \"\"\"\n",
    "    Any module not under timm.models.layers should get this decorator if we don't want to trace through it.\n",
    "    \"\"\"\n",
    "    _leaf_modules.add(module)\n",
    "    return module\n",
    "\n",
    "\n",
    "def is_notrace_module(module: Type[nn.Module]):\n",
    "    return module in _leaf_modules\n",
    "\n",
    "\n",
    "def get_notrace_modules():\n",
    "    return list(_leaf_modules)\n",
    "\n",
    "\n",
    "# Functions we want to autowrap (treat them as leaves)\n",
    "_autowrap_functions = set()\n",
    "\n",
    "\n",
    "def register_notrace_function(func: Callable):\n",
    "    \"\"\"\n",
    "    Decorator for functions which ought not to be traced through\n",
    "    \"\"\"\n",
    "    _autowrap_functions.add(func)\n",
    "    return func\n",
    "\n",
    "\n",
    "def is_notrace_function(func: Callable):\n",
    "    return func in _autowrap_functions\n",
    "\n",
    "\n",
    "def get_notrace_functions():\n",
    "    return list(_autowrap_functions)\n",
    "\n",
    "\n",
    "def create_feature_extractor(model: nn.Module, return_nodes: Union[Dict[str, str], List[str]]):\n",
    "    assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n",
    "    return _create_feature_extractor(\n",
    "        model, return_nodes,\n",
    "        tracer_kwargs={'leaf_modules': list(_leaf_modules), 'autowrap_functions': list(_autowrap_functions)}\n",
    "    )\n",
    "\n",
    "\n",
    "class FeatureGraphNet(nn.Module):\n",
    "    \"\"\" A FX Graph based feature extractor that works with the model feature_info metadata\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            out_indices: Tuple[int, ...],\n",
    "            out_map: Optional[Dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert has_fx_feature_extraction, 'Please update to PyTorch 1.10+, torchvision 0.11+ for FX feature extraction'\n",
    "        self.feature_info = _get_feature_info(model, out_indices)\n",
    "        if out_map is not None:\n",
    "            assert len(out_map) == len(out_indices)\n",
    "        return_nodes = _get_return_layers(self.feature_info, out_map)\n",
    "        self.graph_module = create_feature_extractor(model, return_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return list(self.graph_module(x).values())\n",
    "\n",
    "\n",
    "class GraphExtractNet(nn.Module):\n",
    "    \"\"\" A standalone feature extraction wrapper that maps dict -> list or single tensor\n",
    "    NOTE:\n",
    "      * one can use feature_extractor directly if dictionary output is desired\n",
    "      * unlike FeatureGraphNet, this is intended to be used standalone and not with model feature_info\n",
    "      metadata for builtin feature extraction mode\n",
    "      * create_feature_extractor can be used directly if dictionary output is acceptable\n",
    "\n",
    "    Args:\n",
    "        model: model to extract features from\n",
    "        return_nodes: node names to return features from (dict or list)\n",
    "        squeeze_out: if only one output, and output in list format, flatten to single tensor\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            return_nodes: Union[Dict[str, str], List[str]],\n",
    "            squeeze_out: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.squeeze_out = squeeze_out\n",
    "        self.graph_module = create_feature_extractor(model, return_nodes)\n",
    "\n",
    "    def forward(self, x) -> Union[List[torch.Tensor], torch.Tensor]:\n",
    "        out = list(self.graph_module(x).values())\n",
    "        if self.squeeze_out and len(out) == 1:\n",
    "            return out[0]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a09b4822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:11.936848Z",
     "iopub.status.busy": "2024-04-20T16:43:11.936494Z",
     "iopub.status.idle": "2024-04-20T16:43:12.004626Z",
     "shell.execute_reply": "2024-04-20T16:43:12.003493Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.117484,
     "end_time": "2024-04-20T16:43:12.007019",
     "exception": false,
     "start_time": "2024-04-20T16:43:11.889535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ._builder.py\n",
    "\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Dict, Callable, Any, Tuple\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "# from timm.models._features import FeatureListNet, FeatureDictNet, FeatureHookNet, FeatureGetterNet\n",
    "from timm.models._features_fx import FeatureGraphNet\n",
    "from timm.models._helpers import load_state_dict\n",
    "from timm.models._hub import has_hf_hub, download_cached_file, check_cached_file, load_state_dict_from_hf\n",
    "from timm.models._manipulate import adapt_input_conv\n",
    "from timm.models._pretrained import PretrainedCfg\n",
    "from timm.models._prune import adapt_model_from_file\n",
    "from timm.models._registry import get_pretrained_cfg\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "# Global variables for rarely used pretrained checkpoint download progress and hash check.\n",
    "# Use set_pretrained_download_progress / set_pretrained_check_hash functions to toggle.\n",
    "_DOWNLOAD_PROGRESS = False\n",
    "_CHECK_HASH = False\n",
    "_USE_OLD_CACHE = int(os.environ.get('TIMM_USE_OLD_CACHE', 0)) > 0\n",
    "\n",
    "__all__ = ['set_pretrained_download_progress', 'set_pretrained_check_hash', 'load_custom_pretrained', 'load_pretrained',\n",
    "           'pretrained_cfg_for_features', 'resolve_pretrained_cfg', 'build_model_with_cfg']\n",
    "\n",
    "\n",
    "def _resolve_pretrained_source(pretrained_cfg):\n",
    "    cfg_source = pretrained_cfg.get('source', '')\n",
    "    pretrained_url = pretrained_cfg.get('url', None)\n",
    "    pretrained_file = pretrained_cfg.get('file', None)\n",
    "    pretrained_sd = pretrained_cfg.get('state_dict', None)\n",
    "    hf_hub_id = pretrained_cfg.get('hf_hub_id', None)\n",
    "\n",
    "    # resolve where to load pretrained weights from\n",
    "    load_from = ''\n",
    "    pretrained_loc = ''\n",
    "    if cfg_source == 'hf-hub' and has_hf_hub(necessary=True):\n",
    "        # hf-hub specified as source via model identifier\n",
    "        load_from = 'hf-hub'\n",
    "        assert hf_hub_id\n",
    "        pretrained_loc = hf_hub_id\n",
    "    else:\n",
    "        # default source == timm or unspecified\n",
    "        if pretrained_sd:\n",
    "            # direct state_dict pass through is the highest priority\n",
    "            load_from = 'state_dict'\n",
    "            pretrained_loc = pretrained_sd\n",
    "            assert isinstance(pretrained_loc, dict)\n",
    "        elif pretrained_file:\n",
    "            # file load override is the second-highest priority if set\n",
    "            load_from = 'file'\n",
    "            pretrained_loc = pretrained_file\n",
    "        else:\n",
    "            old_cache_valid = False\n",
    "            if _USE_OLD_CACHE:\n",
    "                # prioritized old cached weights if exists and env var enabled\n",
    "                old_cache_valid = check_cached_file(pretrained_url) if pretrained_url else False\n",
    "            if not old_cache_valid and hf_hub_id and has_hf_hub(necessary=True):\n",
    "                # hf-hub available as alternate weight source in default_cfg\n",
    "                load_from = 'hf-hub'\n",
    "                pretrained_loc = hf_hub_id\n",
    "            elif pretrained_url:\n",
    "                load_from = 'url'\n",
    "                pretrained_loc = pretrained_url\n",
    "\n",
    "    if load_from == 'hf-hub' and pretrained_cfg.get('hf_hub_filename', None):\n",
    "        # if a filename override is set, return tuple for location w/ (hub_id, filename)\n",
    "        pretrained_loc = pretrained_loc, pretrained_cfg['hf_hub_filename']\n",
    "    return load_from, pretrained_loc\n",
    "\n",
    "\n",
    "def set_pretrained_download_progress(enable=True):\n",
    "    \"\"\" Set download progress for pretrained weights on/off (globally). \"\"\"\n",
    "    global _DOWNLOAD_PROGRESS\n",
    "    _DOWNLOAD_PROGRESS = enable\n",
    "\n",
    "\n",
    "def set_pretrained_check_hash(enable=True):\n",
    "    \"\"\" Set hash checking for pretrained weights on/off (globally). \"\"\"\n",
    "    global _CHECK_HASH\n",
    "    _CHECK_HASH = enable\n",
    "\n",
    "\n",
    "def load_custom_pretrained(\n",
    "        model: nn.Module,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        load_fn: Optional[Callable] = None,\n",
    "):\n",
    "    r\"\"\"Loads a custom (read non .pth) weight file\n",
    "\n",
    "    Downloads checkpoint file into cache-dir like torch.hub based loaders, but calls\n",
    "    a passed in custom load fun, or the `load_pretrained` model member fn.\n",
    "\n",
    "    If the object is already present in `model_dir`, it's deserialized and returned.\n",
    "    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where\n",
    "    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.\n",
    "\n",
    "    Args:\n",
    "        model: The instantiated model to load weights into\n",
    "        pretrained_cfg (dict): Default pretrained model cfg\n",
    "        load_fn: An external standalone fn that loads weights into provided model, otherwise a fn named\n",
    "            'laod_pretrained' on the model will be called if it exists\n",
    "    \"\"\"\n",
    "    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n",
    "    if not pretrained_cfg:\n",
    "        _logger.warning(\"Invalid pretrained config, cannot load weights.\")\n",
    "        return\n",
    "\n",
    "    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n",
    "    if not load_from:\n",
    "        _logger.warning(\"No pretrained weights exist for this model. Using random initialization.\")\n",
    "        return\n",
    "    if load_from == 'hf-hub':\n",
    "        _logger.warning(\"Hugging Face hub not currently supported for custom load pretrained models.\")\n",
    "    elif load_from == 'url':\n",
    "        pretrained_loc = download_cached_file(\n",
    "            pretrained_loc,\n",
    "            check_hash=_CHECK_HASH,\n",
    "            progress=_DOWNLOAD_PROGRESS,\n",
    "        )\n",
    "\n",
    "    if load_fn is not None:\n",
    "        load_fn(model, pretrained_loc)\n",
    "    elif hasattr(model, 'load_pretrained'):\n",
    "        model.load_pretrained(pretrained_loc)\n",
    "    else:\n",
    "        _logger.warning(\"Valid function to load pretrained weights is not available, using random initialization.\")\n",
    "\n",
    "\n",
    "def load_pretrained(\n",
    "        model: nn.Module,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        num_classes: int = 1000,\n",
    "        in_chans: int = 3,\n",
    "        filter_fn: Optional[Callable] = None,\n",
    "        strict: bool = True,\n",
    "):\n",
    "    \"\"\" Load pretrained checkpoint\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module) : PyTorch model module\n",
    "        pretrained_cfg (Optional[Dict]): configuration for pretrained weights / target dataset\n",
    "        num_classes (int): num_classes for target model\n",
    "        in_chans (int): in_chans for target model\n",
    "        filter_fn (Optional[Callable]): state_dict filter fn for load (takes state_dict, model as args)\n",
    "        strict (bool): strict load of checkpoint\n",
    "\n",
    "    \"\"\"\n",
    "    pretrained_cfg = pretrained_cfg or getattr(model, 'pretrained_cfg', None)\n",
    "    if not pretrained_cfg:\n",
    "        raise RuntimeError(\"Invalid pretrained config, cannot load weights. Use `pretrained=False` for random init.\")\n",
    "\n",
    "    load_from, pretrained_loc = _resolve_pretrained_source(pretrained_cfg)\n",
    "    if load_from == 'state_dict':\n",
    "        _logger.info(f'Loading pretrained weights from state dict')\n",
    "        state_dict = pretrained_loc  # pretrained_loc is the actual state dict for this override\n",
    "    elif load_from == 'file':\n",
    "        _logger.info(f'Loading pretrained weights from file ({pretrained_loc})')\n",
    "        if pretrained_cfg.get('custom_load', False):\n",
    "            model.load_pretrained(pretrained_loc)\n",
    "            return\n",
    "        else:\n",
    "            state_dict = load_state_dict(pretrained_loc)\n",
    "    elif load_from == 'url':\n",
    "        _logger.info(f'Loading pretrained weights from url ({pretrained_loc})')\n",
    "        if pretrained_cfg.get('custom_load', False):\n",
    "            pretrained_loc = download_cached_file(\n",
    "                pretrained_loc,\n",
    "                progress=_DOWNLOAD_PROGRESS,\n",
    "                check_hash=_CHECK_HASH,\n",
    "            )\n",
    "            model.load_pretrained(pretrained_loc)\n",
    "            return\n",
    "        else:\n",
    "            state_dict = load_state_dict_from_url(\n",
    "                pretrained_loc,\n",
    "                map_location='cpu',\n",
    "                progress=_DOWNLOAD_PROGRESS,\n",
    "                check_hash=_CHECK_HASH,\n",
    "            )\n",
    "    elif load_from == 'hf-hub':\n",
    "        _logger.info(f'Loading pretrained weights from Hugging Face hub ({pretrained_loc})')\n",
    "        if isinstance(pretrained_loc, (list, tuple)):\n",
    "            state_dict = load_state_dict_from_hf(*pretrained_loc)\n",
    "        else:\n",
    "            state_dict = load_state_dict_from_hf(pretrained_loc)\n",
    "    else:\n",
    "        model_name = pretrained_cfg.get('architecture', 'this model')\n",
    "        raise RuntimeError(f\"No pretrained weights exist for {model_name}. Use `pretrained=False` for random init.\")\n",
    "\n",
    "    if filter_fn is not None:\n",
    "        try:\n",
    "            state_dict = filter_fn(state_dict, model)\n",
    "        except TypeError as e:\n",
    "            # for backwards compat with filter fn that take one arg\n",
    "            state_dict = filter_fn(state_dict)\n",
    "\n",
    "    input_convs = pretrained_cfg.get('first_conv', None)\n",
    "    if input_convs is not None and in_chans != 3:\n",
    "        if isinstance(input_convs, str):\n",
    "            input_convs = (input_convs,)\n",
    "        for input_conv_name in input_convs:\n",
    "            weight_name = input_conv_name + '.weight'\n",
    "            try:\n",
    "                state_dict[weight_name] = adapt_input_conv(in_chans, state_dict[weight_name])\n",
    "                _logger.info(\n",
    "                    f'Converted input conv {input_conv_name} pretrained weights from 3 to {in_chans} channel(s)')\n",
    "            except NotImplementedError as e:\n",
    "                del state_dict[weight_name]\n",
    "                strict = False\n",
    "                _logger.warning(\n",
    "                    f'Unable to convert pretrained {input_conv_name} weights, using random init for this layer.')\n",
    "\n",
    "    classifiers = pretrained_cfg.get('classifier', None)\n",
    "    label_offset = pretrained_cfg.get('label_offset', 0)\n",
    "    if classifiers is not None:\n",
    "        if isinstance(classifiers, str):\n",
    "            classifiers = (classifiers,)\n",
    "        if num_classes != pretrained_cfg['num_classes']:\n",
    "            for classifier_name in classifiers:\n",
    "                # completely discard fully connected if model num_classes doesn't match pretrained weights\n",
    "                state_dict.pop(classifier_name + '.weight', None)\n",
    "                state_dict.pop(classifier_name + '.bias', None)\n",
    "            strict = False\n",
    "        elif label_offset > 0:\n",
    "            for classifier_name in classifiers:\n",
    "                # special case for pretrained weights with an extra background class in pretrained weights\n",
    "                classifier_weight = state_dict[classifier_name + '.weight']\n",
    "                state_dict[classifier_name + '.weight'] = classifier_weight[label_offset:]\n",
    "                classifier_bias = state_dict[classifier_name + '.bias']\n",
    "                state_dict[classifier_name + '.bias'] = classifier_bias[label_offset:]\n",
    "\n",
    "    load_result = model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_result.missing_keys:\n",
    "        _logger.info(\n",
    "            f'Missing keys ({\", \".join(load_result.missing_keys)}) discovered while loading pretrained weights.'\n",
    "            f' This is expected if model is being adapted.')\n",
    "    if load_result.unexpected_keys:\n",
    "        _logger.warning(\n",
    "            f'Unexpected keys ({\", \".join(load_result.unexpected_keys)}) found while loading pretrained weights.'\n",
    "            f' This may be expected if model is being adapted.')\n",
    "\n",
    "\n",
    "def pretrained_cfg_for_features(pretrained_cfg):\n",
    "    pretrained_cfg = deepcopy(pretrained_cfg)\n",
    "    # remove default pretrained cfg fields that don't have much relevance for feature backbone\n",
    "    to_remove = ('num_classes', 'classifier', 'global_pool')  # add default final pool size?\n",
    "    for tr in to_remove:\n",
    "        pretrained_cfg.pop(tr, None)\n",
    "    return pretrained_cfg\n",
    "\n",
    "\n",
    "def _filter_kwargs(kwargs, names):\n",
    "    if not kwargs or not names:\n",
    "        return\n",
    "    for n in names:\n",
    "        kwargs.pop(n, None)\n",
    "\n",
    "\n",
    "def _update_default_model_kwargs(pretrained_cfg, kwargs, kwargs_filter):\n",
    "    \"\"\" Update the default_cfg and kwargs before passing to model\n",
    "\n",
    "    Args:\n",
    "        pretrained_cfg: input pretrained cfg (updated in-place)\n",
    "        kwargs: keyword args passed to model build fn (updated in-place)\n",
    "        kwargs_filter: keyword arg keys that must be removed before model __init__\n",
    "    \"\"\"\n",
    "    # Set model __init__ args that can be determined by default_cfg (if not already passed as kwargs)\n",
    "    default_kwarg_names = ('num_classes', 'global_pool', 'in_chans')\n",
    "    if pretrained_cfg.get('fixed_input_size', False):\n",
    "        # if fixed_input_size exists and is True, model takes an img_size arg that fixes its input size\n",
    "        default_kwarg_names += ('img_size',)\n",
    "\n",
    "    for n in default_kwarg_names:\n",
    "        # for legacy reasons, model __init__args uses img_size + in_chans as separate args while\n",
    "        # pretrained_cfg has one input_size=(C, H ,W) entry\n",
    "        if n == 'img_size':\n",
    "            input_size = pretrained_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[-2:])\n",
    "        elif n == 'in_chans':\n",
    "            input_size = pretrained_cfg.get('input_size', None)\n",
    "            if input_size is not None:\n",
    "                assert len(input_size) == 3\n",
    "                kwargs.setdefault(n, input_size[0])\n",
    "        elif n == 'num_classes':\n",
    "            default_val = pretrained_cfg.get(n, None)\n",
    "            # if default is < 0, don't pass through to model\n",
    "            if default_val is not None and default_val >= 0:\n",
    "                kwargs.setdefault(n, pretrained_cfg[n])\n",
    "        else:\n",
    "            default_val = pretrained_cfg.get(n, None)\n",
    "            if default_val is not None:\n",
    "                kwargs.setdefault(n, pretrained_cfg[n])\n",
    "\n",
    "    # Filter keyword args for task specific model variants (some 'features only' models, etc.)\n",
    "    _filter_kwargs(kwargs, names=kwargs_filter)\n",
    "\n",
    "\n",
    "def resolve_pretrained_cfg(\n",
    "        variant: str,\n",
    "        pretrained_cfg=None,\n",
    "        pretrained_cfg_overlay=None,\n",
    ") -> PretrainedCfg:\n",
    "    model_with_tag = variant\n",
    "    pretrained_tag = None\n",
    "    if pretrained_cfg:\n",
    "        if isinstance(pretrained_cfg, dict):\n",
    "            # pretrained_cfg dict passed as arg, validate by converting to PretrainedCfg\n",
    "            pretrained_cfg = PretrainedCfg(**pretrained_cfg)\n",
    "        elif isinstance(pretrained_cfg, str):\n",
    "            pretrained_tag = pretrained_cfg\n",
    "            pretrained_cfg = None\n",
    "\n",
    "    # fallback to looking up pretrained cfg in model registry by variant identifier\n",
    "    if not pretrained_cfg:\n",
    "        if pretrained_tag:\n",
    "            model_with_tag = '.'.join([variant, pretrained_tag])\n",
    "        pretrained_cfg = get_pretrained_cfg(model_with_tag)\n",
    "\n",
    "    if not pretrained_cfg:\n",
    "        _logger.warning(\n",
    "            f\"No pretrained configuration specified for {model_with_tag} model. Using a default.\"\n",
    "            f\" Please add a config to the model pretrained_cfg registry or pass explicitly.\")\n",
    "        pretrained_cfg = PretrainedCfg()  # instance with defaults\n",
    "\n",
    "    pretrained_cfg_overlay = pretrained_cfg_overlay or {}\n",
    "    if not pretrained_cfg.architecture:\n",
    "        pretrained_cfg_overlay.setdefault('architecture', variant)\n",
    "    pretrained_cfg = dataclasses.replace(pretrained_cfg, **pretrained_cfg_overlay)\n",
    "\n",
    "    return pretrained_cfg\n",
    "\n",
    "\n",
    "def build_model_with_cfg(\n",
    "        model_cls: Callable,\n",
    "        variant: str,\n",
    "        pretrained: bool,\n",
    "        pretrained_cfg: Optional[Dict] = None,\n",
    "        pretrained_cfg_overlay: Optional[Dict] = None,\n",
    "        model_cfg: Optional[Any] = None,\n",
    "        feature_cfg: Optional[Dict] = None,\n",
    "        pretrained_strict: bool = True,\n",
    "        pretrained_filter_fn: Optional[Callable] = None,\n",
    "        kwargs_filter: Optional[Tuple[str]] = None,\n",
    "        **kwargs,\n",
    "):\n",
    "    \"\"\" Build model with specified default_cfg and optional model_cfg\n",
    "\n",
    "    This helper fn aids in the construction of a model including:\n",
    "      * handling default_cfg and associated pretrained weight loading\n",
    "      * passing through optional model_cfg for models with config based arch spec\n",
    "      * features_only model adaptation\n",
    "      * pruning config / model adaptation\n",
    "\n",
    "    Args:\n",
    "        model_cls (nn.Module): model class\n",
    "        variant (str): model variant name\n",
    "        pretrained (bool): load pretrained weights\n",
    "        pretrained_cfg (dict): model's pretrained weight/task config\n",
    "        model_cfg (Optional[Dict]): model's architecture config\n",
    "        feature_cfg (Optional[Dict]: feature extraction adapter config\n",
    "        pretrained_strict (bool): load pretrained weights strictly\n",
    "        pretrained_filter_fn (Optional[Callable]): filter callable for pretrained weights\n",
    "        kwargs_filter (Optional[Tuple]): kwargs to filter before passing to model\n",
    "        **kwargs: model args passed through to model __init__\n",
    "    \"\"\"\n",
    "    pruned = kwargs.pop('pruned', False)\n",
    "    features = False\n",
    "    feature_cfg = feature_cfg or {}\n",
    "\n",
    "    # resolve and update model pretrained config and model kwargs\n",
    "    pretrained_cfg = resolve_pretrained_cfg(\n",
    "        variant,\n",
    "        pretrained_cfg=pretrained_cfg,\n",
    "        pretrained_cfg_overlay=pretrained_cfg_overlay\n",
    "    )\n",
    "\n",
    "    # FIXME converting back to dict, PretrainedCfg use should be propagated further, but not into model\n",
    "    pretrained_cfg = pretrained_cfg.to_dict()\n",
    "\n",
    "    _update_default_model_kwargs(pretrained_cfg, kwargs, kwargs_filter)\n",
    "\n",
    "    # Setup for feature extraction wrapper done at end of this fn\n",
    "    if kwargs.pop('features_only', False):\n",
    "        features = True\n",
    "        feature_cfg.setdefault('out_indices', (0, 1, 2, 3, 4))\n",
    "        if 'out_indices' in kwargs:\n",
    "            feature_cfg['out_indices'] = kwargs.pop('out_indices')\n",
    "\n",
    "    # Instantiate the model\n",
    "    if model_cfg is None:\n",
    "        model = model_cls(**kwargs)\n",
    "    else:\n",
    "        model = model_cls(cfg=model_cfg, **kwargs)\n",
    "    model.pretrained_cfg = pretrained_cfg\n",
    "    model.default_cfg = model.pretrained_cfg  # alias for backwards compat\n",
    "\n",
    "    if pruned:\n",
    "        model = adapt_model_from_file(model, variant)\n",
    "\n",
    "    # For classification models, check class attr, then kwargs, then default to 1k, otherwise 0 for feats\n",
    "    num_classes_pretrained = 0 if features else getattr(model, 'num_classes', kwargs.get('num_classes', 1000))\n",
    "    if pretrained:\n",
    "        load_pretrained(\n",
    "            model,\n",
    "            pretrained_cfg=pretrained_cfg,\n",
    "            num_classes=num_classes_pretrained,\n",
    "            in_chans=kwargs.get('in_chans', 3),\n",
    "            filter_fn=pretrained_filter_fn,\n",
    "            strict=pretrained_strict,\n",
    "        )\n",
    "\n",
    "    # Wrap the model in a feature extraction module if enabled\n",
    "    if features:\n",
    "        feature_cls = FeatureListNet\n",
    "        output_fmt = getattr(model, 'output_fmt', None)\n",
    "        if output_fmt is not None:\n",
    "            feature_cfg.setdefault('output_fmt', output_fmt)\n",
    "        if 'feature_cls' in feature_cfg:\n",
    "            feature_cls = feature_cfg.pop('feature_cls')\n",
    "            if isinstance(feature_cls, str):\n",
    "                feature_cls = feature_cls.lower()\n",
    "                if 'hook' in feature_cls:\n",
    "                    feature_cls = FeatureHookNet\n",
    "                elif feature_cls == 'dict':\n",
    "                    feature_cls = FeatureDictNet\n",
    "                elif feature_cls == 'fx':\n",
    "                    feature_cls = FeatureGraphNet\n",
    "                elif feature_cls == 'getter':\n",
    "                    feature_cls = FeatureGetterNet\n",
    "                else:\n",
    "                    assert False, f'Unknown feature class {feature_cls}'\n",
    "        model = feature_cls(model, **feature_cfg)\n",
    "        model.pretrained_cfg = pretrained_cfg_for_features(pretrained_cfg)  # add back pretrained cfg\n",
    "        model.default_cfg = model.pretrained_cfg  # alias for rename backwards compat (default_cfg -> pretrained_cfg)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8821a1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:12.102575Z",
     "iopub.status.busy": "2024-04-20T16:43:12.102178Z",
     "iopub.status.idle": "2024-04-20T16:43:12.151065Z",
     "shell.execute_reply": "2024-04-20T16:43:12.150002Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.099252,
     "end_time": "2024-04-20T16:43:12.153424",
     "exception": false,
     "start_time": "2024-04-20T16:43:12.054172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _manipulate.py\n",
    "\n",
    "import collections.abc\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from typing import Any, Callable, Dict, Iterator, Tuple, Type, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "__all__ = ['model_parameters', 'named_apply', 'named_modules', 'named_modules_with_params', 'adapt_input_conv',\n",
    "           'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq']\n",
    "\n",
    "\n",
    "def model_parameters(model: nn.Module, exclude_head: bool = False):\n",
    "    if exclude_head:\n",
    "        # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering\n",
    "        return [p for p in model.parameters()][:-2]\n",
    "    else:\n",
    "        return model.parameters()\n",
    "\n",
    "\n",
    "def named_apply(\n",
    "        fn: Callable,\n",
    "        module: nn.Module, name='',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    ") -> nn.Module:\n",
    "    if not depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    return module\n",
    "\n",
    "\n",
    "def named_modules(\n",
    "        module: nn.Module,\n",
    "        name: str = '',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    "):\n",
    "    if not depth_first and include_root:\n",
    "        yield name, module\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        yield from named_modules(\n",
    "            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        yield name, module\n",
    "\n",
    "\n",
    "def named_modules_with_params(\n",
    "        module: nn.Module,\n",
    "        name: str = '',\n",
    "        depth_first: bool = True,\n",
    "        include_root: bool = False,\n",
    "):\n",
    "    if module._parameters and not depth_first and include_root:\n",
    "        yield name, module\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = '.'.join((name, child_name)) if name else child_name\n",
    "        yield from named_modules_with_params(\n",
    "            module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if module._parameters and depth_first and include_root:\n",
    "        yield name, module\n",
    "\n",
    "\n",
    "MATCH_PREV_GROUP = (99999,)\n",
    "\n",
    "\n",
    "def group_with_matcher(\n",
    "        named_objects: Iterator[Tuple[str, Any]],\n",
    "        group_matcher: Union[Dict, Callable],\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False\n",
    "):\n",
    "    if isinstance(group_matcher, dict):\n",
    "        # dictionary matcher contains a dict of raw-string regex expr that must be compiled\n",
    "        compiled = []\n",
    "        for group_ordinal, (group_name, mspec) in enumerate(group_matcher.items()):\n",
    "            if mspec is None:\n",
    "                continue\n",
    "            # map all matching specifications into 3-tuple (compiled re, prefix, suffix)\n",
    "            if isinstance(mspec, (tuple, list)):\n",
    "                # multi-entry match specifications require each sub-spec to be a 2-tuple (re, suffix)\n",
    "                for sspec in mspec:\n",
    "                    compiled += [(re.compile(sspec[0]), (group_ordinal,), sspec[1])]\n",
    "            else:\n",
    "                compiled += [(re.compile(mspec), (group_ordinal,), None)]\n",
    "        group_matcher = compiled\n",
    "\n",
    "    def _get_grouping(name):\n",
    "        if isinstance(group_matcher, (list, tuple)):\n",
    "            for match_fn, prefix, suffix in group_matcher:\n",
    "                r = match_fn.match(name)\n",
    "                if r:\n",
    "                    parts = (prefix, r.groups(), suffix)\n",
    "                    # map all tuple elem to int for numeric sort, filter out None entries\n",
    "                    return tuple(map(float, chain.from_iterable(filter(None, parts))))\n",
    "            return float('inf'),  # un-matched layers (neck, head) mapped to largest ordinal\n",
    "        else:\n",
    "            ord = group_matcher(name)\n",
    "            if not isinstance(ord, collections.abc.Iterable):\n",
    "                return ord,\n",
    "            return tuple(ord)\n",
    "\n",
    "    # map layers into groups via ordinals (ints or tuples of ints) from matcher\n",
    "    grouping = defaultdict(list)\n",
    "    for k, v in named_objects:\n",
    "        grouping[_get_grouping(k)].append(v if return_values else k)\n",
    "\n",
    "    # remap to integers\n",
    "    layer_id_to_param = defaultdict(list)\n",
    "    lid = -1\n",
    "    for k in sorted(filter(lambda x: x is not None, grouping.keys())):\n",
    "        if lid < 0 or k[-1] != MATCH_PREV_GROUP[0]:\n",
    "            lid += 1\n",
    "        layer_id_to_param[lid].extend(grouping[k])\n",
    "\n",
    "    if reverse:\n",
    "        assert not return_values, \"reverse mapping only sensible for name output\"\n",
    "        # output reverse mapping\n",
    "        param_to_layer_id = {}\n",
    "        for lid, lm in layer_id_to_param.items():\n",
    "            for n in lm:\n",
    "                param_to_layer_id[n] = lid\n",
    "        return param_to_layer_id\n",
    "\n",
    "    return layer_id_to_param\n",
    "\n",
    "\n",
    "def group_parameters(\n",
    "        module: nn.Module,\n",
    "        group_matcher,\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False,\n",
    "):\n",
    "    return group_with_matcher(\n",
    "        module.named_parameters(), group_matcher, return_values=return_values, reverse=reverse)\n",
    "\n",
    "\n",
    "def group_modules(\n",
    "        module: nn.Module,\n",
    "        group_matcher,\n",
    "        return_values: bool = False,\n",
    "        reverse: bool = False,\n",
    "):\n",
    "    return group_with_matcher(\n",
    "        named_modules_with_params(module), group_matcher, return_values=return_values, reverse=reverse)\n",
    "\n",
    "\n",
    "def flatten_modules(\n",
    "        named_modules: Iterator[Tuple[str, nn.Module]],\n",
    "        depth: int = 1,\n",
    "        prefix: Union[str, Tuple[str, ...]] = '',\n",
    "        module_types: Union[str, Tuple[Type[nn.Module]]] = 'sequential',\n",
    "):\n",
    "    prefix_is_tuple = isinstance(prefix, tuple)\n",
    "    if isinstance(module_types, str):\n",
    "        if module_types == 'container':\n",
    "            module_types = (nn.Sequential, nn.ModuleList, nn.ModuleDict)\n",
    "        else:\n",
    "            module_types = (nn.Sequential,)\n",
    "    for name, module in named_modules:\n",
    "        if depth and isinstance(module, module_types):\n",
    "            yield from flatten_modules(\n",
    "                module.named_children(),\n",
    "                depth - 1,\n",
    "                prefix=(name,) if prefix_is_tuple else name,\n",
    "                module_types=module_types,\n",
    "            )\n",
    "        else:\n",
    "            if prefix_is_tuple:\n",
    "                name = prefix + (name,)\n",
    "                yield name, module\n",
    "            else:\n",
    "                if prefix:\n",
    "                    name = '.'.join([prefix, name])\n",
    "                yield name, module\n",
    "\n",
    "\n",
    "def checkpoint_seq(\n",
    "        functions,\n",
    "        x,\n",
    "        every=1,\n",
    "        flatten=False,\n",
    "        skip_last=False,\n",
    "        preserve_rng_state=True\n",
    "):\n",
    "    r\"\"\"A helper function for checkpointing sequential models.\n",
    "\n",
    "    Sequential models execute a list of modules/functions in order\n",
    "    (sequentially). Therefore, we can divide such a sequence into segments\n",
    "    and checkpoint each segment. All segments except run in :func:`torch.no_grad`\n",
    "    manner, i.e., not storing the intermediate activations. The inputs of each\n",
    "    checkpointed segment will be saved for re-running the segment in the backward pass.\n",
    "\n",
    "    See :func:`~torch.utils.checkpoint.checkpoint` on how checkpointing works.\n",
    "\n",
    "    .. warning::\n",
    "        Checkpointing currently only supports :func:`torch.autograd.backward`\n",
    "        and only if its `inputs` argument is not passed. :func:`torch.autograd.grad`\n",
    "        is not supported.\n",
    "\n",
    "    .. warning:\n",
    "        At least one of the inputs needs to have :code:`requires_grad=True` if\n",
    "        grads are needed for model inputs, otherwise the checkpointed part of the\n",
    "        model won't have gradients.\n",
    "\n",
    "    Args:\n",
    "        functions: A :class:`torch.nn.Sequential` or the list of modules or functions to run sequentially.\n",
    "        x: A Tensor that is input to :attr:`functions`\n",
    "        every: checkpoint every-n functions (default: 1)\n",
    "        flatten (bool): flatten nn.Sequential of nn.Sequentials\n",
    "        skip_last (bool): skip checkpointing the last function in the sequence if True\n",
    "        preserve_rng_state (bool, optional, default=True):  Omit stashing and restoring\n",
    "            the RNG state during each checkpoint.\n",
    "\n",
    "    Returns:\n",
    "        Output of running :attr:`functions` sequentially on :attr:`*inputs`\n",
    "\n",
    "    Example:\n",
    "        >>> model = nn.Sequential(...)\n",
    "        >>> input_var = checkpoint_seq(model, input_var, every=2)\n",
    "    \"\"\"\n",
    "    def run_function(start, end, functions):\n",
    "        def forward(_x):\n",
    "            for j in range(start, end + 1):\n",
    "                _x = functions[j](_x)\n",
    "            return _x\n",
    "        return forward\n",
    "\n",
    "    if isinstance(functions, torch.nn.Sequential):\n",
    "        functions = functions.children()\n",
    "    if flatten:\n",
    "        functions = chain.from_iterable(functions)\n",
    "    if not isinstance(functions, (tuple, list)):\n",
    "        functions = tuple(functions)\n",
    "\n",
    "    num_checkpointed = len(functions)\n",
    "    if skip_last:\n",
    "        num_checkpointed -= 1\n",
    "    end = -1\n",
    "    for start in range(0, num_checkpointed, every):\n",
    "        end = min(start + every - 1, num_checkpointed - 1)\n",
    "        x = checkpoint(run_function(start, end, functions), x, preserve_rng_state=preserve_rng_state)\n",
    "    if skip_last:\n",
    "        return run_function(end + 1, len(functions) - 1, functions)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def adapt_input_conv(in_chans, conv_weight):\n",
    "    conv_type = conv_weight.dtype\n",
    "    conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU\n",
    "    O, I, J, K = conv_weight.shape\n",
    "    if in_chans == 1:\n",
    "        if I > 3:\n",
    "            assert conv_weight.shape[1] % 3 == 0\n",
    "            # For models with space2depth stems\n",
    "            conv_weight = conv_weight.reshape(O, I // 3, 3, J, K)\n",
    "            conv_weight = conv_weight.sum(dim=2, keepdim=False)\n",
    "        else:\n",
    "            conv_weight = conv_weight.sum(dim=1, keepdim=True)\n",
    "    elif in_chans != 3:\n",
    "        if I != 3:\n",
    "            raise NotImplementedError('Weight format not supported by conversion.')\n",
    "        else:\n",
    "            # NOTE this strategy should be better than random init, but there could be other combinations of\n",
    "            # the original RGB input layer weights that'd work better for specific cases.\n",
    "            repeat = int(math.ceil(in_chans / 3))\n",
    "            conv_weight = conv_weight.repeat(1, repeat, 1, 1)[:, :in_chans, :, :]\n",
    "            conv_weight *= (3 / float(in_chans))\n",
    "    conv_weight = conv_weight.to(conv_type)\n",
    "    return conv_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "364eb3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:12.248407Z",
     "iopub.status.busy": "2024-04-20T16:43:12.248003Z",
     "iopub.status.idle": "2024-04-20T16:43:12.273164Z",
     "shell.execute_reply": "2024-04-20T16:43:12.272160Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.075276,
     "end_time": "2024-04-20T16:43:12.275352",
     "exception": false,
     "start_time": "2024-04-20T16:43:12.200076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass, field, replace, asdict\n",
    "from typing import Any, Deque, Dict, Tuple, Optional, Union\n",
    "\n",
    "\n",
    "__all__ = ['PretrainedCfg', 'filter_pretrained_cfg', 'DefaultCfg']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PretrainedCfg:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # weight source locations\n",
    "    url: Optional[Union[str, Tuple[str, str]]] = None  # remote URL\n",
    "    file: Optional[str] = None  # local / shared filesystem path\n",
    "    state_dict: Optional[Dict[str, Any]] = None  # in-memory state dict\n",
    "    hf_hub_id: Optional[str] = None  # Hugging Face Hub model id ('organization/model')\n",
    "    hf_hub_filename: Optional[str] = None  # Hugging Face Hub filename (overrides default)\n",
    "\n",
    "    source: Optional[str] = None  # source of cfg / weight location used (url, file, hf-hub)\n",
    "    architecture: Optional[str] = None  # architecture variant can be set when not implicit\n",
    "    tag: Optional[str] = None  # pretrained tag of source\n",
    "    custom_load: bool = False  # use custom model specific model.load_pretrained() (ie for npz files)\n",
    "\n",
    "    # input / data config\n",
    "    input_size: Tuple[int, int, int] = (3, 224, 224)\n",
    "    test_input_size: Optional[Tuple[int, int, int]] = None\n",
    "    min_input_size: Optional[Tuple[int, int, int]] = None\n",
    "    fixed_input_size: bool = False\n",
    "    interpolation: str = 'bicubic'\n",
    "    crop_pct: float = 0.875\n",
    "    test_crop_pct: Optional[float] = None\n",
    "    crop_mode: str = 'center'\n",
    "    mean: Tuple[float, ...] = (0.485, 0.456, 0.406)\n",
    "    std: Tuple[float, ...] = (0.229, 0.224, 0.225)\n",
    "\n",
    "    # head / classifier config and meta-data\n",
    "    num_classes: int = 1000\n",
    "    label_offset: Optional[int] = None\n",
    "    label_names: Optional[Tuple[str]] = None\n",
    "    label_descriptions: Optional[Dict[str, str]] = None\n",
    "\n",
    "    # model attributes that vary with above or required for pretrained adaptation\n",
    "    pool_size: Optional[Tuple[int, ...]] = None\n",
    "    test_pool_size: Optional[Tuple[int, ...]] = None\n",
    "    first_conv: Optional[str] = None\n",
    "    classifier: Optional[str] = None\n",
    "\n",
    "    license: Optional[str] = None\n",
    "    description: Optional[str] = None\n",
    "    origin_url: Optional[str] = None\n",
    "    paper_name: Optional[str] = None\n",
    "    paper_ids: Optional[Union[str, Tuple[str]]] = None\n",
    "    notes: Optional[Tuple[str]] = None\n",
    "\n",
    "    @property\n",
    "    def has_weights(self):\n",
    "        return self.url or self.file or self.hf_hub_id\n",
    "\n",
    "    def to_dict(self, remove_source=False, remove_null=True):\n",
    "        return filter_pretrained_cfg(\n",
    "            asdict(self),\n",
    "            remove_source=remove_source,\n",
    "            remove_null=remove_null\n",
    "        )\n",
    "\n",
    "\n",
    "def filter_pretrained_cfg(cfg, remove_source=False, remove_null=True):\n",
    "    filtered_cfg = {}\n",
    "    keep_null = {'pool_size', 'first_conv', 'classifier'}  # always keep these keys, even if none\n",
    "    for k, v in cfg.items():\n",
    "        if remove_source and k in {'url', 'file', 'hf_hub_id', 'hf_hub_id', 'hf_hub_filename', 'source'}:\n",
    "            continue\n",
    "        if remove_null and v is None and k not in keep_null:\n",
    "            continue\n",
    "        filtered_cfg[k] = v\n",
    "    return filtered_cfg\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DefaultCfg:\n",
    "    tags: Deque[str] = field(default_factory=deque)  # priority queue of tags (first is default)\n",
    "    cfgs: Dict[str, PretrainedCfg] = field(default_factory=dict)  # pretrained cfgs by tag\n",
    "    is_pretrained: bool = False  # at least one of the configs has a pretrained source set\n",
    "\n",
    "    @property\n",
    "    def default(self):\n",
    "        return self.cfgs[self.tags[0]]\n",
    "\n",
    "    @property\n",
    "    def default_with_tag(self):\n",
    "        tag = self.tags[0]\n",
    "        return tag, self.cfgs[tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20a29ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:12.371459Z",
     "iopub.status.busy": "2024-04-20T16:43:12.371063Z",
     "iopub.status.idle": "2024-04-20T16:43:12.431716Z",
     "shell.execute_reply": "2024-04-20T16:43:12.430900Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.111551,
     "end_time": "2024-04-20T16:43:12.434189",
     "exception": false,
     "start_time": "2024-04-20T16:43:12.322638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _registry.py\n",
    "\n",
    "\n",
    "\"\"\" Model Registry\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "\n",
    "import fnmatch\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from collections import defaultdict, deque\n",
    "from copy import deepcopy\n",
    "from dataclasses import replace\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Set, Sequence, Union, Tuple\n",
    "\n",
    "# from ._pretrained import PretrainedCfg, DefaultCfg\n",
    "\n",
    "__all__ = [\n",
    "    'split_model_name_tag', 'get_arch_name', 'register_model', 'generate_default_cfgs',\n",
    "    'list_models', 'list_pretrained', 'is_model', 'model_entrypoint', 'list_modules', 'is_model_in_modules',\n",
    "    'get_pretrained_cfg_value', 'is_model_pretrained'\n",
    "]\n",
    "\n",
    "_module_to_models: Dict[str, Set[str]] = defaultdict(set)  # dict of sets to check membership of model in module\n",
    "_model_to_module: Dict[str, str] = {}  # mapping of model names to module names\n",
    "_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to architecture entrypoint fns\n",
    "_model_has_pretrained: Set[str] = set()  # set of model names that have pretrained weight url present\n",
    "_model_default_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch -> default cfg objects\n",
    "_model_pretrained_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch.tag -> pretrained cfgs\n",
    "_model_with_tags: Dict[str, List[str]] = defaultdict(list)  # shortcut to map each model arch to all model + tag names\n",
    "_module_to_deprecated_models: Dict[str, Dict[str, Optional[str]]] = defaultdict(dict)\n",
    "_deprecated_models: Dict[str, Optional[str]] = {}\n",
    "\n",
    "\n",
    "def split_model_name_tag(model_name: str, no_tag: str = '') -> Tuple[str, str]:\n",
    "    model_name, *tag_list = model_name.split('.', 1)\n",
    "    tag = tag_list[0] if tag_list else no_tag\n",
    "    return model_name, tag\n",
    "\n",
    "\n",
    "def get_arch_name(model_name: str) -> str:\n",
    "    return split_model_name_tag(model_name)[0]\n",
    "\n",
    "\n",
    "def generate_default_cfgs(cfgs: Dict[str, Union[Dict[str, Any], PretrainedCfg]]):\n",
    "    out = defaultdict(DefaultCfg)\n",
    "    default_set = set()  # no tag and tags ending with * are prioritized as default\n",
    "\n",
    "    for k, v in cfgs.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = PretrainedCfg(**v)\n",
    "        has_weights = v.has_weights\n",
    "\n",
    "        model, tag = split_model_name_tag(k)\n",
    "        is_default_set = model in default_set\n",
    "        priority = (has_weights and not tag) or (tag.endswith('*') and not is_default_set)\n",
    "        tag = tag.strip('*')\n",
    "\n",
    "        default_cfg = out[model]\n",
    "\n",
    "        if priority:\n",
    "            default_cfg.tags.appendleft(tag)\n",
    "            default_set.add(model)\n",
    "        elif has_weights and not default_cfg.is_pretrained:\n",
    "            default_cfg.tags.appendleft(tag)\n",
    "        else:\n",
    "            default_cfg.tags.append(tag)\n",
    "\n",
    "        if has_weights:\n",
    "            default_cfg.is_pretrained = True\n",
    "\n",
    "        default_cfg.cfgs[tag] = v\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def register_model(fn: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    # lookup containing module\n",
    "    mod = sys.modules[fn.__module__]\n",
    "    module_name_split = fn.__module__.split('.')\n",
    "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
    "\n",
    "    # add model to __all__ in module\n",
    "    model_name = fn.__name__\n",
    "    if hasattr(mod, '__all__'):\n",
    "        mod.__all__.append(model_name)\n",
    "    else:\n",
    "        mod.__all__ = [model_name]  # type: ignore\n",
    "\n",
    "    # add entries to registry dict/sets\n",
    "    if model_name in _model_entrypoints:\n",
    "        warnings.warn(\n",
    "            f'Overwriting {model_name} in registry with {fn.__module__}.{model_name}. This is because the name being '\n",
    "            'registered conflicts with an existing name. Please check if this is not expected.',\n",
    "            stacklevel=2,\n",
    "        )\n",
    "    _model_entrypoints[model_name] = fn\n",
    "    _model_to_module[model_name] = module_name\n",
    "    _module_to_models[module_name].add(model_name)\n",
    "    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:\n",
    "        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n",
    "        # entrypoints or non-matching combos\n",
    "        default_cfg = mod.default_cfgs[model_name]\n",
    "        if not isinstance(default_cfg, DefaultCfg):\n",
    "            # new style default cfg dataclass w/ multiple entries per model-arch\n",
    "            assert isinstance(default_cfg, dict)\n",
    "            # old style cfg dict per model-arch\n",
    "            pretrained_cfg = PretrainedCfg(**default_cfg)\n",
    "            default_cfg = DefaultCfg(tags=deque(['']), cfgs={'': pretrained_cfg})\n",
    "\n",
    "        for tag_idx, tag in enumerate(default_cfg.tags):\n",
    "            is_default = tag_idx == 0\n",
    "            pretrained_cfg = default_cfg.cfgs[tag]\n",
    "            model_name_tag = '.'.join([model_name, tag]) if tag else model_name\n",
    "            replace_items = dict(architecture=model_name, tag=tag if tag else None)\n",
    "            if pretrained_cfg.hf_hub_id and pretrained_cfg.hf_hub_id == 'timm/':\n",
    "                # auto-complete hub name w/ architecture.tag\n",
    "                replace_items['hf_hub_id'] = pretrained_cfg.hf_hub_id + model_name_tag\n",
    "            pretrained_cfg = replace(pretrained_cfg, **replace_items)\n",
    "\n",
    "            if is_default:\n",
    "                _model_pretrained_cfgs[model_name] = pretrained_cfg\n",
    "                if pretrained_cfg.has_weights:\n",
    "                    # add tagless entry if it's default and has weights\n",
    "                    _model_has_pretrained.add(model_name)\n",
    "\n",
    "            if tag:\n",
    "                _model_pretrained_cfgs[model_name_tag] = pretrained_cfg\n",
    "                if pretrained_cfg.has_weights:\n",
    "                    # add model w/ tag if tag is valid\n",
    "                    _model_has_pretrained.add(model_name_tag)\n",
    "                _model_with_tags[model_name].append(model_name_tag)\n",
    "            else:\n",
    "                _model_with_tags[model_name].append(model_name)  # has empty tag (to slowly remove these instances)\n",
    "\n",
    "        _model_default_cfgs[model_name] = default_cfg\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def _deprecated_model_shim(deprecated_name: str, current_fn: Callable = None, current_tag: str = ''):\n",
    "    def _fn(pretrained=False, **kwargs):\n",
    "        assert current_fn is not None,  f'Model {deprecated_name} has been removed with no replacement.'\n",
    "        current_name = '.'.join([current_fn.__name__, current_tag]) if current_tag else current_fn.__name__\n",
    "        warnings.warn(f'Mapping deprecated model name {deprecated_name} to current {current_name}.', stacklevel=2)\n",
    "        pretrained_cfg = kwargs.pop('pretrained_cfg', None)\n",
    "        return current_fn(pretrained=pretrained, pretrained_cfg=pretrained_cfg or current_tag, **kwargs)\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def register_model_deprecations(module_name: str, deprecation_map: Dict[str, Optional[str]]):\n",
    "    mod = sys.modules[module_name]\n",
    "    module_name_split = module_name.split('.')\n",
    "    module_name = module_name_split[-1] if len(module_name_split) else ''\n",
    "\n",
    "    for deprecated, current in deprecation_map.items():\n",
    "        if hasattr(mod, '__all__'):\n",
    "            mod.__all__.append(deprecated)\n",
    "        current_fn = None\n",
    "        current_tag = ''\n",
    "        if current:\n",
    "            current_name, current_tag = split_model_name_tag(current)\n",
    "            current_fn = getattr(mod, current_name)\n",
    "        deprecated_entrypoint_fn = _deprecated_model_shim(deprecated, current_fn, current_tag)\n",
    "        setattr(mod, deprecated, deprecated_entrypoint_fn)\n",
    "        _model_entrypoints[deprecated] = deprecated_entrypoint_fn\n",
    "        _model_to_module[deprecated] = module_name\n",
    "        _module_to_models[module_name].add(deprecated)\n",
    "        _deprecated_models[deprecated] = current\n",
    "        _module_to_deprecated_models[module_name][deprecated] = current\n",
    "\n",
    "\n",
    "def _natural_key(string_: str) -> List[Union[int, str]]:\n",
    "    \"\"\"See https://blog.codinghorror.com/sorting-for-humans-natural-sort-order/\"\"\"\n",
    "    return [int(s) if s.isdigit() else s for s in re.split(r'(\\d+)', string_.lower())]\n",
    "\n",
    "\n",
    "def _expand_filter(filter: str):\n",
    "    \"\"\" expand a 'base_filter' to 'base_filter.*' if no tag portion\"\"\"\n",
    "    filter_base, filter_tag = split_model_name_tag(filter)\n",
    "    if not filter_tag:\n",
    "        return ['.'.join([filter_base, '*']), filter]\n",
    "    else:\n",
    "        return [filter]\n",
    "\n",
    "\n",
    "def list_models(\n",
    "        filter: Union[str, List[str]] = '',\n",
    "        module: str = '',\n",
    "        pretrained: bool = False,\n",
    "        exclude_filters: Union[str, List[str]] = '',\n",
    "        name_matches_cfg: bool = False,\n",
    "        include_tags: Optional[bool] = None,\n",
    ") -> List[str]:\n",
    "    \"\"\" Return list of available model names, sorted alphabetically\n",
    "\n",
    "    Args:\n",
    "        filter - Wildcard filter string that works with fnmatch\n",
    "        module - Limit model selection to a specific submodule (ie 'vision_transformer')\n",
    "        pretrained - Include only models with valid pretrained weights if True\n",
    "        exclude_filters - Wildcard filters to exclude models after including them with filter\n",
    "        name_matches_cfg - Include only models w/ model_name matching default_cfg name (excludes some aliases)\n",
    "        include_tags - Include pretrained tags in model names (model.tag). If None, defaults\n",
    "            set to True when pretrained=True else False (default: None)\n",
    "\n",
    "    Returns:\n",
    "        models - The sorted list of models\n",
    "\n",
    "    Example:\n",
    "        model_list('gluon_resnet*') -- returns all models starting with 'gluon_resnet'\n",
    "        model_list('*resnext*, 'resnet') -- returns all models with 'resnext' in 'resnet' module\n",
    "    \"\"\"\n",
    "    if filter:\n",
    "        include_filters = filter if isinstance(filter, (tuple, list)) else [filter]\n",
    "    else:\n",
    "        include_filters = []\n",
    "\n",
    "    if include_tags is None:\n",
    "        # FIXME should this be default behaviour? or default to include_tags=True?\n",
    "        include_tags = pretrained\n",
    "\n",
    "    all_models: Set[str] = _module_to_models[module] if module else set(_model_entrypoints.keys())\n",
    "    all_models = all_models - _deprecated_models.keys()  # remove deprecated models from listings\n",
    "\n",
    "    if include_tags:\n",
    "        # expand model names to include names w/ pretrained tags\n",
    "        models_with_tags: Set[str] = set()\n",
    "        for m in all_models:\n",
    "            models_with_tags.update(_model_with_tags[m])\n",
    "        all_models = models_with_tags\n",
    "        # expand include and exclude filters to include a '.*' for proper match if no tags in filter\n",
    "        include_filters = [ef for f in include_filters for ef in _expand_filter(f)]\n",
    "        exclude_filters = [ef for f in exclude_filters for ef in _expand_filter(f)]\n",
    "\n",
    "    if include_filters:\n",
    "        models: Set[str] = set()\n",
    "        for f in include_filters:\n",
    "            include_models = fnmatch.filter(all_models, f)  # include these models\n",
    "            if len(include_models):\n",
    "                models = models.union(include_models)\n",
    "    else:\n",
    "        models = all_models\n",
    "\n",
    "    if exclude_filters:\n",
    "        if not isinstance(exclude_filters, (tuple, list)):\n",
    "            exclude_filters = [exclude_filters]\n",
    "        for xf in exclude_filters:\n",
    "            exclude_models = fnmatch.filter(models, xf)  # exclude these models\n",
    "            if len(exclude_models):\n",
    "                models = models.difference(exclude_models)\n",
    "\n",
    "    if pretrained:\n",
    "        models = _model_has_pretrained.intersection(models)\n",
    "\n",
    "    if name_matches_cfg:\n",
    "        models = set(_model_pretrained_cfgs).intersection(models)\n",
    "\n",
    "    return sorted(models, key=_natural_key)\n",
    "\n",
    "\n",
    "def list_pretrained(\n",
    "        filter: Union[str, List[str]] = '',\n",
    "        exclude_filters: str = '',\n",
    ") -> List[str]:\n",
    "    return list_models(\n",
    "        filter=filter,\n",
    "        pretrained=True,\n",
    "        exclude_filters=exclude_filters,\n",
    "        include_tags=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_deprecated_models(module: str = '') -> Dict[str, str]:\n",
    "    all_deprecated = _module_to_deprecated_models[module] if module else _deprecated_models\n",
    "    return deepcopy(all_deprecated)\n",
    "\n",
    "\n",
    "def is_model(model_name: str) -> bool:\n",
    "    \"\"\" Check if a model name exists\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    return arch_name in _model_entrypoints\n",
    "\n",
    "\n",
    "def model_entrypoint(model_name: str, module_filter: Optional[str] = None) -> Callable[..., Any]:\n",
    "    \"\"\"Fetch a model entrypoint for specified model name\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    if module_filter and arch_name not in _module_to_models.get(module_filter, {}):\n",
    "        raise RuntimeError(f'Model ({model_name} not found in module {module_filter}.')\n",
    "    return _model_entrypoints[arch_name]\n",
    "\n",
    "\n",
    "def list_modules() -> List[str]:\n",
    "    \"\"\" Return list of module names that contain models / model entrypoints\n",
    "    \"\"\"\n",
    "    modules = _module_to_models.keys()\n",
    "    return sorted(modules)\n",
    "\n",
    "\n",
    "def is_model_in_modules(\n",
    "        model_name: str, module_names: Union[Tuple[str, ...], List[str], Set[str]]\n",
    ") -> bool:\n",
    "    \"\"\"Check if a model exists within a subset of modules\n",
    "\n",
    "    Args:\n",
    "        model_name - name of model to check\n",
    "        module_names - names of modules to search in\n",
    "    \"\"\"\n",
    "    arch_name = get_arch_name(model_name)\n",
    "    assert isinstance(module_names, (tuple, list, set))\n",
    "    return any(arch_name in _module_to_models[n] for n in module_names)\n",
    "\n",
    "\n",
    "def is_model_pretrained(model_name: str) -> bool:\n",
    "    return model_name in _model_has_pretrained\n",
    "\n",
    "\n",
    "def get_pretrained_cfg(model_name: str, allow_unregistered: bool = True) -> Optional[PretrainedCfg]:\n",
    "    if model_name in _model_pretrained_cfgs:\n",
    "        return deepcopy(_model_pretrained_cfgs[model_name])\n",
    "    arch_name, tag = split_model_name_tag(model_name)\n",
    "    if arch_name in _model_default_cfgs:\n",
    "        # if model arch exists, but the tag is wrong, error out\n",
    "        raise RuntimeError(f'Invalid pretrained tag ({tag}) for {arch_name}.')\n",
    "    if allow_unregistered:\n",
    "        # if model arch doesn't exist, it has no pretrained_cfg registered, allow a default to be created\n",
    "        return None\n",
    "    raise RuntimeError(f'Model architecture ({arch_name}) has no pretrained cfg registered.')\n",
    "\n",
    "\n",
    "def get_pretrained_cfg_value(model_name: str, cfg_key: str) -> Optional[Any]:\n",
    "    \"\"\" Get a specific model default_cfg value by key. None if key doesn't exist.\n",
    "    \"\"\"\n",
    "    cfg = get_pretrained_cfg(model_name, allow_unregistered=False)\n",
    "    return getattr(cfg, cfg_key, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c81de1c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:12.534143Z",
     "iopub.status.busy": "2024-04-20T16:43:12.533741Z",
     "iopub.status.idle": "2024-04-20T16:43:12.944776Z",
     "shell.execute_reply": "2024-04-20T16:43:12.943889Z"
    },
    "papermill": {
     "duration": 0.466294,
     "end_time": "2024-04-20T16:43:12.947452",
     "exception": false,
     "start_time": "2024-04-20T16:43:12.481158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" MaxVit and CoAtNet Vision Transformer - CNN Hybrids in PyTorch\n",
    "\n",
    "This is a from-scratch implementation of both CoAtNet and MaxVit in PyTorch.\n",
    "\n",
    "99% of the implementation was done from papers, however last minute some adjustments were made\n",
    "based on the (as yet unfinished?) public code release https://github.com/google-research/maxvit\n",
    "\n",
    "There are multiple sets of models defined for both architectures. Typically, names with a\n",
    " `_rw` suffix are my own original configs prior to referencing https://github.com/google-research/maxvit.\n",
    "These configs work well and appear to be a bit faster / lower resource than the paper.\n",
    "\n",
    "The models without extra prefix / suffix' (coatnet_0_224, maxvit_tiny_224, etc), are intended to\n",
    "match paper, BUT, without any official pretrained weights it's difficult to confirm a 100% match.\n",
    "\n",
    "Papers:\n",
    "\n",
    "MaxViT: Multi-Axis Vision Transformer - https://arxiv.org/abs/2204.01697\n",
    "@article{tu2022maxvit,\n",
    "  title={MaxViT: Multi-Axis Vision Transformer},\n",
    "  author={Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},\n",
    "  journal={ECCV},\n",
    "  year={2022},\n",
    "}\n",
    "\n",
    "CoAtNet: Marrying Convolution and Attention for All Data Sizes - https://arxiv.org/abs/2106.04803\n",
    "@article{DBLP:journals/corr/abs-2106-04803,\n",
    "  author    = {Zihang Dai and Hanxiao Liu and Quoc V. Le and Mingxing Tan},\n",
    "  title     = {CoAtNet: Marrying Convolution and Attention for All Data Sizes},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/2106.04803},\n",
    "  year      = {2021}\n",
    "}\n",
    "\n",
    "Hacked together by / Copyright 2022, Ross Wightman\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass, replace, field\n",
    "from functools import partial\n",
    "from typing import Callable, Optional, Union, Tuple, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.jit import Final\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.layers import Mlp, ConvMlp, DropPath, LayerNorm, ClassifierHead, NormMlpClassifierHead\n",
    "from timm.layers import create_attn, get_act_layer, get_norm_layer, get_norm_act_layer, create_conv2d, create_pool2d\n",
    "from timm.layers import trunc_normal_tf_, to_2tuple, extend_tuple, make_divisible, _assert\n",
    "from timm.layers import RelPosMlp, RelPosBias, RelPosBiasTf, use_fused_attn, resize_rel_pos_bias_table\n",
    "# from ._builder import build_model_with_cfg\n",
    "# from ._features_fx import register_notrace_function\n",
    "# from ._manipulate import named_apply, checkpoint_seq\n",
    "# from ._registry import generate_default_cfgs, register_model\n",
    "\n",
    "__all__ = ['MaxxVitCfg', 'MaxxVitConvCfg', 'MaxxVitTransformerCfg', 'MaxxVit']\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MaxxVitTransformerCfg:\n",
    "    dim_head: int = 32\n",
    "    head_first: bool = True  # head ordering in qkv channel dim\n",
    "    expand_ratio: float = 4.0\n",
    "    expand_first: bool = True\n",
    "    shortcut_bias: bool = True\n",
    "    attn_bias: bool = True\n",
    "    attn_drop: float = 0.\n",
    "    proj_drop: float = 0.\n",
    "    pool_type: str = 'avg2'\n",
    "    rel_pos_type: str = 'bias'\n",
    "    rel_pos_dim: int = 512  # for relative position types w/ MLP\n",
    "    partition_ratio: int = 32\n",
    "    window_size: Optional[Tuple[int, int]] = None\n",
    "    grid_size: Optional[Tuple[int, int]] = None\n",
    "    no_block_attn: bool = False  # disable window block attention for maxvit (ie only grid)\n",
    "    use_nchw_attn: bool = False  # for MaxViT variants (not used for CoAt), keep tensors in NCHW order\n",
    "    init_values: Optional[float] = None\n",
    "    act_layer: str = 'gelu'\n",
    "    norm_layer: str = 'layernorm2d'\n",
    "    norm_layer_cl: str = 'layernorm'\n",
    "    norm_eps: float = 1e-6\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.grid_size is not None:\n",
    "            self.grid_size = to_2tuple(self.grid_size)\n",
    "        if self.window_size is not None:\n",
    "            self.window_size = to_2tuple(self.window_size)\n",
    "            if self.grid_size is None:\n",
    "                self.grid_size = self.window_size\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MaxxVitConvCfg:\n",
    "    block_type: str = 'mbconv'\n",
    "    expand_ratio: float = 4.0\n",
    "    expand_output: bool = True  # calculate expansion channels from output (vs input chs)\n",
    "    kernel_size: int = 3\n",
    "    group_size: int = 1  # 1 == depthwise\n",
    "    pre_norm_act: bool = False  # activation after pre-norm\n",
    "    output_bias: bool = True  # bias for shortcut + final 1x1 projection conv\n",
    "    stride_mode: str = 'dw'  # stride done via one of 'pool', '1x1', 'dw'\n",
    "    pool_type: str = 'avg2'\n",
    "    downsample_pool_type: str = 'avg2'\n",
    "    padding: str = ''\n",
    "    attn_early: bool = False  # apply attn between conv2 and norm2, instead of after norm2\n",
    "    attn_layer: str = 'se'\n",
    "    attn_act_layer: str = 'silu'\n",
    "    attn_ratio: float = 0.25\n",
    "    init_values: Optional[float] = 1e-6  # for ConvNeXt block, ignored by MBConv\n",
    "    act_layer: str = 'gelu'\n",
    "    norm_layer: str = ''\n",
    "    norm_layer_cl: str = ''\n",
    "    norm_eps: Optional[float] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # mbconv vs convnext blocks have different defaults, set in post_init to avoid explicit config args\n",
    "        assert self.block_type in ('mbconv', 'convnext')\n",
    "        use_mbconv = self.block_type == 'mbconv'\n",
    "        if not self.norm_layer:\n",
    "            self.norm_layer = 'batchnorm2d' if use_mbconv else 'layernorm2d'\n",
    "        if not self.norm_layer_cl and not use_mbconv:\n",
    "            self.norm_layer_cl = 'layernorm'\n",
    "        if self.norm_eps is None:\n",
    "            self.norm_eps = 1e-5 if use_mbconv else 1e-6\n",
    "        self.downsample_pool_type = self.downsample_pool_type or self.pool_type\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MaxxVitCfg:\n",
    "    embed_dim: Tuple[int, ...] = (96, 192, 384, 768)\n",
    "    depths: Tuple[int, ...] = (2, 3, 5, 2)\n",
    "    block_type: Tuple[Union[str, Tuple[str, ...]], ...] = ('C', 'C', 'T', 'T')\n",
    "    stem_width: Union[int, Tuple[int, int]] = 64\n",
    "    stem_bias: bool = False\n",
    "    conv_cfg: MaxxVitConvCfg = field(default_factory=MaxxVitConvCfg)\n",
    "    transformer_cfg: MaxxVitTransformerCfg = field(default_factory=MaxxVitTransformerCfg)\n",
    "    head_hidden_size: int = None\n",
    "    weight_init: str = 'vit_eff'\n",
    "\n",
    "\n",
    "class Attention2d(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    \"\"\" multi-head attention for 2D NCHW tensors\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            dim_out: Optional[int] = None,\n",
    "            dim_head: int = 32,\n",
    "            bias: bool = True,\n",
    "            expand_first: bool = True,\n",
    "            head_first: bool = True,\n",
    "            rel_pos_cls: Callable = None,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_out = dim_out or dim\n",
    "        dim_attn = dim_out if expand_first else dim\n",
    "        self.num_heads = dim_attn // dim_head\n",
    "        self.dim_head = dim_head\n",
    "        self.head_first = head_first\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias)\n",
    "        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n",
    "#         print(f\"Inside Attention2d forward: {x.shape}\")\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "#         y = x.clone()\n",
    "#         qkv_temp = self.qkv(y)\n",
    "\n",
    "#         print(f\"After self.qkv {qkv_temp.shape}\")\n",
    "        \n",
    "        if self.head_first:\n",
    "            q, k, v = self.qkv(x).view(B, self.num_heads, self.dim_head * 3, -1).chunk(3, dim=2)\n",
    "#             print(\"self.head_first=True\")\n",
    "            \n",
    "        else:\n",
    "            q, k, v = self.qkv(x).reshape(B, 3, self.num_heads, self.dim_head, -1).unbind(1)\n",
    "#             print(\"self.head_first=False\")\n",
    "            \n",
    "        \n",
    "#         print(f\"q, k, v: {q.shape}, {k.shape}, {v.shape}\")\n",
    "\n",
    "\n",
    "        if self.fused_attn:\n",
    "            # here\n",
    "#             print(\"self.fused_attn = True\")\n",
    "            attn_bias = None\n",
    "            if self.rel_pos is not None:\n",
    "                #here\n",
    "                attn_bias = self.rel_pos.get_bias()\n",
    "#                 print(\"self.rel_pos is not None\")\n",
    "                \n",
    "            elif shared_rel_pos is not None:\n",
    "                attn_bias = shared_rel_pos\n",
    "#                 print(\"shared_rel_pos is not None\")\n",
    "            \n",
    "            \n",
    "#             if attn_bias is not None:    \n",
    "#                 print(f\"attn_bias: {attn_bias.shape}\")\n",
    "#             else:\n",
    "#                 print(\"attn_bias is none\")\n",
    "            \n",
    "#             print(f\"Now applying x = torch.nn.functional.scaled_dot_product_attention(\\\n",
    "#                 q.transpose(-1, -2).contiguous(),\\\n",
    "#                 k.transpose(-1, -2).contiguous(),\\\n",
    "#                 v.transpose(-1, -2).contiguous(),\\\n",
    "#                 attn_mask=attn_bias,\\\n",
    "#                 dropout_p=self.attn_drop.p if self.training else 0.,\\\n",
    "#             ).transpose(-1, -2).reshape(B, -1, H, W)\")\n",
    "\n",
    "            x = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q.transpose(-1, -2).contiguous(),\n",
    "                k.transpose(-1, -2).contiguous(),\n",
    "                v.transpose(-1, -2).contiguous(),\n",
    "                attn_mask=attn_bias,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            ).transpose(-1, -2).reshape(B, -1, H, W)\n",
    "        else:\n",
    "#             print(\"self.fused_attn = False\")\n",
    "            q = q * self.scale\n",
    "            attn = q.transpose(-2, -1) @ k\n",
    "            if self.rel_pos is not None:\n",
    "                attn = self.rel_pos(attn)\n",
    "            elif shared_rel_pos is not None:\n",
    "                attn = attn + shared_rel_pos\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = (v @ attn.transpose(-2, -1)).view(B, -1, H, W)\n",
    "        \n",
    "        \n",
    "#         print(f\"After attention mechanism: {x.shape}\")\n",
    "        x = self.proj(x)\n",
    "#         print(f\"After self.proj: {x.shape}\")\n",
    "        x = self.proj_drop(x)\n",
    "#         print(f\"After self.proj_drop: {x.shape}\")\n",
    "#         print('*'*100)\n",
    "        return x\n",
    "\n",
    "\n",
    "### PROPOSED ATTENTION MECHANISM ###\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention2dPyramidal(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "        \n",
    "    def __init__(self, num_levels: int, \n",
    "            dim: int,\n",
    "            dim_out: Optional[int] = None,\n",
    "            dim_head: int = 32,\n",
    "            bias: bool = True,\n",
    "            expand_first: bool = True,\n",
    "            head_first: bool = True,\n",
    "            rel_pos_cls: Callable = None,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.attention_modules = nn.ModuleList([\n",
    "            Attention2d(dim, dim_out, dim_head, expand_first, bias, rel_pos_cls, attn_drop, proj_drop) \n",
    "            for _ in range(num_levels)\n",
    "        ])\n",
    "        self.attn_bias = bias\n",
    "        self.rel_pos_cls = rel_pos_cls\n",
    "        \n",
    "\n",
    "    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n",
    "#         print(f\"In Attention2dPyramidal forward num_levels={self.num_levels} and x={x.shape}:-\")\n",
    "    \n",
    "        # Divide input into pyramidal levels\n",
    "        pyramidal_levels = [x]\n",
    "        for i in range(1, self.num_levels):\n",
    "            downsample_factor = 2 * i\n",
    "            level_input = F.avg_pool2d(x, kernel_size=downsample_factor, stride=downsample_factor)\n",
    "#             print(f\"Shape after downsampling(avgpool2D) {i}th level={level_input.shape}\")\n",
    "            pyramidal_levels.append(level_input)\n",
    "\n",
    "        # Compute attention at each level\n",
    "        attention_outputs = []\n",
    "        for level, level_input in enumerate(pyramidal_levels):\n",
    "            attention_output = self.attention_modules[level](level_input)\n",
    "#             print(f\"Shape after applying attention at {level}th level={attention_output.shape}\")\n",
    "            attention_outputs.append(attention_output)\n",
    "\n",
    "        \n",
    "        # Upsample and combine attention outputs starting from the last level\n",
    "        combined_attention_output = attention_outputs[-1]\n",
    "        for i in range(self.num_levels - 2, -1, -1): \n",
    "            # Interpolate the attention output of the current level to match the spatial dimensions of the level above it\n",
    "            combined_attention_output = F.interpolate(combined_attention_output, size=attention_outputs[i].shape[2:], mode='bilinear', align_corners=False)\n",
    "#             print(f\"Shape after applying interpolation at {i+1}th level={combined_attention_output.shape}\")\n",
    "\n",
    "            # Combine the attention output of the current level with the level above it\n",
    "            combined_attention_output += attention_outputs[i]\n",
    "#             print(f\"Shape after combining {i+1} and {i}th level={combined_attention_output.shape}\")\n",
    "\n",
    "\n",
    "        # Resize the final combined attention output to match the spatial dimensions of the input at the first level\n",
    "        combined_attention_output = F.interpolate(combined_attention_output, size=pyramidal_levels[0].shape[2:], mode='bilinear', align_corners=False)\n",
    "#         print(f\"Final shape after (interpolation_last) Attention2dPyramidal : {combined_attention_output.shape}\")\n",
    "#         print(\"$\"*100)\n",
    "        return combined_attention_output\n",
    "\n",
    "    def match_spatial_dimensions(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        Pad or crop tensor1 to match the spatial dimensions of tensor2.\n",
    "        \"\"\"\n",
    "        if tensor1.shape[2:] != tensor2.shape[2:]:\n",
    "            diff_h = tensor2.shape[2] - tensor1.shape[2]\n",
    "            diff_w = tensor2.shape[3] - tensor1.shape[3]\n",
    "            pad_left = diff_w // 2\n",
    "            pad_right = diff_w - pad_left\n",
    "            pad_top = diff_h // 2\n",
    "            pad_bottom = diff_h - pad_top\n",
    "            tensor1 = F.pad(tensor1, (pad_left, pad_right, pad_top, pad_bottom))\n",
    "        return tensor1\n",
    "    \n",
    "\n",
    "class AttentionCl(nn.Module):\n",
    "    \"\"\" Channels-last multi-head attention (B, ..., C) \"\"\"\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            dim_out: Optional[int] = None,\n",
    "            dim_head: int = 32,\n",
    "            bias: bool = True,\n",
    "            expand_first: bool = True,\n",
    "            head_first: bool = True,\n",
    "            rel_pos_cls: Callable = None,\n",
    "            attn_drop: float = 0.,\n",
    "            proj_drop: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_out = dim_out or dim\n",
    "        dim_attn = dim_out if expand_first and dim_out > dim else dim\n",
    "        assert dim_attn % dim_head == 0, 'attn dim should be divisible by head_dim'\n",
    "        self.num_heads = dim_attn // dim_head\n",
    "        self.dim_head = dim_head\n",
    "        self.head_first = head_first\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias)\n",
    "        self.rel_pos = rel_pos_cls(num_heads=self.num_heads) if rel_pos_cls else None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim_attn, dim_out, bias=bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n",
    "        B = x.shape[0]\n",
    "        restore_shape = x.shape[:-1]\n",
    "\n",
    "        if self.head_first:\n",
    "            q, k, v = self.qkv(x).view(B, -1, self.num_heads, self.dim_head * 3).transpose(1, 2).chunk(3, dim=3)\n",
    "        else:\n",
    "            q, k, v = self.qkv(x).reshape(B, -1, 3, self.num_heads, self.dim_head).transpose(1, 3).unbind(2)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            attn_bias = None\n",
    "            if self.rel_pos is not None:\n",
    "                attn_bias = self.rel_pos.get_bias()\n",
    "            elif shared_rel_pos is not None:\n",
    "                attn_bias = shared_rel_pos\n",
    "\n",
    "            x = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                attn_mask=attn_bias,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            if self.rel_pos is not None:\n",
    "                attn = self.rel_pos(attn, shared_rel_pos=shared_rel_pos)\n",
    "            elif shared_rel_pos is not None:\n",
    "                attn = attn + shared_rel_pos\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(restore_shape + (-1,))\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gamma = self.gamma\n",
    "        return x.mul_(gamma) if self.inplace else x * gamma\n",
    "\n",
    "\n",
    "class LayerScale2d(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        gamma = self.gamma.view(1, -1, 1, 1)\n",
    "        return x.mul_(gamma) if self.inplace else x * gamma\n",
    "\n",
    "\n",
    "class Downsample2d(nn.Module):\n",
    "    \"\"\" A downsample pooling module supporting several maxpool and avgpool modes\n",
    "    * 'max' - MaxPool2d w/ kernel_size 3, stride 2, padding 1\n",
    "    * 'max2' - MaxPool2d w/ kernel_size = stride = 2\n",
    "    * 'avg' - AvgPool2d w/ kernel_size 3, stride 2, padding 1\n",
    "    * 'avg2' - AvgPool2d w/ kernel_size = stride = 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            dim_out: int,\n",
    "            pool_type: str = 'avg2',\n",
    "            padding: str = '',\n",
    "            bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert pool_type in ('max', 'max2', 'avg', 'avg2')\n",
    "        if pool_type == 'max':\n",
    "            self.pool = create_pool2d('max', kernel_size=3, stride=2, padding=padding or 1)\n",
    "        elif pool_type == 'max2':\n",
    "            self.pool = create_pool2d('max', 2, padding=padding or 0)  # kernel_size == stride == 2\n",
    "        elif pool_type == 'avg':\n",
    "            self.pool = create_pool2d(\n",
    "                'avg', kernel_size=3, stride=2, count_include_pad=False, padding=padding or 1)\n",
    "        else:\n",
    "            self.pool = create_pool2d('avg', 2, padding=padding or 0)\n",
    "\n",
    "        if dim != dim_out:\n",
    "            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias)\n",
    "        else:\n",
    "            self.expand = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"In Downsample2D forward:- {x.shape}\")\n",
    "        x = self.pool(x)  # spatial downsample\n",
    "#         print(f\"After self.pool: {x.shape}\")\n",
    "        x = self.expand(x)  # expand chs\n",
    "#         print(f\"After self.expand: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_transformer(module, name, scheme=''):\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        if scheme == 'normal':\n",
    "            nn.init.normal_(module.weight, std=.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif scheme == 'trunc_normal':\n",
    "            trunc_normal_tf_(module.weight, std=.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif scheme == 'xavier_normal':\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            # vit like\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                if 'mlp' in name:\n",
    "                    nn.init.normal_(module.bias, std=1e-6)\n",
    "                else:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "class TransformerBlock2d(nn.Module):\n",
    "    \"\"\" Transformer block with 2D downsampling\n",
    "    '2D' NCHW tensor layout\n",
    "\n",
    "    Some gains can be seen on GPU using a 1D / CL block, BUT w/ the need to switch back/forth to NCHW\n",
    "    for spatial pooling, the benefit is minimal so ended up using just this variant for CoAt configs.\n",
    "\n",
    "    This impl was faster on TPU w/ PT XLA than the 1D experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            dim_out: int,\n",
    "            stride: int = 1,\n",
    "            rel_pos_cls: Callable = None,\n",
    "            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)\n",
    "        act_layer = get_act_layer(cfg.act_layer)\n",
    "\n",
    "        if stride == 2:\n",
    "            self.shortcut = Downsample2d(dim, dim_out, pool_type=cfg.pool_type, bias=cfg.shortcut_bias)\n",
    "            self.norm1 = nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('down', Downsample2d(dim, dim, pool_type=cfg.pool_type)),\n",
    "            ]))\n",
    "        else:\n",
    "            assert dim == dim_out\n",
    "            self.shortcut = nn.Identity()\n",
    "            self.norm1 = norm_layer(dim)\n",
    "\n",
    "        ### MODIFIED HERE    \n",
    "        self.attn = Attention2dPyramidal(\n",
    "            4, # num_levels\n",
    "            dim,\n",
    "            dim_out,\n",
    "            dim_head=cfg.dim_head,\n",
    "            expand_first=cfg.expand_first,\n",
    "            bias=cfg.attn_bias,\n",
    "            rel_pos_cls=rel_pos_cls,\n",
    "            attn_drop=cfg.attn_drop,\n",
    "            proj_drop=cfg.proj_drop\n",
    "        )\n",
    "        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim_out)\n",
    "        self.mlp = ConvMlp(\n",
    "            in_features=dim_out,\n",
    "            hidden_features=int(dim_out * cfg.expand_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=cfg.proj_drop)\n",
    "        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def init_weights(self, scheme=''):\n",
    "        named_apply(partial(_init_transformer, scheme=scheme), self)\n",
    "\n",
    "    def forward(self, x, shared_rel_pos: Optional[torch.Tensor] = None):\n",
    "#         print(f\"Inside TransformerBlock2d forward: {x.shape}\")\n",
    "        \n",
    "#         y = x.clone()\n",
    "        \n",
    "#         y_shortcut = y.clone()\n",
    "        \n",
    "        \n",
    "#         y = self.norm1(y)\n",
    "#         print(f\"After self.norm1 {y.shape}\")\n",
    "        \n",
    "#         y = self.attn(y, shared_rel_pos=shared_rel_pos)\n",
    "#         print(f\"After self.attn(y, shared_rel_pos=shared_rel_pos): {y.shape} \")\n",
    "        \n",
    "#         y = self.ls1(y)\n",
    "#         print(f\"After self.ls1(y): {y.shape}\")\n",
    "        \n",
    "#         y = self.drop_path1(y)\n",
    "#         print(f\"After self.drop_path1(y): {y.shape}\")\n",
    "        \n",
    "#         y_shortcut = self.shortcut(y_shortcut)\n",
    "#         print(f\"After self.shortcut(y_shortcut): {y_shortcut.shape}\")\n",
    "        \n",
    "#         y = y_shortcut + y\n",
    "#         print(f\"After y_shortcut + y : {y.shape}\")\n",
    "        \n",
    "        \n",
    "        x = self.shortcut(x) + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))\n",
    "#         print(f\"after self.shortcut(x) + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos))) : {x.shape}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#         y = x.clone()\n",
    "        \n",
    "#         y_shortcut = x.clone()\n",
    "        \n",
    "#         y = self.norm2(y)\n",
    "#         print(f\"After self.norm2(y): {y.shape}\")\n",
    "        \n",
    "#         y = self.mlp(y)\n",
    "#         print(f\"After self.mlp: {y.shape}\")\n",
    "        \n",
    "#         y = self.ls2(y)\n",
    "#         print(f\"After self.ls2: {y.shape}\")\n",
    "\n",
    "#         y = self.drop_path2(y)\n",
    "#         print(f\"After self.drop_path2: {y.shape}\")\n",
    "\n",
    "        \n",
    "#         y = y_shortcut + y\n",
    "#         print(f\"After y_shortcut + y: {y.shape}\")\n",
    "        \n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "#         print(f\"After x + self.drop_path2(self.ls2(self.mlp(self.norm2(x)))): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_conv(module, name, scheme=''):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        if scheme == 'normal':\n",
    "            nn.init.normal_(module.weight, std=.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif scheme == 'trunc_normal':\n",
    "            trunc_normal_tf_(module.weight, std=.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif scheme == 'xavier_normal':\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        else:\n",
    "            # efficientnet like\n",
    "            fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n",
    "            fan_out //= module.groups\n",
    "            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def num_groups(group_size, channels):\n",
    "    if not group_size:  # 0 or None\n",
    "        return 1  # normal conv with 1 group\n",
    "    else:\n",
    "        # NOTE group_size == 1 -> depthwise conv\n",
    "        assert channels % group_size == 0\n",
    "        return channels // group_size\n",
    "\n",
    "\n",
    "class MbConvBlock(nn.Module):\n",
    "    \"\"\" Pre-Norm Conv Block - 1x1 - kxk - 1x1, w/ inverted bottleneck (expand)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs: int,\n",
    "            out_chs: int,\n",
    "            stride: int = 1,\n",
    "            dilation: Tuple[int, int] = (1, 1),\n",
    "            cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n",
    "            drop_path: float = 0.\n",
    "    ):\n",
    "        super(MbConvBlock, self).__init__()\n",
    "        norm_act_layer = partial(get_norm_act_layer(cfg.norm_layer, cfg.act_layer), eps=cfg.norm_eps)\n",
    "        mid_chs = make_divisible((out_chs if cfg.expand_output else in_chs) * cfg.expand_ratio)\n",
    "        groups = num_groups(cfg.group_size, mid_chs)\n",
    "\n",
    "        if stride == 2:\n",
    "            self.shortcut = Downsample2d(\n",
    "                in_chs, out_chs, pool_type=cfg.pool_type, bias=cfg.output_bias, padding=cfg.padding)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        assert cfg.stride_mode in ('pool', '1x1', 'dw')\n",
    "        stride_pool, stride_1, stride_2 = 1, 1, 1\n",
    "        if cfg.stride_mode == 'pool':\n",
    "            # NOTE this is not described in paper, experiment to find faster option that doesn't stride in 1x1\n",
    "            stride_pool, dilation_2 = stride, dilation[1]\n",
    "            # FIXME handle dilation of avg pool\n",
    "        elif cfg.stride_mode == '1x1':\n",
    "            # NOTE I don't like this option described in paper, 1x1 w/ stride throws info away\n",
    "            stride_1, dilation_2 = stride, dilation[1]\n",
    "        else:\n",
    "            stride_2, dilation_2 = stride, dilation[0]\n",
    "\n",
    "        self.pre_norm = norm_act_layer(in_chs, apply_act=cfg.pre_norm_act)\n",
    "        if stride_pool > 1:\n",
    "            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, padding=cfg.padding)\n",
    "        else:\n",
    "            self.down = nn.Identity()\n",
    "        self.conv1_1x1 = create_conv2d(in_chs, mid_chs, 1, stride=stride_1)\n",
    "        self.norm1 = norm_act_layer(mid_chs)\n",
    "\n",
    "        self.conv2_kxk = create_conv2d(\n",
    "            mid_chs, mid_chs, cfg.kernel_size,\n",
    "            stride=stride_2, dilation=dilation_2, groups=groups, padding=cfg.padding)\n",
    "\n",
    "        attn_kwargs = {}\n",
    "        if isinstance(cfg.attn_layer, str):\n",
    "            if cfg.attn_layer == 'se' or cfg.attn_layer == 'eca':\n",
    "                attn_kwargs['act_layer'] = cfg.attn_act_layer\n",
    "                attn_kwargs['rd_channels'] = int(cfg.attn_ratio * (out_chs if cfg.expand_output else mid_chs))\n",
    "\n",
    "        # two different orderings for SE and norm2 (due to some weights and trials using SE before norm2)\n",
    "        if cfg.attn_early:\n",
    "            self.se_early = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs)\n",
    "            self.norm2 = norm_act_layer(mid_chs)\n",
    "            self.se = None\n",
    "        else:\n",
    "            self.se_early = None\n",
    "            self.norm2 = norm_act_layer(mid_chs)\n",
    "            self.se = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs)\n",
    "\n",
    "        self.conv3_1x1 = create_conv2d(mid_chs, out_chs, 1, bias=cfg.output_bias)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def init_weights(self, scheme=''):\n",
    "        named_apply(partial(_init_conv, scheme=scheme), self)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print('-'*100)\n",
    "#         print(f\"Inside MbConvBlock forward:{x.shape}\")\n",
    "        shortcut = self.shortcut(x)\n",
    "#         print(f\"After self.shortcut:{x.shape}\")\n",
    "\n",
    "        x = self.pre_norm(x)\n",
    "#         print(f\"After self.pre_norm:{x.shape}\")\n",
    "        x = self.down(x)\n",
    "#         print(f\"After self.down:{x.shape}\")\n",
    "\n",
    "\n",
    "        # 1x1 expansion conv & norm-act\n",
    "        x = self.conv1_1x1(x)\n",
    "#         print(f\"After self.conv1_1x1 {x.shape}\")\n",
    "        x = self.norm1(x)\n",
    "#         print(f\"After self.norm1 {x.shape}\")\n",
    "\n",
    "\n",
    "        # depthwise / grouped 3x3 conv w/ SE (or other) channel attention & norm-act\n",
    "        x = self.conv2_kxk(x)\n",
    "#         print(f\"After self.conv2_kxk {x.shape}\")\n",
    "        if self.se_early is not None:\n",
    "            x = self.se_early(x)\n",
    "#             print(f\"self.se_early is not NONE. After self.se_early {x.shape}\")\n",
    "\n",
    "        x = self.norm2(x)\n",
    "#         print(f\"After self.norm2 {x.shape}\")\n",
    "\n",
    "        if self.se is not None:\n",
    "            # here\n",
    "            x = self.se(x)\n",
    "#             print(f\"self.se is not NONE. After self.se: {x.shape}\")\n",
    "\n",
    "        # 1x1 linear projection to output width\n",
    "        x = self.conv3_1x1(x)\n",
    "#         print(f\"After self.conv3_1x1: {x.shape}\")\n",
    "\n",
    "        x = self.drop_path(x) + shortcut\n",
    "#         print(f\"After self.drop_path(x) + shortcut: {x.shape}\")\n",
    "#         print('-'*100)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\" ConvNeXt Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs: int,\n",
    "            out_chs: Optional[int] = None,\n",
    "            kernel_size: int = 7,\n",
    "            stride: int = 1,\n",
    "            dilation: Tuple[int, int] = (1, 1),\n",
    "            cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n",
    "            conv_mlp: bool = True,\n",
    "            drop_path: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_chs = out_chs or in_chs\n",
    "        act_layer = get_act_layer(cfg.act_layer)\n",
    "        if conv_mlp:\n",
    "            norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)\n",
    "            mlp_layer = ConvMlp\n",
    "        else:\n",
    "            assert 'layernorm' in cfg.norm_layer\n",
    "            norm_layer = LayerNorm\n",
    "            mlp_layer = Mlp\n",
    "        self.use_conv_mlp = conv_mlp\n",
    "\n",
    "        if stride == 2:\n",
    "            self.shortcut = Downsample2d(in_chs, out_chs)\n",
    "        elif in_chs != out_chs:\n",
    "            self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        assert cfg.stride_mode in ('pool', 'dw')\n",
    "        stride_pool, stride_dw = 1, 1\n",
    "        # FIXME handle dilation?\n",
    "        if cfg.stride_mode == 'pool':\n",
    "            stride_pool = stride\n",
    "        else:\n",
    "            stride_dw = stride\n",
    "\n",
    "        if stride_pool == 2:\n",
    "            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type)\n",
    "        else:\n",
    "            self.down = nn.Identity()\n",
    "\n",
    "        self.conv_dw = create_conv2d(\n",
    "            in_chs, out_chs, kernel_size=kernel_size, stride=stride_dw, dilation=dilation[1],\n",
    "            depthwise=True, bias=cfg.output_bias)\n",
    "        self.norm = norm_layer(out_chs)\n",
    "        self.mlp = mlp_layer(out_chs, int(cfg.expand_ratio * out_chs), bias=cfg.output_bias, act_layer=act_layer)\n",
    "        if conv_mlp:\n",
    "            self.ls = LayerScale2d(out_chs, cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        else:\n",
    "            self.ls = LayerScale(out_chs, cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.down(x)\n",
    "        x = self.conv_dw(x)\n",
    "        if self.use_conv_mlp:\n",
    "            x = self.norm(x)\n",
    "            x = self.mlp(x)\n",
    "            x = self.ls(x)\n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "            x = self.norm(x)\n",
    "            x = self.mlp(x)\n",
    "            x = self.ls(x)\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = self.drop_path(x) + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size: List[int]):\n",
    "    B, H, W, C = x.shape\n",
    "    _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')\n",
    "    _assert(W % window_size[1] == 0, '')\n",
    "    x = x.view(B, H // window_size[0], window_size[0], W // window_size[1], window_size[1], C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0], window_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "@register_notrace_function  # reason: int argument is a Proxy\n",
    "def window_reverse(windows, window_size: List[int], img_size: List[int]):\n",
    "    H, W = img_size\n",
    "    C = windows.shape[-1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], window_size[0], window_size[1], C)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, H, W, C)\n",
    "    return x\n",
    "\n",
    "\n",
    "def grid_partition(x, grid_size: List[int]):\n",
    "    B, H, W, C = x.shape\n",
    "    _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')\n",
    "    _assert(W % grid_size[1] == 0, '')\n",
    "    x = x.view(B, grid_size[0], H // grid_size[0], grid_size[1], W // grid_size[1], C)\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, grid_size[0], grid_size[1], C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "@register_notrace_function  # reason: int argument is a Proxy\n",
    "def grid_reverse(windows, grid_size: List[int], img_size: List[int]):\n",
    "    H, W = img_size\n",
    "    C = windows.shape[-1]\n",
    "    x = windows.view(-1, H // grid_size[0], W // grid_size[1], grid_size[0], grid_size[1], C)\n",
    "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, H, W, C)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_rel_pos_cls(cfg: MaxxVitTransformerCfg, window_size):\n",
    "    rel_pos_cls = None\n",
    "    if cfg.rel_pos_type == 'mlp':\n",
    "        rel_pos_cls = partial(RelPosMlp, window_size=window_size, hidden_dim=cfg.rel_pos_dim)\n",
    "    elif cfg.rel_pos_type == 'bias':\n",
    "        rel_pos_cls = partial(RelPosBias, window_size=window_size)\n",
    "    elif cfg.rel_pos_type == 'bias_tf':\n",
    "        rel_pos_cls = partial(RelPosBiasTf, window_size=window_size)\n",
    "    return rel_pos_cls\n",
    "\n",
    "\n",
    "class PartitionAttentionCl(nn.Module):\n",
    "    \"\"\" Grid or Block partition + Attn + FFN.\n",
    "    NxC 'channels last' tensor layout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            partition_type: str = 'block',\n",
    "            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        norm_layer = partial(get_norm_layer(cfg.norm_layer_cl), eps=cfg.norm_eps)  # NOTE this block is channels-last\n",
    "        act_layer = get_act_layer(cfg.act_layer)\n",
    "\n",
    "        self.partition_block = partition_type == 'block'\n",
    "        self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)\n",
    "        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = AttentionCl(\n",
    "            dim,\n",
    "            dim,\n",
    "            dim_head=cfg.dim_head,\n",
    "            bias=cfg.attn_bias,\n",
    "            head_first=cfg.head_first,\n",
    "            rel_pos_cls=rel_pos_cls,\n",
    "            attn_drop=cfg.attn_drop,\n",
    "            proj_drop=cfg.proj_drop,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * cfg.expand_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=cfg.proj_drop)\n",
    "        self.ls2 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def _partition_attn(self, x):\n",
    "        img_size = x.shape[1:3]\n",
    "        if self.partition_block:\n",
    "            partitioned = window_partition(x, self.partition_size)\n",
    "        else:\n",
    "            partitioned = grid_partition(x, self.partition_size)\n",
    "\n",
    "        partitioned = self.attn(partitioned)\n",
    "\n",
    "        if self.partition_block:\n",
    "            x = window_reverse(partitioned, self.partition_size, img_size)\n",
    "        else:\n",
    "            x = grid_reverse(partitioned, self.partition_size, img_size)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelPartitionAttention(nn.Module):\n",
    "    \"\"\" Experimental. Grid and Block partition + single FFN\n",
    "    NxC tensor layout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        norm_layer = partial(get_norm_layer(cfg.norm_layer_cl), eps=cfg.norm_eps)  # NOTE this block is channels-last\n",
    "        act_layer = get_act_layer(cfg.act_layer)\n",
    "\n",
    "        assert cfg.window_size == cfg.grid_size\n",
    "        self.partition_size = to_2tuple(cfg.window_size)\n",
    "        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn_block = AttentionCl(\n",
    "            dim,\n",
    "            dim // 2,\n",
    "            dim_head=cfg.dim_head,\n",
    "            bias=cfg.attn_bias,\n",
    "            head_first=cfg.head_first,\n",
    "            rel_pos_cls=rel_pos_cls,\n",
    "            attn_drop=cfg.attn_drop,\n",
    "            proj_drop=cfg.proj_drop,\n",
    "        )\n",
    "        self.attn_grid = AttentionCl(\n",
    "            dim,\n",
    "            dim // 2,\n",
    "            dim_head=cfg.dim_head,\n",
    "            bias=cfg.attn_bias,\n",
    "            head_first=cfg.head_first,\n",
    "            rel_pos_cls=rel_pos_cls,\n",
    "            attn_drop=cfg.attn_drop,\n",
    "            proj_drop=cfg.proj_drop,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * cfg.expand_ratio),\n",
    "            out_features=dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=cfg.proj_drop)\n",
    "        self.ls2 = LayerScale(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def _partition_attn(self, x):\n",
    "        img_size = x.shape[1:3]\n",
    "\n",
    "        partitioned_block = window_partition(x, self.partition_size)\n",
    "        partitioned_block = self.attn_block(partitioned_block)\n",
    "        x_window = window_reverse(partitioned_block, self.partition_size, img_size)\n",
    "\n",
    "        partitioned_grid = grid_partition(x, self.partition_size)\n",
    "        partitioned_grid = self.attn_grid(partitioned_grid)\n",
    "        x_grid = grid_reverse(partitioned_grid, self.partition_size, img_size)\n",
    "\n",
    "        return torch.cat([x_window, x_grid], dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition_nchw(x, window_size: List[int]):\n",
    "    B, C, H, W = x.shape\n",
    "    _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')\n",
    "    _assert(W % window_size[1] == 0, '')\n",
    "    x = x.view(B, C, H // window_size[0], window_size[0], W // window_size[1], window_size[1])\n",
    "    windows = x.permute(0, 2, 4, 1, 3, 5).contiguous().view(-1, C, window_size[0], window_size[1])\n",
    "    return windows\n",
    "\n",
    "\n",
    "@register_notrace_function  # reason: int argument is a Proxy\n",
    "def window_reverse_nchw(windows, window_size: List[int], img_size: List[int]):\n",
    "    H, W = img_size\n",
    "    C = windows.shape[1]\n",
    "    x = windows.view(-1, H // window_size[0], W // window_size[1], C, window_size[0], window_size[1])\n",
    "    x = x.permute(0, 3, 1, 4, 2, 5).contiguous().view(-1, C, H, W)\n",
    "    return x\n",
    "\n",
    "\n",
    "def grid_partition_nchw(x, grid_size: List[int]):\n",
    "    B, C, H, W = x.shape\n",
    "    _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')\n",
    "    _assert(W % grid_size[1] == 0, '')\n",
    "    x = x.view(B, C, grid_size[0], H // grid_size[0], grid_size[1], W // grid_size[1])\n",
    "    windows = x.permute(0, 3, 5, 1, 2, 4).contiguous().view(-1, C, grid_size[0], grid_size[1])\n",
    "    return windows\n",
    "\n",
    "\n",
    "@register_notrace_function  # reason: int argument is a Proxy\n",
    "def grid_reverse_nchw(windows, grid_size: List[int], img_size: List[int]):\n",
    "    H, W = img_size\n",
    "    C = windows.shape[1]\n",
    "    x = windows.view(-1, H // grid_size[0], W // grid_size[1], C, grid_size[0], grid_size[1])\n",
    "    x = x.permute(0, 3, 4, 1, 5, 2).contiguous().view(-1, C, H, W)\n",
    "    return x\n",
    "\n",
    "\n",
    "class PartitionAttention2d(nn.Module):\n",
    "    \"\"\" Grid or Block partition + Attn + FFN\n",
    "\n",
    "    '2D' NCHW tensor layout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            partition_type: str = 'block',\n",
    "            cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        norm_layer = partial(get_norm_layer(cfg.norm_layer), eps=cfg.norm_eps)  # NOTE this block is channels-last\n",
    "        act_layer = get_act_layer(cfg.act_layer)\n",
    "\n",
    "        self.partition_block = partition_type == 'block'\n",
    "        self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)\n",
    "        rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention2d(\n",
    "            dim,\n",
    "            dim,\n",
    "            dim_head=cfg.dim_head,\n",
    "            bias=cfg.attn_bias,\n",
    "            head_first=cfg.head_first,\n",
    "            rel_pos_cls=rel_pos_cls,\n",
    "            attn_drop=cfg.attn_drop,\n",
    "            proj_drop=cfg.proj_drop,\n",
    "        )\n",
    "        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = ConvMlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * cfg.expand_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=cfg.proj_drop)\n",
    "        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values) if cfg.init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def _partition_attn(self, x):\n",
    "        img_size = x.shape[-2:]\n",
    "        if self.partition_block:\n",
    "            partitioned = window_partition_nchw(x, self.partition_size)\n",
    "        else:\n",
    "            partitioned = grid_partition_nchw(x, self.partition_size)\n",
    "\n",
    "        partitioned = self.attn(partitioned)\n",
    "\n",
    "        if self.partition_block:\n",
    "            x = window_reverse_nchw(partitioned, self.partition_size, img_size)\n",
    "        else:\n",
    "            x = grid_reverse_nchw(partitioned, self.partition_size, img_size)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaxxVitBlock(nn.Module):\n",
    "    \"\"\" MaxVit conv, window partition + FFN , grid partition + FFN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            dim_out: int,\n",
    "            stride: int = 1,\n",
    "            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n",
    "            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.nchw_attn = transformer_cfg.use_nchw_attn\n",
    "\n",
    "        conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n",
    "        self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)\n",
    "\n",
    "        attn_kwargs = dict(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path)\n",
    "        partition_layer = PartitionAttention2d if self.nchw_attn else PartitionAttentionCl\n",
    "        self.attn_block = None if transformer_cfg.no_block_attn else partition_layer(**attn_kwargs)\n",
    "        self.attn_grid = partition_layer(partition_type='grid', **attn_kwargs)\n",
    "\n",
    "    def init_weights(self, scheme=''):\n",
    "        if self.attn_block is not None:\n",
    "            named_apply(partial(_init_transformer, scheme=scheme), self.attn_block)\n",
    "        named_apply(partial(_init_transformer, scheme=scheme), self.attn_grid)\n",
    "        named_apply(partial(_init_conv, scheme=scheme), self.conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # NCHW format\n",
    "        x = self.conv(x)\n",
    "\n",
    "        if not self.nchw_attn:\n",
    "            x = x.permute(0, 2, 3, 1)  # to NHWC (channels-last)\n",
    "        if self.attn_block is not None:\n",
    "            x = self.attn_block(x)\n",
    "        x = self.attn_grid(x)\n",
    "        if not self.nchw_attn:\n",
    "            x = x.permute(0, 3, 1, 2)  # back to NCHW\n",
    "        return x\n",
    "\n",
    "\n",
    "class ParallelMaxxVitBlock(nn.Module):\n",
    "    \"\"\" MaxVit block with parallel cat(window + grid), one FF\n",
    "    Experimental timm block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            dim_out,\n",
    "            stride=1,\n",
    "            num_conv=2,\n",
    "            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n",
    "            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            drop_path=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n",
    "        if num_conv > 1:\n",
    "            convs = [conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)]\n",
    "            convs += [conv_cls(dim_out, dim_out, cfg=conv_cfg, drop_path=drop_path)] * (num_conv - 1)\n",
    "            self.conv = nn.Sequential(*convs)\n",
    "        else:\n",
    "            self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path)\n",
    "        self.attn = ParallelPartitionAttention(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path)\n",
    "\n",
    "    def init_weights(self, scheme=''):\n",
    "        named_apply(partial(_init_transformer, scheme=scheme), self.attn)\n",
    "        named_apply(partial(_init_conv, scheme=scheme), self.conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.attn(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaxxVitStage(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs: int,\n",
    "            out_chs: int,\n",
    "            stride: int = 2,\n",
    "            depth: int = 4,\n",
    "            feat_size: Tuple[int, int] = (14, 14),\n",
    "            block_types: Union[str, Tuple[str]] = 'C',\n",
    "            transformer_cfg: MaxxVitTransformerCfg = MaxxVitTransformerCfg(),\n",
    "            conv_cfg: MaxxVitConvCfg = MaxxVitConvCfg(),\n",
    "            drop_path: Union[float, List[float]] = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        block_types = extend_tuple(block_types, depth)\n",
    "        blocks = []\n",
    "        for i, t in enumerate(block_types):\n",
    "            block_stride = stride if i == 0 else 1\n",
    "            assert t in ('C', 'T', 'M', 'PM')\n",
    "            if t == 'C':\n",
    "                conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock\n",
    "                blocks += [conv_cls(\n",
    "                    in_chs,\n",
    "                    out_chs,\n",
    "                    stride=block_stride,\n",
    "                    cfg=conv_cfg,\n",
    "                    drop_path=drop_path[i],\n",
    "                )]\n",
    "            elif t == 'T':\n",
    "                rel_pos_cls = get_rel_pos_cls(transformer_cfg, feat_size)\n",
    "                blocks += [TransformerBlock2d(\n",
    "                    in_chs,\n",
    "                    out_chs,\n",
    "                    stride=block_stride,\n",
    "                    rel_pos_cls=rel_pos_cls,\n",
    "                    cfg=transformer_cfg,\n",
    "                    drop_path=drop_path[i],\n",
    "                )]\n",
    "            elif t == 'M':\n",
    "                blocks += [MaxxVitBlock(\n",
    "                    in_chs,\n",
    "                    out_chs,\n",
    "                    stride=block_stride,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    transformer_cfg=transformer_cfg,\n",
    "                    drop_path=drop_path[i],\n",
    "                )]\n",
    "            elif t == 'PM':\n",
    "                blocks += [ParallelMaxxVitBlock(\n",
    "                    in_chs,\n",
    "                    out_chs,\n",
    "                    stride=block_stride,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    transformer_cfg=transformer_cfg,\n",
    "                    drop_path=drop_path[i],\n",
    "                )]\n",
    "            in_chs = out_chs\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"Inside MaxxVitStage x.shape: {x.shape}\")\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "#         print(\"After self.blocks\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Stem(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_chs: int,\n",
    "            out_chs: int,\n",
    "            kernel_size: int = 3,\n",
    "            padding: str = '',\n",
    "            bias: bool = False,\n",
    "            act_layer: str = 'gelu',\n",
    "            norm_layer: str = 'batchnorm2d',\n",
    "            norm_eps: float = 1e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not isinstance(out_chs, (list, tuple)):\n",
    "            out_chs = to_2tuple(out_chs)\n",
    "\n",
    "        norm_act_layer = partial(get_norm_act_layer(norm_layer, act_layer), eps=norm_eps)\n",
    "        self.in_chs = in_chs # added this line\n",
    "        self.out_chs = out_chs[-1]\n",
    "        self.stride = 2\n",
    "\n",
    "        self.conv1 = create_conv2d(in_chs, out_chs[0], kernel_size, stride=2, padding=padding, bias=bias)\n",
    "        self.norm1 = norm_act_layer(out_chs[0])\n",
    "        self.conv2 = create_conv2d(out_chs[0], out_chs[1], kernel_size, stride=1, padding=padding, bias=bias)\n",
    "\n",
    "    def init_weights(self, scheme=''):\n",
    "        named_apply(partial(_init_conv, scheme=scheme), self)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"Inside Stem class\", x.shape)\n",
    "#         print(f\"in_chs:{self.in_chs} and out_chs:{self.out_chs}\")\n",
    "        x = self.conv1(x)\n",
    "#         print(\"after self.conv1\", x.shape)\n",
    "        x = self.norm1(x)\n",
    "#         print(\"after self.norm1\", x.shape)\n",
    "        x = self.conv2(x)\n",
    "#         print(\"after self.conv2\", x.shape)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def cfg_window_size(cfg: MaxxVitTransformerCfg, img_size: Tuple[int, int]):\n",
    "    if cfg.window_size is not None:\n",
    "        assert cfg.grid_size\n",
    "        return cfg\n",
    "    partition_size = img_size[0] // cfg.partition_ratio, img_size[1] // cfg.partition_ratio\n",
    "    cfg = replace(cfg, window_size=partition_size, grid_size=partition_size)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def _overlay_kwargs(cfg: MaxxVitCfg, **kwargs):\n",
    "    transformer_kwargs = {}\n",
    "    conv_kwargs = {}\n",
    "    base_kwargs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if k.startswith('transformer_'):\n",
    "            transformer_kwargs[k.replace('transformer_', '')] = v\n",
    "        elif k.startswith('conv_'):\n",
    "            conv_kwargs[k.replace('conv_', '')] = v\n",
    "        else:\n",
    "            base_kwargs[k] = v\n",
    "    cfg = replace(\n",
    "        cfg,\n",
    "        transformer_cfg=replace(cfg.transformer_cfg, **transformer_kwargs),\n",
    "        conv_cfg=replace(cfg.conv_cfg, **conv_kwargs),\n",
    "        **base_kwargs\n",
    "    )\n",
    "    return cfg\n",
    "\n",
    "\n",
    "class MaxxVit(nn.Module):\n",
    "    \"\"\" CoaTNet + MaxVit base model.\n",
    "\n",
    "    Highly configurable for different block compositions, tensor layouts, pooling types.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cfg: MaxxVitCfg,\n",
    "            img_size: Union[int, Tuple[int, int]] = 224,\n",
    "            in_chans: int = 3,\n",
    "            num_classes: int = 1000,\n",
    "            global_pool: str = 'avg',\n",
    "            drop_rate: float = 0.,\n",
    "            drop_path_rate: float = 0.,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        if kwargs:\n",
    "            cfg = _overlay_kwargs(cfg, **kwargs)\n",
    "        transformer_cfg = cfg_window_size(cfg.transformer_cfg, img_size)\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "        self.num_features = self.embed_dim = cfg.embed_dim[-1]\n",
    "        self.drop_rate = drop_rate\n",
    "        self.grad_checkpointing = False\n",
    "        self.feature_info = []\n",
    "\n",
    "        self.stem = Stem(\n",
    "            in_chs=in_chans,\n",
    "            out_chs=cfg.stem_width,\n",
    "            padding=cfg.conv_cfg.padding,\n",
    "            bias=cfg.stem_bias,\n",
    "            act_layer=cfg.conv_cfg.act_layer,\n",
    "            norm_layer=cfg.conv_cfg.norm_layer,\n",
    "            norm_eps=cfg.conv_cfg.norm_eps,\n",
    "        )\n",
    "        stride = self.stem.stride\n",
    "        self.feature_info += [dict(num_chs=self.stem.out_chs, reduction=2, module='stem')]\n",
    "        feat_size = tuple([i // s for i, s in zip(img_size, to_2tuple(stride))])\n",
    "\n",
    "        num_stages = len(cfg.embed_dim)\n",
    "        assert len(cfg.depths) == num_stages\n",
    "        dpr = [x.tolist() for x in torch.linspace(0, drop_path_rate, sum(cfg.depths)).split(cfg.depths)]\n",
    "        in_chs = self.stem.out_chs\n",
    "        stages = []\n",
    "        for i in range(num_stages):\n",
    "            stage_stride = 2\n",
    "            out_chs = cfg.embed_dim[i]\n",
    "            feat_size = tuple([(r - 1) // stage_stride + 1 for r in feat_size])\n",
    "            stages += [MaxxVitStage(\n",
    "                in_chs,\n",
    "                out_chs,\n",
    "                depth=cfg.depths[i],\n",
    "                block_types=cfg.block_type[i],\n",
    "                conv_cfg=cfg.conv_cfg,\n",
    "                transformer_cfg=transformer_cfg,\n",
    "                feat_size=feat_size,\n",
    "                drop_path=dpr[i],\n",
    "            )]\n",
    "            stride *= stage_stride\n",
    "            in_chs = out_chs\n",
    "            self.feature_info += [dict(num_chs=out_chs, reduction=stride, module=f'stages.{i}')]\n",
    "        self.stages = nn.Sequential(*stages)\n",
    "\n",
    "        final_norm_layer = partial(get_norm_layer(cfg.transformer_cfg.norm_layer), eps=cfg.transformer_cfg.norm_eps)\n",
    "        self.head_hidden_size = cfg.head_hidden_size\n",
    "        if self.head_hidden_size:\n",
    "            self.norm = nn.Identity()\n",
    "            self.head = NormMlpClassifierHead(\n",
    "                self.num_features,\n",
    "                num_classes,\n",
    "                hidden_size=self.head_hidden_size,\n",
    "                pool_type=global_pool,\n",
    "                drop_rate=drop_rate,\n",
    "                norm_layer=final_norm_layer,\n",
    "            )\n",
    "        else:\n",
    "            # standard classifier head w/ norm, pooling, fc classifier\n",
    "            self.norm = final_norm_layer(self.num_features)\n",
    "            self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate)\n",
    "\n",
    "        # Weight init (default PyTorch init works well for AdamW if scheme not set)\n",
    "        assert cfg.weight_init in ('', 'normal', 'trunc_normal', 'xavier_normal', 'vit_eff')\n",
    "        if cfg.weight_init:\n",
    "            named_apply(partial(self._init_weights, scheme=cfg.weight_init), self)\n",
    "\n",
    "    def _init_weights(self, module, name, scheme=''):\n",
    "        if hasattr(module, 'init_weights'):\n",
    "            try:\n",
    "                module.init_weights(scheme=scheme)\n",
    "            except TypeError:\n",
    "                module.init_weights()\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\n",
    "            k for k, _ in self.named_parameters()\n",
    "            if any(n in k for n in [\"relative_position_bias_table\", \"rel_pos.mlp\"])}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def group_matcher(self, coarse=False):\n",
    "        matcher = dict(\n",
    "            stem=r'^stem',  # stem and embed\n",
    "            blocks=[(r'^stages\\.(\\d+)', None), (r'^norm', (99999,))]\n",
    "        )\n",
    "        return matcher\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def set_grad_checkpointing(self, enable=True):\n",
    "        for s in self.stages:\n",
    "            s.grad_checkpointing = enable\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def get_classifier(self):\n",
    "        return self.head.fc\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.head.reset(num_classes, global_pool)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "#         print(f\"In forward_features: {x.shape}\")\n",
    "        x = self.stem(x)\n",
    "#         print(f\"After self.stem: {x.shape}\")\n",
    "        x = self.stages(x)\n",
    "#         print(f\"After self.stages: {x.shape}\")\n",
    "        x = self.norm(x)\n",
    "#         print(f\"After self.norm: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "#         print(f\"In forward_head function: {x.shape}\")\n",
    "#         print(f\"pre_logits is {pre_logits}\")\n",
    "        return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(f\"In forward (main): {x.shape}\")\n",
    "        x = self.forward_features(x)\n",
    "#         print(f\"After forward_features (main): {x.shape}\")\n",
    "        x = self.forward_head(x)\n",
    "#         print(f\"After forward_head (main): {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "def _rw_coat_cfg(\n",
    "        stride_mode='pool',\n",
    "        pool_type='avg2',\n",
    "        conv_output_bias=False,\n",
    "        conv_attn_early=False,\n",
    "        conv_attn_act_layer='relu',\n",
    "        conv_norm_layer='',\n",
    "        transformer_shortcut_bias=True,\n",
    "        transformer_norm_layer='layernorm2d',\n",
    "        transformer_norm_layer_cl='layernorm',\n",
    "        init_values=None,\n",
    "        rel_pos_type='bias',\n",
    "        rel_pos_dim=512,\n",
    "):\n",
    "    # 'RW' timm variant models were created and trained before seeing https://github.com/google-research/maxvit\n",
    "    # Common differences for initial timm models:\n",
    "    # - pre-norm layer in MZBConv included an activation after norm\n",
    "    # - mbconv expansion calculated from input instead of output chs\n",
    "    # - mbconv shortcut and final 1x1 conv did not have a bias\n",
    "    # - SE act layer was relu, not silu\n",
    "    # - mbconv uses silu in timm, not gelu\n",
    "    # - expansion in attention block done via output proj, not input proj\n",
    "    # Variable differences (evolved over training initial models):\n",
    "    # - avg pool with kernel_size=2 favoured downsampling (instead of maxpool for coat)\n",
    "    # - SE attention was between conv2 and norm/act\n",
    "    # - default to avg pool for mbconv downsample instead of 1x1 or dw conv\n",
    "    # - transformer block shortcut has no bias\n",
    "    return dict(\n",
    "        conv_cfg=MaxxVitConvCfg(\n",
    "            stride_mode=stride_mode,\n",
    "            pool_type=pool_type,\n",
    "            pre_norm_act=True,\n",
    "            expand_output=False,\n",
    "            output_bias=conv_output_bias,\n",
    "            attn_early=conv_attn_early,\n",
    "            attn_act_layer=conv_attn_act_layer,\n",
    "            act_layer='silu',\n",
    "            norm_layer=conv_norm_layer,\n",
    "        ),\n",
    "        transformer_cfg=MaxxVitTransformerCfg(\n",
    "            expand_first=False,\n",
    "            shortcut_bias=transformer_shortcut_bias,\n",
    "            pool_type=pool_type,\n",
    "            init_values=init_values,\n",
    "            norm_layer=transformer_norm_layer,\n",
    "            norm_layer_cl=transformer_norm_layer_cl,\n",
    "            rel_pos_type=rel_pos_type,\n",
    "            rel_pos_dim=rel_pos_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def _rw_max_cfg(\n",
    "        stride_mode='dw',\n",
    "        pool_type='avg2',\n",
    "        conv_output_bias=False,\n",
    "        conv_attn_ratio=1 / 16,\n",
    "        conv_norm_layer='',\n",
    "        transformer_norm_layer='layernorm2d',\n",
    "        transformer_norm_layer_cl='layernorm',\n",
    "        window_size=None,\n",
    "        dim_head=32,\n",
    "        init_values=None,\n",
    "        rel_pos_type='bias',\n",
    "        rel_pos_dim=512,\n",
    "):\n",
    "    # 'RW' timm variant models were created and trained before seeing https://github.com/google-research/maxvit\n",
    "    # Differences of initial timm models:\n",
    "    # - mbconv expansion calculated from input instead of output chs\n",
    "    # - mbconv shortcut and final 1x1 conv did not have a bias\n",
    "    # - mbconv uses silu in timm, not gelu\n",
    "    # - expansion in attention block done via output proj, not input proj\n",
    "    return dict(\n",
    "        conv_cfg=MaxxVitConvCfg(\n",
    "            stride_mode=stride_mode,\n",
    "            pool_type=pool_type,\n",
    "            expand_output=False,\n",
    "            output_bias=conv_output_bias,\n",
    "            attn_ratio=conv_attn_ratio,\n",
    "            act_layer='silu',\n",
    "            norm_layer=conv_norm_layer,\n",
    "        ),\n",
    "        transformer_cfg=MaxxVitTransformerCfg(\n",
    "            expand_first=False,\n",
    "            pool_type=pool_type,\n",
    "            dim_head=dim_head,\n",
    "            window_size=window_size,\n",
    "            init_values=init_values,\n",
    "            norm_layer=transformer_norm_layer,\n",
    "            norm_layer_cl=transformer_norm_layer_cl,\n",
    "            rel_pos_type=rel_pos_type,\n",
    "            rel_pos_dim=rel_pos_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def _next_cfg(\n",
    "        stride_mode='dw',\n",
    "        pool_type='avg2',\n",
    "        conv_norm_layer='layernorm2d',\n",
    "        conv_norm_layer_cl='layernorm',\n",
    "        transformer_norm_layer='layernorm2d',\n",
    "        transformer_norm_layer_cl='layernorm',\n",
    "        window_size=None,\n",
    "        no_block_attn=False,\n",
    "        init_values=1e-6,\n",
    "        rel_pos_type='mlp',  # MLP by default for maxxvit\n",
    "        rel_pos_dim=512,\n",
    "):\n",
    "    # For experimental models with convnext instead of mbconv\n",
    "    init_values = to_2tuple(init_values)\n",
    "    return dict(\n",
    "        conv_cfg=MaxxVitConvCfg(\n",
    "            block_type='convnext',\n",
    "            stride_mode=stride_mode,\n",
    "            pool_type=pool_type,\n",
    "            expand_output=False,\n",
    "            init_values=init_values[0],\n",
    "            norm_layer=conv_norm_layer,\n",
    "            norm_layer_cl=conv_norm_layer_cl,\n",
    "        ),\n",
    "        transformer_cfg=MaxxVitTransformerCfg(\n",
    "            expand_first=False,\n",
    "            pool_type=pool_type,\n",
    "            window_size=window_size,\n",
    "            no_block_attn=no_block_attn,  # enabled for MaxxViT-V2\n",
    "            init_values=init_values[1],\n",
    "            norm_layer=transformer_norm_layer,\n",
    "            norm_layer_cl=transformer_norm_layer_cl,\n",
    "            rel_pos_type=rel_pos_type,\n",
    "            rel_pos_dim=rel_pos_dim,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def _tf_cfg():\n",
    "    return dict(\n",
    "        conv_cfg=MaxxVitConvCfg(\n",
    "            norm_eps=1e-3,\n",
    "            act_layer='gelu_tanh',\n",
    "            padding='same',\n",
    "        ),\n",
    "        transformer_cfg=MaxxVitTransformerCfg(\n",
    "            norm_eps=1e-5,\n",
    "            act_layer='gelu_tanh',\n",
    "            head_first=False,  # heads are interleaved (q_nh, q_hdim, k_nh, q_hdim, ....)\n",
    "            rel_pos_type='bias_tf',\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "model_cfgs = dict(\n",
    "    # timm specific CoAtNet configs\n",
    "    coatnet_pico_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 3, 5, 2),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(  # using newer max defaults here\n",
    "            conv_output_bias=True,\n",
    "            conv_attn_ratio=0.25,\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(3, 4, 6, 3),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(  # using newer max defaults here\n",
    "            stride_mode='pool',\n",
    "            conv_output_bias=True,\n",
    "            conv_attn_ratio=0.25,\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_0_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            conv_attn_early=True,\n",
    "            transformer_shortcut_bias=False,\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_1_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_early=True,\n",
    "            transformer_shortcut_bias=False,\n",
    "        )\n",
    "    ),\n",
    "    coatnet_2_rw=MaxxVitCfg(\n",
    "        embed_dim=(128, 256, 512, 1024),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(64, 128),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_act_layer='silu',\n",
    "            #init_values=1e-6,\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_3_rw=MaxxVitCfg(\n",
    "        embed_dim=(192, 384, 768, 1536),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(96, 192),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_act_layer='silu',\n",
    "            init_values=1e-6,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)\n",
    "    coatnet_bn_0_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_early=True,\n",
    "            transformer_shortcut_bias=False,\n",
    "            transformer_norm_layer='batchnorm2d',\n",
    "        )\n",
    "    ),\n",
    "    coatnet_rmlp_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(3, 4, 6, 3),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(\n",
    "            conv_output_bias=True,\n",
    "            conv_attn_ratio=0.25,\n",
    "            rel_pos_type='mlp',\n",
    "            rel_pos_dim=384,\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_rmlp_0_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 3, 7, 2),  # deeper than paper '0' model\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            rel_pos_type='mlp',\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_rmlp_1_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            pool_type='max',\n",
    "            conv_attn_early=True,\n",
    "            transformer_shortcut_bias=False,\n",
    "            rel_pos_type='mlp',\n",
    "            rel_pos_dim=384,  # was supposed to be 512, woops\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_rmlp_1_rw2=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            rel_pos_type='mlp',\n",
    "            rel_pos_dim=512,  # was supposed to be 512, woops\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_rmlp_2_rw=MaxxVitCfg(\n",
    "        embed_dim=(128, 256, 512, 1024),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(64, 128),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_act_layer='silu',\n",
    "            init_values=1e-6,\n",
    "            rel_pos_type='mlp'\n",
    "        ),\n",
    "    ),\n",
    "    coatnet_rmlp_3_rw=MaxxVitCfg(\n",
    "        embed_dim=(192, 384, 768, 1536),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=(96, 192),\n",
    "        **_rw_coat_cfg(\n",
    "            stride_mode='dw',\n",
    "            conv_attn_act_layer='silu',\n",
    "            init_values=1e-6,\n",
    "            rel_pos_type='mlp'\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    coatnet_nano_cc=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(3, 4, 6, 3),\n",
    "        stem_width=(32, 64),\n",
    "        block_type=('C', 'C', ('C', 'T'), ('C', 'T')),\n",
    "        **_rw_coat_cfg(),\n",
    "    ),\n",
    "    coatnext_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(3, 4, 6, 3),\n",
    "        stem_width=(32, 64),\n",
    "        weight_init='normal',\n",
    "        **_next_cfg(\n",
    "            rel_pos_type='bias',\n",
    "            init_values=(1e-5, None)\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # Trying to be like the CoAtNet paper configs\n",
    "    coatnet_0=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 3, 5, 2),\n",
    "        stem_width=64,\n",
    "        head_hidden_size=768,\n",
    "    ),\n",
    "    coatnet_1=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=64,\n",
    "        head_hidden_size=768,\n",
    "    ),\n",
    "    coatnet_2=MaxxVitCfg(\n",
    "        embed_dim=(128, 256, 512, 1024),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=128,\n",
    "        head_hidden_size=1024,\n",
    "    ),\n",
    "    coatnet_3=MaxxVitCfg(\n",
    "        embed_dim=(192, 384, 768, 1536),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        stem_width=192,\n",
    "        head_hidden_size=1536,\n",
    "    ),\n",
    "    coatnet_4=MaxxVitCfg(\n",
    "        embed_dim=(192, 384, 768, 1536),\n",
    "        depths=(2, 12, 28, 2),\n",
    "        stem_width=192,\n",
    "        head_hidden_size=1536,\n",
    "    ),\n",
    "    coatnet_5=MaxxVitCfg(\n",
    "        embed_dim=(256, 512, 1280, 2048),\n",
    "        depths=(2, 12, 28, 2),\n",
    "        stem_width=192,\n",
    "        head_hidden_size=2048,\n",
    "    ),\n",
    "\n",
    "    # Experimental MaxVit configs\n",
    "    maxvit_pico_rw=MaxxVitCfg(\n",
    "        embed_dim=(32, 64, 128, 256),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(24, 32),\n",
    "        **_rw_max_cfg(),\n",
    "    ),\n",
    "    maxvit_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(1, 2, 3, 1),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(),\n",
    "    ),\n",
    "    maxvit_tiny_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(),\n",
    "    ),\n",
    "    maxvit_tiny_pm=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('PM',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(),\n",
    "    ),\n",
    "\n",
    "    maxvit_rmlp_pico_rw=MaxxVitCfg(\n",
    "        embed_dim=(32, 64, 128, 256),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(24, 32),\n",
    "        **_rw_max_cfg(rel_pos_type='mlp'),\n",
    "    ),\n",
    "    maxvit_rmlp_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(1, 2, 3, 1),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(rel_pos_type='mlp'),\n",
    "    ),\n",
    "    maxvit_rmlp_tiny_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(rel_pos_type='mlp'),\n",
    "    ),\n",
    "    maxvit_rmlp_small_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_rw_max_cfg(\n",
    "            rel_pos_type='mlp',\n",
    "            init_values=1e-6,\n",
    "        ),\n",
    "    ),\n",
    "    maxvit_rmlp_base_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        head_hidden_size=768,\n",
    "        **_rw_max_cfg(\n",
    "            rel_pos_type='mlp',\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    maxxvit_rmlp_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(1, 2, 3, 1),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        weight_init='normal',\n",
    "        **_next_cfg(),\n",
    "    ),\n",
    "    maxxvit_rmlp_tiny_rw=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(32, 64),\n",
    "        **_next_cfg(),\n",
    "    ),\n",
    "    maxxvit_rmlp_small_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(48, 96),\n",
    "        **_next_cfg(),\n",
    "    ),\n",
    "\n",
    "    maxxvitv2_nano_rw=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(1, 2, 3, 1),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(48, 96),\n",
    "        weight_init='normal',\n",
    "        **_next_cfg(\n",
    "            no_block_attn=True,\n",
    "            rel_pos_type='bias',\n",
    "        ),\n",
    "    ),\n",
    "    maxxvitv2_rmlp_base_rw=MaxxVitCfg(\n",
    "        embed_dim=(128, 256, 512, 1024),\n",
    "        depths=(2, 6, 12, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(64, 128),\n",
    "        **_next_cfg(\n",
    "            no_block_attn=True,\n",
    "        ),\n",
    "    ),\n",
    "    maxxvitv2_rmlp_large_rw=MaxxVitCfg(\n",
    "        embed_dim=(160, 320, 640, 1280),\n",
    "        depths=(2, 6, 16, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=(80, 160),\n",
    "        head_hidden_size=1280,\n",
    "        **_next_cfg(\n",
    "            no_block_attn=True,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    # Trying to be like the MaxViT paper configs\n",
    "    maxvit_tiny_tf=MaxxVitCfg(\n",
    "        embed_dim=(64, 128, 256, 512),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=64,\n",
    "        stem_bias=True,\n",
    "        head_hidden_size=512,\n",
    "        **_tf_cfg(),\n",
    "    ),\n",
    "    maxvit_small_tf=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 2, 5, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=64,\n",
    "        stem_bias=True,\n",
    "        head_hidden_size=768,\n",
    "        **_tf_cfg(),\n",
    "    ),\n",
    "    maxvit_base_tf=MaxxVitCfg(\n",
    "        embed_dim=(96, 192, 384, 768),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=64,\n",
    "        stem_bias=True,\n",
    "        head_hidden_size=768,\n",
    "        **_tf_cfg(),\n",
    "    ),\n",
    "    maxvit_large_tf=MaxxVitCfg(\n",
    "        embed_dim=(128, 256, 512, 1024),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=128,\n",
    "        stem_bias=True,\n",
    "        head_hidden_size=1024,\n",
    "        **_tf_cfg(),\n",
    "    ),\n",
    "    maxvit_xlarge_tf=MaxxVitCfg(\n",
    "        embed_dim=(192, 384, 768, 1536),\n",
    "        depths=(2, 6, 14, 2),\n",
    "        block_type=('M',) * 4,\n",
    "        stem_width=192,\n",
    "        stem_bias=True,\n",
    "        head_hidden_size=1536,\n",
    "        **_tf_cfg(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model: nn.Module):\n",
    "    model_state_dict = model.state_dict()\n",
    "    out_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.endswith('relative_position_bias_table'):\n",
    "            m = model.get_submodule(k[:-29])\n",
    "            if v.shape != m.relative_position_bias_table.shape or m.window_size[0] != m.window_size[1]:\n",
    "                v = resize_rel_pos_bias_table(\n",
    "                    v,\n",
    "                    new_window_size=m.window_size,\n",
    "                    new_bias_shape=m.relative_position_bias_table.shape,\n",
    "                )\n",
    "\n",
    "        if k in model_state_dict and v.ndim != model_state_dict[k].ndim and v.numel() == model_state_dict[k].numel():\n",
    "            # adapt between conv2d / linear layers\n",
    "            assert v.ndim in (2, 4)\n",
    "            v = v.reshape(model_state_dict[k].shape)\n",
    "        out_dict[k] = v\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def _create_maxxvit(variant, cfg_variant=None, pretrained=False, **kwargs):\n",
    "    if cfg_variant is None:\n",
    "        if variant in model_cfgs:\n",
    "            cfg_variant = variant\n",
    "        else:\n",
    "            cfg_variant = '_'.join(variant.split('_')[:-1])\n",
    "    return build_model_with_cfg(\n",
    "        MaxxVit, variant, pretrained,\n",
    "        model_cfg=model_cfgs[cfg_variant],\n",
    "        feature_cfg=dict(flatten_sequential=True),\n",
    "        pretrained_filter_fn=checkpoint_filter_fn,\n",
    "        **kwargs)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url, 'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': (7, 7),\n",
    "        'crop_pct': 0.95, 'interpolation': 'bicubic',\n",
    "        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n",
    "        'first_conv': 'stem.conv1', 'classifier': 'head.fc',\n",
    "        'fixed_input_size': True,\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = generate_default_cfgs({\n",
    "    # timm specific CoAtNet configs, ImageNet-1k pretrain, fixed rel-pos\n",
    "    'coatnet_pico_rw_224.untrained': _cfg(url=''),\n",
    "    'coatnet_nano_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_nano_rw_224_sw-f53093b4.pth',\n",
    "        crop_pct=0.9),\n",
    "    'coatnet_0_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_0_rw_224_sw-a6439706.pth'),\n",
    "    'coatnet_1_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_1_rw_224_sw-5cae1ea8.pth'\n",
    "    ),\n",
    "\n",
    "    # timm specific CoAtNet configs, ImageNet-12k pretrain w/ 1k fine-tune, fixed rel-pos\n",
    "    'coatnet_2_rw_224.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    #'coatnet_3_rw_224.untrained': _cfg(url=''),\n",
    "\n",
    "    # Experimental CoAtNet configs w/ ImageNet-12k pretrain -> 1k fine-tune (different norm layers, MLP rel-pos)\n",
    "    'coatnet_rmlp_1_rw2_224.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'coatnet_rmlp_2_rw_224.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'coatnet_rmlp_2_rw_384.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "\n",
    "    # Experimental CoAtNet configs w/ ImageNet-1k train (different norm layers, MLP rel-pos)\n",
    "    'coatnet_bn_0_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_bn_0_rw_224_sw-c228e218.pth',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD,\n",
    "        crop_pct=0.95),\n",
    "    'coatnet_rmlp_nano_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_nano_rw_224_sw-bd1d51b3.pth',\n",
    "        crop_pct=0.9),\n",
    "    'coatnet_rmlp_0_rw_224.untrained': _cfg(url=''),\n",
    "    'coatnet_rmlp_1_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_1_rw_224_sw-9051e6c3.pth'),\n",
    "    'coatnet_rmlp_2_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnet_rmlp_2_rw_224_sw-5ccfac55.pth'),\n",
    "    'coatnet_rmlp_3_rw_224.untrained': _cfg(url=''),\n",
    "    'coatnet_nano_cc_224.untrained': _cfg(url=''),\n",
    "    'coatnext_nano_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/coatnext_nano_rw_224_ad-22cb71c2.pth',\n",
    "        crop_pct=0.9),\n",
    "\n",
    "    # ImagenNet-12k pretrain CoAtNet\n",
    "    'coatnet_2_rw_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821),\n",
    "    'coatnet_3_rw_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821),\n",
    "    'coatnet_rmlp_1_rw2_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821),\n",
    "    'coatnet_rmlp_2_rw_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821),\n",
    "\n",
    "    # Trying to be like the CoAtNet paper configs (will adapt if 'tf' weights are ever released)\n",
    "    'coatnet_0_224.untrained': _cfg(url=''),\n",
    "    'coatnet_1_224.untrained': _cfg(url=''),\n",
    "    'coatnet_2_224.untrained': _cfg(url=''),\n",
    "    'coatnet_3_224.untrained': _cfg(url=''),\n",
    "    'coatnet_4_224.untrained': _cfg(url=''),\n",
    "    'coatnet_5_224.untrained': _cfg(url=''),\n",
    "\n",
    "    # timm specific MaxVit configs, ImageNet-1k pretrain or untrained\n",
    "    'maxvit_pico_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_nano_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_nano_rw_256_sw-fb127241.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_tiny_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_tiny_rw_224_sw-7d0dffeb.pth'),\n",
    "    'maxvit_tiny_rw_256.untrained': _cfg(\n",
    "        url='',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_tiny_pm_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "\n",
    "    # timm specific MaxVit w/ MLP rel-pos, ImageNet-1k pretrain\n",
    "    'maxvit_rmlp_pico_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_pico_rw_256_sw-8d82f2c6.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_rmlp_nano_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_nano_rw_256_sw-c17bb0d6.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_rmlp_tiny_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_tiny_rw_256_sw-bbef0ff5.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxvit_rmlp_small_rw_224.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxvit_rmlp_small_rw_224_sw-6ef0ae4f.pth',\n",
    "        crop_pct=0.9,\n",
    "    ),\n",
    "    'maxvit_rmlp_small_rw_256.untrained': _cfg(\n",
    "        url='',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "\n",
    "    # timm specific MaxVit w/ ImageNet-12k pretrain and 1k fine-tune\n",
    "    'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "    ),\n",
    "    'maxvit_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "\n",
    "    # timm specific MaxVit w/ ImageNet-12k pretrain\n",
    "    'maxvit_rmlp_base_rw_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821,\n",
    "    ),\n",
    "\n",
    "    # timm MaxxViT configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks)\n",
    "    'maxxvit_rmlp_nano_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_nano_rw_256_sw-0325d459.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxxvit_rmlp_tiny_rw_256.untrained': _cfg(url='', input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxxvit_rmlp_small_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights-maxx/maxxvit_rmlp_small_rw_256_sw-37e217ff.pth',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "\n",
    "    # timm MaxxViT-V2 configs (ConvNeXt conv blocks mixed with MaxVit transformer blocks, more width, no block attn)\n",
    "    'maxxvitv2_nano_rw_256.sw_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 256, 256), pool_size=(8, 8)),\n",
    "    'maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/'),\n",
    "    'maxxvitv2_rmlp_base_rw_384.sw_in12k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxxvitv2_rmlp_large_rw_224.untrained': _cfg(url=''),\n",
    "\n",
    "    'maxxvitv2_rmlp_base_rw_224.sw_in12k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=11821),\n",
    "\n",
    "    # MaxViT models ported from official Tensorflow impl\n",
    "    'maxvit_tiny_tf_224.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'maxvit_tiny_tf_384.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_tiny_tf_512.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_small_tf_224.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'maxvit_small_tf_384.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_small_tf_512.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_base_tf_224.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'maxvit_base_tf_384.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_base_tf_512.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_large_tf_224.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "    'maxvit_large_tf_384.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_large_tf_512.in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "\n",
    "    'maxvit_base_tf_224.in21k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=21843),\n",
    "    'maxvit_base_tf_384.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_base_tf_512.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_large_tf_224.in21k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=21843),\n",
    "    'maxvit_large_tf_384.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_large_tf_512.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_xlarge_tf_224.in21k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        num_classes=21843),\n",
    "    'maxvit_xlarge_tf_384.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 384, 384), pool_size=(12, 12), crop_pct=1.0, crop_mode='squash'),\n",
    "    'maxvit_xlarge_tf_512.in21k_ft_in1k': _cfg(\n",
    "        hf_hub_id='timm/',\n",
    "        input_size=(3, 512, 512), pool_size=(16, 16), crop_pct=1.0, crop_mode='squash'),\n",
    "})\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_pico_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_pico_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_nano_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_0_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_1_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_2_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_3_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_bn_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_bn_0_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_nano_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_0_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_0_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_1_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_1_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_1_rw2_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_1_rw2_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_2_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_2_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_2_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_2_rw_384', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_rmlp_3_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_rmlp_3_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_nano_cc_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_nano_cc_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnext_nano_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnext_nano_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_0_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_0_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_1_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_1_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_2_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_2_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_3_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_3_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_4_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_4_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def coatnet_5_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('coatnet_5_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_pico_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_nano_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_pico_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_pico_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_small_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_small_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_rmlp_base_rw_384', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_pm_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_pm_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvit_rmlp_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvit_rmlp_tiny_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvit_rmlp_small_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvitv2_nano_rw_256(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvitv2_nano_rw_256', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvitv2_rmlp_base_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvitv2_rmlp_base_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvitv2_rmlp_base_rw_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvitv2_rmlp_base_rw_384', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxxvitv2_rmlp_large_rw_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxxvitv2_rmlp_large_rw_224', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_tf_224', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_tf_384', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_tiny_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_tiny_tf_512', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_small_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_small_tf_224', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_small_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_small_tf_384', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_small_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_small_tf_512', 'maxvit_small_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_base_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_base_tf_224', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_base_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_base_tf_384', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_base_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_base_tf_512', 'maxvit_base_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_large_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_large_tf_224', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_large_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_large_tf_384', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_large_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_large_tf_512', 'maxvit_large_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_xlarge_tf_224(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_xlarge_tf_224', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_xlarge_tf_384(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_xlarge_tf_384', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n",
    "\n",
    "\n",
    "@register_model\n",
    "def maxvit_xlarge_tf_512(pretrained=False, **kwargs) -> MaxxVit:\n",
    "    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5739e86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:13.043345Z",
     "iopub.status.busy": "2024-04-20T16:43:13.042945Z",
     "iopub.status.idle": "2024-04-20T16:43:30.577055Z",
     "shell.execute_reply": "2024-04-20T16:43:30.575985Z"
    },
    "papermill": {
     "duration": 17.583492,
     "end_time": "2024-04-20T16:43:30.579388",
     "exception": false,
     "start_time": "2024-04-20T16:43:12.995896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model output's shape: torch.Size([1, 15])\n",
      "tensor([[-0.0622,  0.5884,  0.0112,  0.0047, -0.3784, -0.0343, -0.3528, -0.6443,\n",
      "         -0.0919, -0.6064,  0.2091, -0.2661,  0.1060,  0.1185, -0.1239]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Number of parameters in millions: 309.49 M\n",
      "Number of trainable parameters in millions: 0.02 M\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_avgpool() for <class 'torch.nn.modules.pooling.AvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 34.96G, Params: 309.36M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class CoAtNetMultiscalePyramidal(nn.Module):\n",
    "    def __init__(self, num_classes, fine_tune=False):\n",
    "        super(CoAtNetMultiscalePyramidal, self).__init__()\n",
    "\n",
    "        # Add a convolutional layer at the top\n",
    "        self.conv = nn.Conv2d(1, 3, kernel_size=3, stride=1, padding=1)  # Assuming input is grayscale (1 channel)\n",
    "\n",
    "        ## set the num_classes here according to the pretraining experiment\n",
    "        # loading the weights for coatnet module from DTD(47classes) or Flowers102(102classes)\n",
    "        #pretrained weights\n",
    "        \n",
    "        # DTD weights (kaggle dataset version-2)\n",
    "        self.coatnet = coatnet_3_rw_224(pretrained=False, in_chans=3, num_classes=47)\n",
    "        model_path = '/kaggle/input/18-04-2024-dtd-coatnetmultiscalev1-multiclass-prwt/best_model_precision.pth'\n",
    "        \n",
    "        #OR (comment one of these)\n",
    "        \n",
    "        # Flowers102 weights (kaggle dataset version-1)\n",
    "#         self.coatnet = coatnet_3_rw_224(pretrained=False, in_chans=3, num_classes=102)\n",
    "#         model_path = '/kaggle/input/18-04-2024-flowers102-coatnetmultiscale-pretwts/best_model_precision.pth'\n",
    "\n",
    "        \n",
    "        self.coatnet.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "        \n",
    "        if not fine_tune:\n",
    "            # Freeze all layers except classifier layers\n",
    "            for param in self.coatnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Unfreeze the classifier layers\n",
    "            for param in self.coatnet.head.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "\n",
    "        # Get the number of input features for the final fully connected layer\n",
    "        in_features = self.coatnet.head.fc.in_features\n",
    "\n",
    "        # Replace the final fully connected layer with a new one for the specified number of classes\n",
    "        self.coatnet.head.fc = nn.Linear(in_features, num_classes)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.coatnet(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CoAtNetMultiscalePyramidal(num_classes, fine_tune=False) # change fine_tune as required\n",
    "model.to(device)\n",
    "\n",
    "# print(model)\n",
    "print()\n",
    "\n",
    "x = torch.randn(1, 1, IMAGE_SIZE[0], IMAGE_SIZE[1]).to(device)\n",
    "\n",
    "\n",
    "output = model(x)\n",
    "print(\"Model output's shape:\", output.shape)\n",
    "print(output) # logits \n",
    "display_params_flops(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25f28994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T16:43:30.675140Z",
     "iopub.status.busy": "2024-04-20T16:43:30.674389Z",
     "iopub.status.idle": "2024-04-20T16:43:30.682673Z",
     "shell.execute_reply": "2024-04-20T16:43:30.681706Z"
    },
    "papermill": {
     "duration": 0.059052,
     "end_time": "2024-04-20T16:43:30.684979",
     "exception": false,
     "start_time": "2024-04-20T16:43:30.625927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierHead(\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (fc): Linear(in_features=1536, out_features=15, bias=True)\n",
       "  (flatten): Identity()\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coatnet.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e4578",
   "metadata": {
    "papermill": {
     "duration": 0.045772,
     "end_time": "2024-04-20T16:43:30.778187",
     "exception": false,
     "start_time": "2024-04-20T16:43:30.732415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddd95d",
   "metadata": {
    "papermill": {
     "duration": 0.045748,
     "end_time": "2024-04-20T16:43:30.870083",
     "exception": false,
     "start_time": "2024-04-20T16:43:30.824335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4829043,
     "sourceId": 8175275,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4836979,
     "isSourceIdPinned": true,
     "sourceId": 8172516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 260.436404,
   "end_time": "2024-04-20T16:43:34.725129",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-20T16:39:14.288725",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00b6c418af9f49e69a613a488d56e55a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "01efbc911e9d48e58365e5bbbb84bc3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "02d55ef7d3104ec3958464cead23929f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b5e8c512dca040a2acf811f3c68c4ab9",
        "IPY_MODEL_f2e56f3f4c34455393394dc04637c095",
        "IPY_MODEL_9abab8c456934c0fa1dcf3272ca5b95e"
       ],
       "layout": "IPY_MODEL_878926bf0a104e7c95ae4b04e0d82f3b"
      }
     },
     "058dd793ed514eee8448af14a512f65a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0a52192a30224800bc7650036282d0da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a83fad8b6546439fbb89cd78e45c1c8d",
       "placeholder": "​",
       "style": "IPY_MODEL_91e8abf394a44bccbdf5d65f5c64ff9a",
       "value": " 32.3M/32.3M [00:00&lt;00:00, 40.5MB/s]"
      }
     },
     "13951af86b514e2a9e34f04ed6016eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7eb6d278593476e85c9542d77fe9f7d",
       "placeholder": "​",
       "style": "IPY_MODEL_a9680d8ea9174dd296c41493a0c3382d",
       "value": " 916M/916M [00:04&lt;00:00, 244MB/s]"
      }
     },
     "13d997c09c7b46fe91d51cf8fbe36490": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7de274960573470bb7037c414ababa88",
       "placeholder": "​",
       "style": "IPY_MODEL_ee772da1f38c4ceda5c78c8c78c956d6",
       "value": "model.safetensors: 100%"
      }
     },
     "13f78f50e0f1484988cb9d1fbe073867": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "18e7439072304e2e8e37533cc4e3e4f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1b0e6a5a8e6340b089a91ca2c0ec3ae0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9f8d35700d17485ba4f538b9d97b6e6e",
        "IPY_MODEL_b3a1c821d99a4b49bebb79cfd048035f",
        "IPY_MODEL_e8db46616f9343588993ce82c1064213"
       ],
       "layout": "IPY_MODEL_32981108a4984ba3bf0d6139bd79b861"
      }
     },
     "24c323f230024eec96fd553896666dd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ba6c9a8606d407795cd7b711eb8330d",
        "IPY_MODEL_8bc3c7e20dd34c15b2c7004fbc81fca0",
        "IPY_MODEL_cf8caf6714194fafb9279aaba86565ad"
       ],
       "layout": "IPY_MODEL_e28a52ed9a19444489e2bd928ec25f02"
      }
     },
     "27fbbfb92ed84b4a83c995183324eb92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2b51bf8482204ddfad6f7b408efc8d09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b6bb1446f8e54d29ab75e12fbfb962b2",
       "placeholder": "​",
       "style": "IPY_MODEL_862d7b62c9134af0b4b3ed8f9f2c649f",
       "value": " 352M/352M [00:02&lt;00:00, 221MB/s]"
      }
     },
     "2dac71752f8948b3827ff394d196ad4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f2402db3150475384e5056aad2005a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32981108a4984ba3bf0d6139bd79b861": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33d4bf1101914325879cce25cd4d7345": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "342cda8761cf48a3a2ec4dd807a60e18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "380f257d39da459a855f21c52b1ce1a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6fcb4237192d4d928f0e5a79b14f7f5a",
       "max": 13684888.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c6fc70966e4f4309ab0d92d8755ea2c5",
       "value": 13684888.0
      }
     },
     "38c29e5b6aff432c999b04fb4d4cb607": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3aa320c9dae34e539be2d7d555316f60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b51fdbc652c424788d7dc093e597686": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3caebbaeb06c4950a1b3af1da5de5194": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9cb1752907ca4bc38fb6ffcc1a22e7ec",
       "max": 327931912.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c21f70c06a034cd8bec4db525d3f5e8f",
       "value": 327931912.0
      }
     },
     "3d7afc6b48ae414e8848387ce7b0487e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3ddf72f20f2f47a9ac4c3e9e1d15af98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_63d2e275f2784a80b27ea40ba3d886a2",
        "IPY_MODEL_cd9e872c32074d5188eb00bb5dbaa83d",
        "IPY_MODEL_13951af86b514e2a9e34f04ed6016eca"
       ],
       "layout": "IPY_MODEL_7e3af23d5b6b4c46b95649fc63639013"
      }
     },
     "427c9d5d54684bf89c6a904aaa1a2114": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "438457235cf94e54b185dd7b5b42bb92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "439363fd09f1420992f30a2c5f688de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2dac71752f8948b3827ff394d196ad4b",
       "placeholder": "​",
       "style": "IPY_MODEL_8da454aa486844a1b1d180f48f8b23ca",
       "value": "model.safetensors: 100%"
      }
     },
     "4987313b0e804e50bf169dae570181f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a00ccae134446dabc0defc3ccc8c19a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b705801d7254feb86e26e8dea6ab9b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7d21493d1c3641f792709dbe06a10a31",
        "IPY_MODEL_ea9cd2da633948d7b0f89ade955a1b50",
        "IPY_MODEL_2b51bf8482204ddfad6f7b408efc8d09"
       ],
       "layout": "IPY_MODEL_00b6c418af9f49e69a613a488d56e55a"
      }
     },
     "4fc3c288712b4d749abff03ade3a1a37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "519bc9024bb24363a9a50da9f2cc7e88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "591f472037f54397b6424dab00cb4243": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5946f38f5e89456dba30bfdaa3ffa75f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9532245efd34529845ef1c7919a9a91",
       "max": 205948668.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e729b1769aca445ab16c7b5ed1138670",
       "value": 205948668.0
      }
     },
     "5ba6c9a8606d407795cd7b711eb8330d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d76852f92700460f90e376aa062b14d8",
       "placeholder": "​",
       "style": "IPY_MODEL_3d7afc6b48ae414e8848387ce7b0487e",
       "value": "model.safetensors: 100%"
      }
     },
     "5eba371cae0e4c279c598060aa391d9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5f6fee7cdf67426ea3a445df5300e270": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63d2e275f2784a80b27ea40ba3d886a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c3dec4660f7407ca606e65e487cfbce",
       "placeholder": "​",
       "style": "IPY_MODEL_bb8118f0b101456387acaeb47d76525c",
       "value": "model.safetensors: 100%"
      }
     },
     "66069118eea949acaf6c119a217b9663": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "67646f3a544a48d0ae834019b9a0bc23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e194832740774bd7b41d2f03d065e1a6",
        "IPY_MODEL_380f257d39da459a855f21c52b1ce1a5",
        "IPY_MODEL_6f627e2cbff04fa0aa937921aa1fb725"
       ],
       "layout": "IPY_MODEL_b56260afef784e23a542d0ccc489afce"
      }
     },
     "68631f764e2b48ad8a6de081d96a438f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b463cf896ef4b85ad43de188fff906d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6be6b3283d3847bca74bd78f167ee925": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6d5fe0b0a1114da99a3b3978037005c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d79ebbecb2d40b095e3697d2f885818": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6f627e2cbff04fa0aa937921aa1fb725": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aa16c0c1545d40b5b04c2300e7f20781",
       "placeholder": "​",
       "style": "IPY_MODEL_13f78f50e0f1484988cb9d1fbe073867",
       "value": " 13.7M/13.7M [00:00&lt;00:00, 224MB/s]"
      }
     },
     "6fcb4237192d4d928f0e5a79b14f7f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "749f9500fd6447c3b1d516cd2392e8fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "759a2a5efd764cc492f77a412fc976aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c317b75bae2c4ccbbe80479ed39f2a60",
       "max": 727446832.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_33d4bf1101914325879cce25cd4d7345",
       "value": 727446832.0
      }
     },
     "7d21493d1c3641f792709dbe06a10a31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f2402db3150475384e5056aad2005a3",
       "placeholder": "​",
       "style": "IPY_MODEL_eb66dc441f204e9ea563cb7def6e3ddc",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "7de274960573470bb7037c414ababa88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e3af23d5b6b4c46b95649fc63639013": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82c96039bdd54c9c8481793bf836a167": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "84055dcb41e545d099a93d8609b0f3bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_38c29e5b6aff432c999b04fb4d4cb607",
       "placeholder": "​",
       "style": "IPY_MODEL_749f9500fd6447c3b1d516cd2392e8fd",
       "value": " 206M/206M [00:03&lt;00:00, 58.7MB/s]"
      }
     },
     "84964edcfd92439787a353bcfc748ade": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_13d997c09c7b46fe91d51cf8fbe36490",
        "IPY_MODEL_5946f38f5e89456dba30bfdaa3ffa75f",
        "IPY_MODEL_84055dcb41e545d099a93d8609b0f3bd"
       ],
       "layout": "IPY_MODEL_058dd793ed514eee8448af14a512f65a"
      }
     },
     "862d7b62c9134af0b4b3ed8f9f2c649f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "878926bf0a104e7c95ae4b04e0d82f3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "88bad69ebac44d43a24c03f4fd668658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_439363fd09f1420992f30a2c5f688de4",
        "IPY_MODEL_3caebbaeb06c4950a1b3af1da5de5194",
        "IPY_MODEL_e08ade4bdbd04ac2956f93e266e2ce99"
       ],
       "layout": "IPY_MODEL_3b51fdbc652c424788d7dc093e597686"
      }
     },
     "8bc3c7e20dd34c15b2c7004fbc81fca0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6d5fe0b0a1114da99a3b3978037005c0",
       "max": 114374272.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_427c9d5d54684bf89c6a904aaa1a2114",
       "value": 114374272.0
      }
     },
     "8da454aa486844a1b1d180f48f8b23ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8f7f163353364617a4e6f5c6c7e2426d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b9e648adb6804d018b9f2e39a73cb42f",
        "IPY_MODEL_f18ce60b968349b4a9ba161548727b04",
        "IPY_MODEL_0a52192a30224800bc7650036282d0da"
       ],
       "layout": "IPY_MODEL_f1632993a6294f80837f4eb166c38416"
      }
     },
     "91e8abf394a44bccbdf5d65f5c64ff9a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9abab8c456934c0fa1dcf3272ca5b95e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c469c93e09b1416291c19f443eb917c6",
       "placeholder": "​",
       "style": "IPY_MODEL_438457235cf94e54b185dd7b5b42bb92",
       "value": " 124M/124M [00:00&lt;00:00, 157MB/s]"
      }
     },
     "9c3dec4660f7407ca606e65e487cfbce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cb1752907ca4bc38fb6ffcc1a22e7ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9f8d35700d17485ba4f538b9d97b6e6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b3f93f879c874a3abeca8bd938f2d2c9",
       "placeholder": "​",
       "style": "IPY_MODEL_b4e2a2334593484a9f11e241496e40da",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "a83fad8b6546439fbb89cd78e45c1c8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a9680d8ea9174dd296c41493a0c3382d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "aa16c0c1545d40b5b04c2300e7f20781": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa7caecc319e4ca187a2449e8059f4d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b3a1c821d99a4b49bebb79cfd048035f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_591f472037f54397b6424dab00cb4243",
       "max": 199137629.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_01efbc911e9d48e58365e5bbbb84bc3c",
       "value": 199137629.0
      }
     },
     "b3f93f879c874a3abeca8bd938f2d2c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4e2a2334593484a9f11e241496e40da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b56260afef784e23a542d0ccc489afce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5e8c512dca040a2acf811f3c68c4ab9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_519bc9024bb24363a9a50da9f2cc7e88",
       "placeholder": "​",
       "style": "IPY_MODEL_f3db5ea657f14503a8f4189930eb29f4",
       "value": "model.safetensors: 100%"
      }
     },
     "b6bb1446f8e54d29ab75e12fbfb962b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7e24d03eb984da68898d2d7cde5cba3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9532245efd34529845ef1c7919a9a91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9e648adb6804d018b9f2e39a73cb42f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_66069118eea949acaf6c119a217b9663",
       "placeholder": "​",
       "style": "IPY_MODEL_18e7439072304e2e8e37533cc4e3e4f6",
       "value": "model.safetensors: 100%"
      }
     },
     "bb8118f0b101456387acaeb47d76525c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bd09970399d24876a04455a29e12b5a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fdaf31bd154e4a01b46631b97e4014dd",
       "placeholder": "​",
       "style": "IPY_MODEL_cda0dc04c5e64c568f03ae7bf571dd1e",
       "value": " 727M/727M [00:03&lt;00:00, 230MB/s]"
      }
     },
     "c1c48f737aff481abc22aa32daae573a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82c96039bdd54c9c8481793bf836a167",
       "placeholder": "​",
       "style": "IPY_MODEL_5eba371cae0e4c279c598060aa391d9d",
       "value": "model.safetensors: 100%"
      }
     },
     "c21f70c06a034cd8bec4db525d3f5e8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c317b75bae2c4ccbbe80479ed39f2a60": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c469c93e09b1416291c19f443eb917c6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5c43abbb52d4ac5ac984d52599aa420": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6fc70966e4f4309ab0d92d8755ea2c5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cd9e872c32074d5188eb00bb5dbaa83d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7e24d03eb984da68898d2d7cde5cba3",
       "max": 915873508.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ec95d2e90150472f80978e29c9a3c205",
       "value": 915873508.0
      }
     },
     "cda0dc04c5e64c568f03ae7bf571dd1e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cf8caf6714194fafb9279aaba86565ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4987313b0e804e50bf169dae570181f9",
       "placeholder": "​",
       "style": "IPY_MODEL_aa7caecc319e4ca187a2449e8059f4d1",
       "value": " 114M/114M [00:02&lt;00:00, 54.4MB/s]"
      }
     },
     "cf9ffca07d324dfba9f584f6d7e2f90a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c1c48f737aff481abc22aa32daae573a",
        "IPY_MODEL_759a2a5efd764cc492f77a412fc976aa",
        "IPY_MODEL_bd09970399d24876a04455a29e12b5a1"
       ],
       "layout": "IPY_MODEL_d4812d4a6bb94e7985ae57b73dc10cc5"
      }
     },
     "d0fdae856f354794969a0cdd4c3a6596": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d4812d4a6bb94e7985ae57b73dc10cc5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d76852f92700460f90e376aa062b14d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e08ade4bdbd04ac2956f93e266e2ce99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c5c43abbb52d4ac5ac984d52599aa420",
       "placeholder": "​",
       "style": "IPY_MODEL_6b463cf896ef4b85ad43de188fff906d",
       "value": " 328M/328M [00:06&lt;00:00, 58.1MB/s]"
      }
     },
     "e194832740774bd7b41d2f03d065e1a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5f6fee7cdf67426ea3a445df5300e270",
       "placeholder": "​",
       "style": "IPY_MODEL_d0fdae856f354794969a0cdd4c3a6596",
       "value": "model.safetensors: 100%"
      }
     },
     "e28a52ed9a19444489e2bd928ec25f02": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e729b1769aca445ab16c7b5ed1138670": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e8db46616f9343588993ce82c1064213": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_27fbbfb92ed84b4a83c995183324eb92",
       "placeholder": "​",
       "style": "IPY_MODEL_4fc3c288712b4d749abff03ade3a1a37",
       "value": " 199M/199M [00:04&lt;00:00, 43.4MB/s]"
      }
     },
     "ea9cd2da633948d7b0f89ade955a1b50": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4a00ccae134446dabc0defc3ccc8c19a",
       "max": 351971805.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6d79ebbecb2d40b095e3697d2f885818",
       "value": 351971805.0
      }
     },
     "eb66dc441f204e9ea563cb7def6e3ddc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ec95d2e90150472f80978e29c9a3c205": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ee772da1f38c4ceda5c78c8c78c956d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1632993a6294f80837f4eb166c38416": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f18ce60b968349b4a9ba161548727b04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_342cda8761cf48a3a2ec4dd807a60e18",
       "max": 32334434.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6be6b3283d3847bca74bd78f167ee925",
       "value": 32334434.0
      }
     },
     "f2e56f3f4c34455393394dc04637c095": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3aa320c9dae34e539be2d7d555316f60",
       "max": 123917994.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_68631f764e2b48ad8a6de081d96a438f",
       "value": 123917994.0
      }
     },
     "f3db5ea657f14503a8f4189930eb29f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7eb6d278593476e85c9542d77fe9f7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fdaf31bd154e4a01b46631b97e4014dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
